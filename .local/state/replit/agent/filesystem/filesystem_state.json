{"file_contents":{"app.py":{"content":"import streamlit as st\nimport os\nimport logging\nfrom pathlib import Path\nimport sys\n\n# Add project root to path\nproject_root = Path(__file__).parent\nsys.path.append(str(project_root))\n\n# Import custom components\nfrom components.rag_system import RAGSystem\nfrom components.fine_tuning import FineTuningInterface\nfrom components.evaluation import EvaluationDashboard\nfrom components.safety import SafetySystem\nfrom components.model_export import ModelExport\nfrom components.chat_interface import ChatInterface\nfrom config.settings import Settings\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize settings\nsettings = Settings()\n\ndef check_dependencies():\n    \"\"\"Check and log available dependencies\"\"\"\n    from utils.fallbacks import (\n        HAS_TORCH, HAS_TRANSFORMERS, HAS_SENTENCE_TRANSFORMERS, \n        HAS_FAISS, HAS_PEFT, HAS_NEMO\n    )\n    \n    logger.info(f\"TORCH: {HAS_TORCH}\")\n    logger.info(f\"TRANSFORMERS: {HAS_TRANSFORMERS}\")\n    logger.info(f\"SENTENCE_TRANSFORMERS: {HAS_SENTENCE_TRANSFORMERS}\")\n    logger.info(f\"FAISS: {HAS_FAISS}\")\n    logger.info(f\"PEFT: {HAS_PEFT}\")\n    logger.info(f\"NEMO: {HAS_NEMO}\")\n    \ndef initialize_session_state():\n    \"\"\"Initialize session state variables\"\"\"\n    if 'rag_system' not in st.session_state:\n        st.session_state.rag_system = RAGSystem()\n\n    \n    if 'rag_initialized' not in st.session_state:\n        st.session_state.rag_initialized = False\n    \n    # Load existing index if available\n    if not st.session_state.rag_initialized:\n        rag_index_path = \"data/rag_index.faiss\"\n        rag_meta_path = \"data/rag_meta.pkl\"\n        \n        if (os.path.exists(rag_index_path) or os.path.exists(rag_index_path + \".fallback\")) and os.path.exists(rag_meta_path):\n            if st.session_state.rag_system.load_index():\n                st.session_state.rag_initialized = True\n                logger.info(\"Loaded existing RAG index\")\n                \n    if 'model_trained' not in st.session_state:\n        st.session_state.model_trained = False\n    if 'current_model' not in st.session_state:\n        st.session_state.current_model = None\n    if 'safety_system' not in st.session_state:\n        st.session_state.safety_system = SafetySystem()\n    if 'chat_history' not in st.session_state:\n        st.session_state.chat_history = []\n    if 'rag_backend' not in st.session_state:\n        st.session_state.rag_backend = settings.RAG_BACKEND  # default from config\n\n\ndef main():\n    st.set_page_config(\n        page_title=\"MedGemma AI Platform\",\n        page_icon=\"🏥\",\n        layout=\"wide\",\n        initial_sidebar_state=\"expanded\"\n    )\n    check_dependencies()\n    initialize_session_state()\n    \n    # Main title\n    st.title(\"🏥 MedGemma AI Platform\")\n    st.markdown(\"*Advanced Medical AI with RAG, Fine-tuning, and Safety Systems*\")\n    \n    # Sidebar navigation\n    st.sidebar.title(\"Navigation\")\n    page = st.sidebar.selectbox(\n        \"Select Module\",\n        [\n            \"🏠 Dashboard\",\n            \"🔍 RAG System\",\n            \"🎯 Fine-tuning\",\n            \"📊 Evaluation\",\n            \"🛡️ Safety & Guardrails\",\n            \"📦 Model Export\",\n            \"💬 Chat Interface\"\n        ]\n    )\n    \n    # System status in sidebar\n    st.sidebar.markdown(\"---\")\n    st.sidebar.markdown(\"### System Status\")\n    \n    # Show active backend\n    st.sidebar.markdown(f\"**RAG Backend:** {st.session_state.rag_backend.upper()}\")\n\n    # Check RAG system status\n    rag_status = \"✅ Ready\" if st.session_state.rag_system else \"❌ Not Initialized\"\n    st.sidebar.markdown(f\"**RAG System:** {rag_status}\")\n    \n    # Check model status\n    model_status = \"✅ Trained\" if st.session_state.model_trained else \"❌ Not Trained\"\n    st.sidebar.markdown(f\"**Model:** {model_status}\")\n    \n    # Display current model info\n    if st.session_state.current_model:\n        st.sidebar.markdown(f\"**Current Model:** {st.session_state.current_model}\")\n    \n    # Main content based on selected page\n    if page == \"🏠 Dashboard\":\n        show_dashboard()\n    elif page == \"🔍 RAG System\":\n        rag_interface = RAGSystem()\n        rag_interface.render()\n    elif page == \"🎯 Fine-tuning\":\n        ft_interface = FineTuningInterface()\n        ft_interface.render()\n    elif page == \"📊 Evaluation\":\n        eval_interface = EvaluationDashboard()\n        eval_interface.render()\n    elif page == \"🛡️ Safety & Guardrails\":\n        st.session_state.safety_system.render()\n    elif page == \"📦 Model Export\":\n        export_interface = ModelExport()\n        export_interface.render()\n    elif page == \"💬 Chat Interface\":\n        chat_interface = ChatInterface()\n        chat_interface.render()\n\n\ndef show_dashboard():\n    \"\"\"Display main dashboard with system overview\"\"\"\n    st.header(\"System Overview\")\n    \n    # Create metrics columns\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\n            label=\"RAG Documents\",\n            value=len(st.session_state.rag_system.passages) if st.session_state.rag_system else 0,\n            delta=\"Ready\" if st.session_state.rag_system else \"Not loaded\"\n        )\n    \n    with col2:\n        st.metric(\n            label=\"Model Status\",\n            value=\"Trained\" if st.session_state.model_trained else \"Base\",\n            delta=\"Fine-tuned\" if st.session_state.model_trained else \"Not trained\"\n        )\n    \n    with col3:\n        st.metric(\n            label=\"Safety Checks\",\n            value=\"Active\",\n            delta=\"Monitoring\"\n        )\n    \n    with col4:\n        st.metric(\n            label=\"Chat Sessions\",\n            value=len(st.session_state.chat_history),\n            delta=\"Active\"\n        )\n    \n    # Quick start guide\n    st.markdown(\"---\")\n    st.header(\"Quick Start Guide\")\n    \n    steps = [\n        {\n            \"title\": \"1. Initialize RAG System\",\n            \"description\": \"Upload medical documents and build the knowledge base\",\n            \"status\": \"✅ Complete\" if st.session_state.rag_system else \"⏳ Pending\"\n        },\n        {\n            \"title\": \"2. Fine-tune Model\",\n            \"description\": \"Train the model on your specific medical data\",\n            \"status\": \"✅ Complete\" if st.session_state.model_trained else \"⏳ Pending\"\n        },\n        {\n            \"title\": \"3. Evaluate Performance\",\n            \"description\": \"Run evaluation metrics and review results\",\n            \"status\": \"🔄 Available\"\n        },\n        {\n            \"title\": \"4. Deploy for Chat\",\n            \"description\": \"Start using the medical AI assistant\",\n            \"status\": \"🔄 Available\"\n        }\n    ]\n    \n    for step in steps:\n        with st.expander(step[\"title\"]):\n            st.markdown(f\"**Status:** {step['status']}\")\n            st.markdown(step[\"description\"])\n    \n    # Recent activity\n    st.markdown(\"---\")\n    st.header(\"Recent Activity\")\n    \n    if st.session_state.chat_history:\n        st.markdown(\"**Latest Chat Sessions:**\")\n        for i, chat in enumerate(st.session_state.chat_history[-5:]):\n            st.markdown(f\"- Session {i+1}: {chat.get('timestamp', 'Unknown time')}\")\n    else:\n        st.info(\"No recent activity. Start by initializing the RAG system or begin a chat session.\")\n    \n    # System configuration\n    st.markdown(\"---\")\n    st.header(\"System Configuration\")\n    \n    config_col1, config_col2 = st.columns(2)\n    \n    with config_col1:\n        st.subheader(\"Model Settings\")\n        st.code(f\"\"\"\nBase Model: {settings.BASE_MODEL}\nMax Tokens: {settings.MAX_NEW_TOKENS}\nTemperature: {settings.TEMPERATURE}\n        \"\"\")\n    \n    with config_col2:\n        st.subheader(\"RAG Settings\")\n        st.code(f\"\"\"\nEmbedding Model: {settings.EMBED_MODEL}\nTop-K Retrieval: {settings.RETRIEVAL_TOP_K}\nBackend: {st.session_state.rag_backend.upper()}\nIndex Type: FAISS\n        \"\"\")\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":7919},"pyproject.toml":{"content":"[project]\nname = \"medgemma-ai-platform\"\nversion = \"1.0.0\"\ndescription = \"World-class MedGemma AI Healthcare Platform\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"streamlit>=1.48.1\",\n    \"numpy>=1.24.0\",\n    \"pandas>=2.0.0\",\n    \"plotly>=5.0.0\",\n    \"scikit-learn>=1.3.0\",\n    \"torch\",\n    \"transformers\",\n    \"sentence-transformers\",\n    \"faiss-cpu\",\n    \"peft\",\n    \"trl\", \n    \"datasets\",\n    \"rouge-score\",\n    \"nltk\",\n    \"accelerate\",\n    \"huggingface-hub\",\n    \"safetensors\",\n    \"PyPDF2\",\n    \"pdfplumber\",\n    \"Pillow\",\n    \"requests\",\n]\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[[tool.uv.index]]\nexplicit = true\nname = \"pytorch-cpu\"\nurl = \"https://download.pytorch.org/whl/cpu\"\n\n[tool.uv.sources]\nAA-module = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nABlooper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nAnalysisG = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nAutoRAG = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nBERTeam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nBxTorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nByaldi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCALM-Pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCOPEX-high-rate-compression-quality-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCityLearn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCoCa-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCoLT5-attention = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nComfyUI-EasyNodes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nCrawl4AI = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDALL-E = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDI-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDatasetRising = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDeepCache = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDeepMatter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nDraugr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nESRNN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nEn-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nExpoSeq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nFLAML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nFSRS-Optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGANDLF = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGQLAlchemy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGhostScan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nGraKeL = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nHEBO = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nIOPaint = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nISLP = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nInvokeAI = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nJAEN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nKapoorLabs-Lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nLightAutoML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nLingerGRN = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nMMEdu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nMRzeroCore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nModeva = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNeuralFoil = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNiMARE = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nNinjaTools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nOpenHosta = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nOpenNMT-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPOT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPVNet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPaLM-rlhf-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPepperPepper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPiML = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nPoutyne = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nQNCP = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRAGatouille = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRareGO = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRealtimeSTT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nRelevanceAI-Workflows-Core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nResemblyzer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nScandEval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nSimba-UW-tf-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nSwissArmyTransformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTPOT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTTS = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTorchCRF = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nTotalSegmentator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nUtilsRL = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nWhisperSpeech = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nXAISuite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\na-unet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\na5dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccelerate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccelerated-scan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naccern-xyme = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nachatbot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nacids-rave = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nactorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nacvl-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadabelief-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadam-atan2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadan-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadapters = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadmin-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadtoolbox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nadversarial-robustness-toolbox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naeiou = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naeon = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nafricanwhisper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nag-llama-api = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nagentdojo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nagilerl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-edge-torch-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-parrot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai-transform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-olmo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-olmo-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nai2-tango = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naicmder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naider-chat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naider-chat-x = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naif360 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naihwkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naimodelshare = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairtestProject = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nairunner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naisak = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naislib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naisquared = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naistore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naithree = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nakasha-terminal = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalibi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalibi-detect = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalignn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nall-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallennlp-pvt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallophant = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nallosaurus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naloy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalpaca-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphafold2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphafold3-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphamed-federated = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nalphawave = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\namazon-braket-pennylane-plugin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\namazon-photos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanemoi-graphs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanemoi-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nanomalib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\napache-beam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\napache-tvm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naperturedb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naphrodite-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naqlm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narcAGI2024 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narchisound = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nargbind = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narize = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narm-pytorch-utilities = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narray-api-compat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\narus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nassert-llm-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nasteroid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nasteroid-filterbanks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nastra-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nastrovision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\natomate2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nattacut = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-encoders-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudio-separator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudiocraft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naudiolm-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauralis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauraloss = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nauto-gptq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautoawq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautoawq-kernels = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.multimodal\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.tabular\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"autogluon.timeseries\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nautotrain-advanced = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\navdeepfake1m = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\naws-fortuna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nax-platform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-automl-dnn-vision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-contrib-automl-dnn-forecasting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-evaluate-mlflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nazureml-train-automl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nb2bTools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbackpack-for-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbalrog-nle = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatch-face = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchalign = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchgeneratorsv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbatchtensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbbrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbenchpots = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbent = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbert-score = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbertopic = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbertviz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbestOf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbetty-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbig-sleep = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-core-cpp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-core-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbigdl-nano = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"bioimageio.core\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbitfount = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbitsandbytes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbittensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbittensor-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblackboxopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblanc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nblindai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbm25-pt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nboltz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbotorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nboxmot = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrainchain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbraindecode = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrevitas = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbriton = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbrowsergym-visualwebarena = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbuzz-captions = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbyotrack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nbyzerllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nc4v-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncalflops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncame-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncamel-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncamel-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncannai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncaptum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncarte-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncarvekit-colab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncatalyst = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausalml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausalnex = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncausy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncbrkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncca-zoo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncdp-backend = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellacdc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellfinder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellpose = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncellxgene-census = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchattts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchemprop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchgnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nchitra = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncircuitsvis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncjm-yolox-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclarinpl-embeddings = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclass-resolver = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassifier-free-guidance-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassiq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclassy-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclean-fid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncleanvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-anytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-benchmark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-by-openai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-interrogator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclip-retrieval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncltk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nclusterops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncnocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncnstd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoba = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncofi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncolbert-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncolpali-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-ray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-ray-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-train = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposabl-train-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncomposer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressed-tensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncompressed-tensors-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconcrete-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconfit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontextualSpellCheck = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontinual-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncontrolnet-aux = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nconvokit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoola = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoqui-tts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncoqui-tts-trainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncraft-text-detector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncreme = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncrocodile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncrowd-kit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncryoSPHERE = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncsle-common = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncsle-system-identification = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nctgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncurated-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncut-cross-entropy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncvat-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ncybertask = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nd3rlpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndalle-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndalle2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndanila-lib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndanling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndarts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndarwin-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndata-gradients = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatachain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndataclass-array = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndataeval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatarobot-drum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatarobotx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndatumaro = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndctorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeep-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepchecks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepchem = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepctr-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepecho = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepepochs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepforest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeeplabcut = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepmd-kit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepmultilingualpunctuation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeeprobust = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepsparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepsparse-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndeepspeed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndenoising-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndescript-audio-codec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndescript-audiotools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndetecto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndetoxify = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndgenerate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndghs-imgutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndgl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndialogy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndice-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffgram = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndiffusers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndistilabel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndistrifuser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndnikit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocarray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndoclayout-yolo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocling-ibm-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndocquery = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndomino-code-assist = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndreamsim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndropblock = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndruida = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ndvclive = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne2-tts-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne2cnn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ne3nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neasyocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nebtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\necallisto-ng = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nedsnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neffdet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neinx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neir-dl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neis1600 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\neland = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nema-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nembedchain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nenformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nentmax = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nesm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nespaloma-charge = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nespnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\netils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\netna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevadb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevalscope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nevaluate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nexllamav2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nextractable = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nface-alignment = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfacenet-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfacexlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfair-esm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfairseq2n = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfaker-file = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfarm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfast-bert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfast-pytorch-kmeans = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastcore = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfastestimator-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfasttreeshap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfedml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfelupe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfemr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfft-conv-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfickling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfireworks-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflair = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflashrag-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflax = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflexgen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflgo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflopth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflowcept = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflytekitplugins-kfpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nflytekitplugins-onnxpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfmbench = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfocal-frequency-loss = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfoldedtensor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfractal-tasks-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfreegenius = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfreqtrade = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfschat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunasr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunctorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunlbm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nfunsor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngalore-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngarak = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngarf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngateloop-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngeffnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngenutility = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngfpgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngigagan-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngin-config = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nglasflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngliner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngluonts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngmft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngoogle-cloud-aiplatform = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpforecaster = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpt3discord = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngpytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngrad-cam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngraph-weather = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngraphistry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngravitorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngretel-synthetics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngsplat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nguardrails-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nguidance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ngymnasium = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhanlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhappytransformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhbutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nheavyball = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhezar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhf-deepali = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhf-doc-builder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhigher = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhjxdl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhkkang-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhordelib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhpsv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhuggingface-hub = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhummingbird-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhvae-backbone = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhya = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nhypothesis-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-metrics-plugin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-watson-machine-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nibm-watsonx-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nicetk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nicevision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niden = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nidvpackage = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niglovikov-helper-functions = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimagededup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimagen-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimaginAIry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nimg2vec-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nincendio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninference-gpu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfinity-emb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfo-nce-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninfoapps-mlops-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-dolomite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-sdg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninstructlab-training = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ninvisible-watermark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niobm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nipex-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\niree-turbine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-azure-openai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-torchvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nirisml-tasks-training = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nitem-matching = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nivadomed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njaqpotpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njina = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njudo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\njunky = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk-diffusion = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk1lib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nk2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkappadata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkappamodules = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkarbonn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkats = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkbnf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkedro-datasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkeybert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkeytotext = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkhoj = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkiui = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkonfuzio-sdk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkornia = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkornia-moons = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkraken = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkwarray = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nkwimage = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlabml-nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlagent = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlaion-clap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlama-cleaner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlancedb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangcheck = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangroid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlangtest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlayoutparser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nldp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleafmap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleap-ie = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleibniz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nleptonai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nletmedoit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlhotse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlib310 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibpecos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibrec-auto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlibretranslate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliger-kernel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliger-kernel-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-bolts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-fabric = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-habana = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightning-lite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightrag = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightweight-gan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlightwood = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinear-attention-transformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinear-operator = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinformer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlinformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nliom-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlit-nlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitelama = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlitgpt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-adapter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-embeddings-instructor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-llms-huggingface = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllama-index-postprocessor-colbert-rerank = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-blender = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-foundry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-guard = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm-rs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllm2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmcompressor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmlingua = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nllmvm-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlm-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlmdeploy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlmms-eval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlocal-attention = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlovely-tensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlpips = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nlycoris-lora = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmace-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagic-pdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagicsoup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmagvit2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmaite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanga-ocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanifest-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmanipulation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmarker-pdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmatgl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmed-imagetools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedaka = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedcat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmedmnist = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmegablocks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmegatron-energon = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmemos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmeshgpt-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmetatensor-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmflux = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmia-vgg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmiditok = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nminari = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nminicons = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nml2rt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlagents = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlbench-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlcroissant = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlpfile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmlx-whisper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmaction2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmengine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmengine-lite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmpose = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmmsegmentation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodeci-mdf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodel2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodelscope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmodelspec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonai-weekly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonotonic-alignment-search = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmonty = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmosaicml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmosaicml-streaming = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmoshi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmteb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmtmtrain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmulti-quantization = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nmyhand = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnGPT-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnaeural-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnapari = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnapatrackmater = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnara-wpe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnatten = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnbeats-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnebulae = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnemo-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneptune = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneptune-client = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnerfacc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnerfstudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnessai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnetcal = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneural-rag = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralforecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralnets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuralprophet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nneuspell = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnevergrad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnexfort = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnimblephysics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnirtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnkululeko = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnlptooltest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnAudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnodely = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnsight = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnnunetv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnoisereduce = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnonebot-plugin-nailongremove = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnowcasting-dataloader = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnowcasting-forecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnshtrainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnuwa-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnvflare = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nnvidia-modelopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nocf-datapipes = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nocnn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nogb = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nohmeow-blurr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nolive-ai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nomlt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nommlx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonediff = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonediffx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonnx2pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nonnx2torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopacus = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-clip-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-flamingo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopen-interpreter = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenbb-terminal-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenmim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenunmix = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-tokenizers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenvino-xai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopenwakeword = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nopt-einsum-fx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-habana = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-intel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-neuron = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptimum-quanto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptree = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna-dashboard = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noptuna-integration = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noracle-ads = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\norbit-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\notx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutetts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutlines = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\noutlines-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npaddlenlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npai-easycv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npandasai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npanns-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npatchwork-cli = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npeft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npegasuspy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npelutils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npenn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nperforatedai-freemium = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nperformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npetastorm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npfio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npgmpy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nphenolrs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nphobos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npi-zero-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npinecone-text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npiq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npix2tex = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npix2text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npnnx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npolicyengine-us-data = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npolyfuzz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npomegranate = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npositional-encodings = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nprefigure = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nproduct-key-memory = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nptflops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nptwt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npulser-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npunctuators = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npy2ls = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyabsa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n\"pyannote.audio\" = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyawd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyclarity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npycox = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyfemtet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyg-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npygrinder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyhealth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyhf = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyiqa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npykeen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npykeops = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npylance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npylineaGT = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npymanopt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npymde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npypots = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyqlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyqtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyro-ppl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npysentimiento = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyserini = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npysr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npythainlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npython-doctr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-fid = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-forecasting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-ignite = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-kinematics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-lightning-bolts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-metric-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-model-summary = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-msssim = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-pfn-extras = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-pretrained-bert = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-ranger = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-seed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-tabnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-tabular = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-toolbelt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-transformers-pvt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-triton-rocm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-warmup = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch-wavelets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch_optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorch_revgrad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorchcv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npytorchltr2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyvene = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\npyvespa = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqianfan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqibo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqiskit-machine-learning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nqtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nquanto = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nquick-anomaly-detector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision-pytorch-backend = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrastervision-pytorch-learner = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nray-lightning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrclip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrealesrgan = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrecbole = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrecommenders = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nredcat = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nreformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nregex-sampler = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nreplay-rec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrerankers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresearch-framework = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresemble-enhance = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nresnest = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrf-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrf-groundingdino = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrfconv = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrich-logger = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nring-attention-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrltrade-test = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrotary-embedding-torch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrsp-ml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nrust-circuit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns2fft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns3prl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ns3torchconnector = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsaferx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsafetensors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsagemaker-huggingface-inference-toolkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsagemaker-ssh-helper = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsalesforce-lavis = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsalesforce-merlion = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsamv2 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscib-metrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nscvi-tools = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsdmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsecretflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegment-anything-hq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegment-anything-py = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsegmentation-models-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nself-rewarding-lm-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsemantic-kernel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsemantic-router = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsenselab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsent2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsentence-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsequence-model-train = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nserotiny = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsevenn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsglang = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nshap = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilero-api-server = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilero-vad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsilicondiff-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsimclr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsimple-lama-inpainting = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsinabs = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsixdrepnet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskforecast = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nskt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsktime = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsktmls = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nslangtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmartnoise-synth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmashed = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmplx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmqtk-descriptors = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsmqtk-detection = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnntorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnorkel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsnowflake-ml-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nso-vits-svc-fork = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsonusai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsony-custom-layers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsotopia = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-curated-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-experimental = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-huggingface-pipelines = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspacy-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspan-marker = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspandrel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspandrel-extra-arches = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsparrow-python = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspatialdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspeechbrain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspeechtokenizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspikeinterface = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspikingjelly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotiflow = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotpython = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nspotriver = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsquirrel-core = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-baselines3 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-diffusion-sdkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstable-ts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanford-stk = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanfordnlp = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstanza = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstartorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstreamtasks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstruct-eqtable = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nstylegan2-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsupar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuper-gradients = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuper-image = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsuperlinked = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsupervisely = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsurya-ocr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsvdiff-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarm-models = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarmauri = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswarms-memory = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nswebench = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsyft = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsympytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsyne-tune = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nsynthcity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nt5 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntab-transformer-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntabpfn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaming-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaming-transformers-rom1504 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntaskwiz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntbparse = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntecton = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensor-parallel = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorcircuit-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensordict = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensordict-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntensorrt-llm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntexify = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntext2text = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntextattack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntfkit = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthepipe-api = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthinc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthingsvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthirdai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nthop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntianshou = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntidy3d = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntimesfm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntimm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntipo-kgen = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntmnt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntoad = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntomesd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntop2vec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-audiomentations = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-dct = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-delaunay = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-directml = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ema = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-encoding = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-fidelity = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-geometric = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-geopooling = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-harmonics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-kmeans = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-lr-finder = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-max-mem = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-npu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-optimi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-optimizer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ort = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-pitch-shift = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-ppr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-pruning = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-snippets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-stoi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-struct = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorch-tensorrt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchani = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchattacks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchaudio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchbiggraph = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcam = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcfm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchcrepe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdata = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdatasets-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdiffeq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchdyn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchestra = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorcheval = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorcheval-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchextractor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfcpe = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfun = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchfunc-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchgeo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchgeometry = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchio = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchjpeg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchlayers-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmeta = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchmocks = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpack = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpippy = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchpq = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchprofile = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchquantlib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrec-nightly-cpu = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrl = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchrl-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchscale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsde = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchserve = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchserve-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsnapshot-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsr = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchstain = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchsummaryX = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtext = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtnt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtnt-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchtyping = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchutil = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchvinecopulib = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchviz = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchx-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntorchxrayvision = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntotalspineseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntracebloc-package-dev = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntrainer = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-engine = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-lens = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformer-smaller-training-vocab = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransformers-domain-adaptation = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransfusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntransparent-background = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntreescope = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntrolo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntsai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntslearn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nttspod = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntxtai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\ntyro = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nu8darts = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nuhg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nuitestrunner-syberos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultimate-rvc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultralytics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nultralytics-thop = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunav = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunbabel-comet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunderthesea = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunfoldNd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunimernet = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunitorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunitxt = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunsloth = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunsloth-zoo = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunstructured = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nunstructured-inference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nutilsd = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nv-diffusion-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvIQA = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvectice = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvector-quantize-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvectorhub-nightly = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nversatile-audio-upscaler = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvertexai = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvesin = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvgg-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvideo-representations-extractor = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nviser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvision-datasets = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvisionmetrics = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvisu3d = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvit-pytorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nviturka-nn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvllm = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvllm-flash-attn = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvocos = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvollseg = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nvtorch = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwavmark = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwdoc = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisper-live = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisper-timestamped = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwhisperx = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwilds = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwordllama = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nworker-automate-hub = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nwxbtool = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nx-clip = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nx-transformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxaitk_saliency = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxformers = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxgrammar = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxinference = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nxtts-api-server = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolo-poser = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolov5 = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyolov7-package = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nyta-general-utils = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzensvi = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzetascale = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\nzuko = [{ index = \"pytorch-cpu\", marker = \"platform_system == 'Linux'\" }]\n","size_bytes":91022},"replit.md":{"content":"# Overview\n\nMedGemma AI Platform is a comprehensive medical AI system built with Streamlit that combines Retrieval-Augmented Generation (RAG), fine-tuning capabilities, and safety systems specifically designed for medical applications. The platform enables users to train and deploy medical AI models using the Gemma architecture while ensuring safety guardrails and proper citation handling for medical information.\n\nThe system provides a complete workflow from data ingestion and model fine-tuning to evaluation and deployment, with specialized components for medical document processing, safety validation, and model export capabilities.\n\n# User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n# System Architecture\n\n## Frontend Architecture\n- **Streamlit-based Web Interface**: Single-page application with sidebar navigation providing access to different modules including RAG system, fine-tuning, evaluation, safety checks, model export, and chat interface\n- **Session State Management**: Persistent state across components using Streamlit's session state for maintaining model instances, training status, and chat history\n- **Modular Component Design**: Each major functionality (RAG, fine-tuning, evaluation, etc.) is implemented as a separate component class for maintainability and separation of concerns\n\n## Backend Architecture\n- **Transformer-based Models**: Built around Hugging Face transformers with primary support for Google's Gemma models (2B and 7B variants)\n- **LoRA Fine-tuning**: Parameter-Efficient Fine-Tuning using Low-Rank Adaptation (LoRA) with configurable rank, alpha, and dropout parameters\n- **Mixed Precision Training**: Support for both FP16 and BF16 training with automatic device detection (CUDA, MPS, CPU)\n- **Memory Optimization**: 4-bit and 8-bit quantization support using BitsAndBytesConfig for resource-constrained environments\n\n## Data Storage Solutions\n- **FAISS Vector Database**: High-performance similarity search using Facebook AI Similarity Search for document retrieval\n- **Local File System**: Models, indices, and metadata stored locally with configurable paths\n- **Pickle Serialization**: Metadata and embeddings cached using Python pickle for fast loading\n- **Multiple Data Formats**: Support for TXT, JSON, CSV, and PDF document ingestion\n\n## Authentication and Authorization\n- **Environment-based Configuration**: Settings managed through environment variables with sensible defaults\n- **No Built-in Authentication**: Currently designed for single-user deployment without authentication layer\n\n## RAG System Architecture\n- **Sentence Transformers**: Uses all-mpnet-base-v2 model for generating document embeddings\n- **Document Processing Pipeline**: Automated extraction and chunking of medical documents with metadata preservation\n- **Retrieval Pipeline**: Top-K similarity search with configurable threshold filtering\n- **Context Injection**: Retrieved passages automatically prepended to model prompts for grounding\n\n## Safety and Compliance\n- **Medical Safety Rules**: Hardcoded safety patterns to prevent personalized medical advice, diagnosis, or treatment recommendations\n- **Citation Requirements**: Enforced citation formatting from retrieved sources\n- **Content Filtering**: Pattern-based blocking of potentially harmful medical content\n- **Emergency Response**: Automatic detection and appropriate responses for medical emergency queries\n\n## Model Training and Evaluation\n- **SFT Trainer Integration**: Supervised Fine-Tuning using TRL (Transformer Reinforcement Learning) library\n- **Comprehensive Metrics**: Exact Match, F1-score, BLEU, and ROUGE evaluation metrics\n- **Training Monitoring**: Real-time training progress with configurable logging and checkpointing\n- **Gradient Accumulation**: Support for effective large batch training on limited hardware\n\n## Model Export and Deployment\n- **Multiple Export Formats**: LoRA adapters, merged FP16/FP32 models, GGUF for llama.cpp, ONNX, and TensorRT formats\n- **Hugging Face Integration**: Direct model publishing to Hugging Face Hub\n- **Local Serving Options**: Support for various deployment scenarios from local inference to production serving\n\n# External Dependencies\n\n## Core ML Libraries\n- **Hugging Face Ecosystem**: transformers, peft, trl, datasets for model handling and training\n- **PyTorch**: Primary deep learning framework with CUDA support\n- **Sentence Transformers**: Document embedding generation for RAG system\n- **FAISS**: Facebook AI Similarity Search for efficient vector similarity search\n\n## Data Processing and Visualization\n- **Streamlit**: Web application framework for the user interface\n- **Pandas**: Data manipulation and analysis for training data processing\n- **NumPy**: Numerical computations and array operations\n- **Plotly**: Interactive visualizations for evaluation dashboards and metrics\n\n## Model Training and Optimization\n- **BitsAndBytesConfig**: 4-bit and 8-bit model quantization for memory efficiency\n- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning approach\n- **Mixed Precision Training**: Automatic Mixed Precision (AMP) for faster training\n\n## File Processing and Utilities\n- **Pathlib**: Modern path handling for cross-platform compatibility\n- **JSON/CSV Processing**: Built-in Python libraries for data format handling\n- **Pickle**: Object serialization for caching embeddings and metadata\n- **Regular Expressions**: Text processing and safety pattern matching\n\n## Optional Integrations\n- **Hugging Face Hub**: Model sharing and deployment platform\n- **PDF Processing**: Document extraction capabilities (implementation pending)\n- **ONNX Runtime**: Model optimization and deployment format\n- **TensorRT**: NVIDIA GPU optimization for inference acceleration\n\n## Development and Monitoring\n- **Python Logging**: Comprehensive logging throughout the application\n- **Environment Variables**: Configuration management via OS environment\n- **Git/Version Control**: Standard development workflow support","size_bytes":6003},"components/__init__.py":{"content":"","size_bytes":0},"components/chat_interface.py":{"content":"import streamlit as st\nimport time\nimport json\nimport logging\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom components.safety import SafetySystem\nfrom config.settings import Settings\nfrom utils.fallbacks import (\n    HAS_TORCH, HAS_TRANSFORMERS, HAS_NEMO,\n    get_model_and_tokenizer, \n    FallbackModel, FallbackTokenizer\n)\n\n# Import available packages - moved to avoid import errors\ntorch = None\npipeline = None\n\ntry:\n    if HAS_TORCH:\n        import torch\nexcept ImportError:\n    pass\n\ntry:\n    if HAS_TRANSFORMERS:\n        from transformers import pipeline\nexcept ImportError:\n    pass\n\nlogger = logging.getLogger(__name__)\n\nclass ChatInterface:\n    def __init__(self):\n        self.settings = Settings()\n        self.safety_system = SafetySystem()\n        self.model = None\n        self.tokenizer = None\n        self.chat_pipeline = None\n        \n    def load_model(self, model_name: str = None):\n        \"\"\"Unified model loading\"\"\"\n        if model_name is None:\n            model_name = self.settings.BASE_MODEL\n            \n        try:\n            if self.model is None:\n                from utils.model_utils import load_base_model_unified\n                self.model, self.tokenizer = load_base_model_unified(model_name)\n                logger.info(f\"Loaded {model_name} using unified loader\")\n                \n                # Additional setup for existing functionality\n                if hasattr(self.tokenizer, 'pad_token') and self.tokenizer.pad_token is None:\n                    self.tokenizer.pad_token = self.tokenizer.eos_token\n                \n                if HAS_TRANSFORMERS and not isinstance(self.model, FallbackModel):\n                    self.chat_pipeline = pipeline(\n                        \"text-generation\",\n                        model=self.model,\n                        tokenizer=self.tokenizer,\n                        torch_dtype=torch.float16 if HAS_TORCH else None,\n                        device_map=\"auto\"\n                    )\n            return True\n        except Exception as e:\n            logger.error(f\"Model loading failed: {e}\")\n            st.error(f\"Error loading model: {str(e)}\")\n            \n            # Fallback to original approach if unified loading fails\n            try:\n                if self.model is None or self.tokenizer is None:\n                    with st.spinner(f\"Loading model {model_name}...\"):\n                        \n                        # ---- NeMo loader ----\n                        if \"nvidia/\" in model_name and HAS_NEMO:\n                            import nemo.collections.nlp as nemo_nlp\n                            from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n                            self.model = MegatronGPTModel.from_pretrained(model_name)\n                            self.tokenizer = self.model.tokenizer\n                            logger.info(f\"Loaded NeMo model: {model_name}\")\n                        \n                        # ---- Default HuggingFace loader ----\n                        else:\n                            self.model, self.tokenizer = get_model_and_tokenizer(model_name)\n                            \n                            if hasattr(self.tokenizer, 'pad_token') and self.tokenizer.pad_token is None:\n                                self.tokenizer.pad_token = self.tokenizer.eos_token\n                            \n                            if HAS_TRANSFORMERS and not isinstance(self.model, FallbackModel):\n                                self.chat_pipeline = pipeline(\n                                    \"text-generation\",\n                                    model=self.model,\n                                    tokenizer=self.tokenizer,\n                                    torch_dtype=torch.float16 if HAS_TORCH else None,\n                                    device_map=\"auto\"\n                                )\n                return True\n            except Exception as e2:\n                logger.error(f\"Fallback model loading also failed: {e2}\")\n                self.model = FallbackModel(model_name)\n                self.tokenizer = FallbackTokenizer(model_name)\n                return True\n    \n    def generate_response(self, question: str, context: str = \"\", max_tokens: int = None) -> str:\n        \"\"\"Unified response generation\"\"\"\n        if max_tokens is None:\n            max_tokens = self.settings.MAX_NEW_TOKENS\n            \n        if not self.model or not self.tokenizer:\n            if not self.load_model():\n                return \"Error: Model not available. Please check model loading.\"\n        \n        # Use the unified model interface\n        if hasattr(self.model, 'generate'):\n            # Proper transformer model\n            return self._generate_with_transformers(question, context, max_tokens)\n        else:\n            # Fallback model\n            return self._generate_with_fallback(question, context)\n    \n    def _generate_with_transformers(self, question: str, context: str, max_tokens: int) -> str:\n        \"\"\"Generate with proper transformer model\"\"\"\n        # Safety check\n        is_safe, rule_triggered, safe_response = self.safety_system.check_input_safety(question)\n        if not is_safe:\n            return safe_response\n        \n        # Build messages with safety system\n        system_prompt = self.safety_system.get_safety_system_prompt()\n        \n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"} if context else \n            {\"role\": \"user\", \"content\": question}\n        ]\n        \n        # Apply chat template\n        if hasattr(self.tokenizer, 'apply_chat_template'):\n            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        else:\n            prompt = f\"{system_prompt}\\n\\nUser: {question}\"\n            if context:\n                prompt = f\"{system_prompt}\\n\\nContext: {context}\\n\\nQuestion: {question}\"\n        \n        # Generate\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n        \n        try:\n            if HAS_TORCH and hasattr(self.model, 'device'):\n                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n            \n            if HAS_TORCH:\n                with torch.inference_mode():\n                    outputs = self.model.generate(\n                        **inputs,\n                        max_new_tokens=max_tokens,\n                        temperature=self.settings.TEMPERATURE,\n                        do_sample=True,\n                        top_p=0.9,\n                        repetition_penalty=1.1,\n                        pad_token_id=self.tokenizer.pad_token_id,\n                        eos_token_id=self.tokenizer.eos_token_id\n                    )\n            else:\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_tokens,\n                    temperature=self.settings.TEMPERATURE,\n                    do_sample=True,\n                    top_p=0.9,\n                    repetition_penalty=1.1,\n                    pad_token_id=getattr(self.tokenizer, 'pad_token_id', 0),\n                    eos_token_id=getattr(self.tokenizer, 'eos_token_id', 1)\n                )\n            \n            # Decode\n            if hasattr(self.tokenizer, 'decode'):\n                if HAS_TORCH and hasattr(inputs, 'input_ids'):\n                    response = self.tokenizer.decode(\n                        outputs[0][inputs['input_ids'].shape[-1]:], \n                        skip_special_tokens=True\n                    ).strip()\n                else:\n                    response = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n            else:\n                response = f\"Generated response for: {question}\"\n            \n            return self.safety_system.sanitize_response(response)\n            \n        except Exception as e:\n            logger.error(f\"Error generating response: {str(e)}\")\n            return f\"Error generating response: {str(e)}\"\n    \n    def _generate_with_fallback(self, question: str, context: str) -> str:\n        \"\"\"Generate with fallback model\"\"\"\n        # Safety check\n        is_safe, rule_triggered, safe_response = self.safety_system.check_input_safety(question)\n        if not is_safe:\n            return safe_response\n            \n        # Handle NeMo models\n        if \"nvidia/\" in self.settings.BASE_MODEL and HAS_NEMO:\n            system_prompt = self.safety_system.get_safety_system_prompt()\n            prompt = f\"{system_prompt}\\n\\nUser: {question}\"\n            if context:\n                prompt = f\"{system_prompt}\\n\\nContext: {context}\\n\\nQuestion: {question}\"\n            \n            try:\n                response = self.model.complete(prompt, tokens_to_generate=self.settings.MAX_NEW_TOKENS)\n                if isinstance(response, list):\n                    response = response[0] if response else \"\"\n                return self.safety_system.sanitize_response(response)\n            except Exception as e:\n                logger.error(f\"NeMo generation error: {str(e)}\")\n        \n        # Handle other fallback models\n        return f\"I understand you're asking about: {question}. As an educational medical AI, I can provide general information, but please consult healthcare professionals for specific medical advice.\"\n    \n    def generate_response_with_rag(self, question: str, context: str, rag_system) -> Tuple[str, List[Dict[str, Any]]]:\n        \"\"\"Generate response using RAG system (faiss | nemo | hybrid supported)\"\"\"\n        try:\n            retrieved_docs = rag_system.retrieve_documents(\n                question + (\" \" + context if context else \"\"), \n                k=self.settings.RETRIEVAL_TOP_K\n            )\n            \n            if retrieved_docs:\n                retrieved_context = \"\\n\\n\".join([\n                    f\"[{i+1}] Source: {doc.get('source', 'unknown')}\\n{doc['text'][:500]}...\"\n                    for i, doc in enumerate(retrieved_docs)\n                ])\n                full_context = f\"{context}\\n\\nRetrieved Information:\\n{retrieved_context}\" if context else f\"Retrieved Information:\\n{retrieved_context}\"\n            else:\n                full_context = context\n                retrieved_docs = []\n            \n            response = self.generate_response(question, full_context)\n            \n            if retrieved_docs:\n                response = self.safety_system.enforce_citation_format(response, retrieved_docs)\n            \n            return response, retrieved_docs\n            \n        except Exception as e:\n            logger.error(f\"Error in RAG generation: {str(e)}\")\n            return f\"Error in RAG generation: {str(e)}\", []\n\n    \n    def format_chat_message(self, message: str, sender: str, timestamp: str = None) -> Dict[str, Any]:\n        \"\"\"Format a chat message for display\"\"\"\n        if timestamp is None:\n            timestamp = time.strftime(\"%H:%M:%S\")\n        \n        return {\n            \"sender\": sender,\n            \"message\": message,\n            \"timestamp\": timestamp\n        }\n    \n    def display_chat_history(self, chat_history: List[Dict[str, Any]]):\n        \"\"\"Display chat history in the interface\"\"\"\n        for msg in chat_history:\n            if msg[\"sender\"] == \"user\":\n                with st.chat_message(\"user\"):\n                    st.write(msg[\"message\"])\n                    st.caption(f\"Sent at {msg['timestamp']}\")\n            else:\n                with st.chat_message(\"assistant\"):\n                    st.markdown(msg[\"message\"])\n                    st.caption(f\"Generated at {msg['timestamp']}\")\n    \n    def display_retrieved_context(self, retrieved_docs: List[Dict[str, Any]]):\n        \"\"\"Display retrieved context in an expandable section\"\"\"\n        if not retrieved_docs:\n            return\n        \n        with st.expander(f\"📚 Retrieved Sources ({len(retrieved_docs)} documents)\", expanded=False):\n            for i, doc in enumerate(retrieved_docs, 1):\n                st.write(f\"**Source {i}: {doc.get('source', 'Unknown')}**\")\n                st.write(f\"Relevance Score: {doc.get('score', 0):.3f}\")\n                st.write(f\"Content: {doc['text'][:300]}...\")\n                if i < len(retrieved_docs):\n                    st.divider()\n    \n    def export_chat_session(self, chat_history: List[Dict[str, Any]]) -> str:\n        \"\"\"Export chat session as JSON\"\"\"\n        export_data = {\n            \"session_id\": f\"chat_{int(time.time())}\",\n            \"export_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"model_info\": {\n                \"base_model\": self.settings.BASE_MODEL,\n                \"rag_enabled\": st.session_state.rag_system is not None,\n                \"safety_enabled\": True\n            },\n            \"chat_history\": chat_history,\n            \"settings\": {\n                \"max_tokens\": self.settings.MAX_NEW_TOKENS,\n                \"temperature\": self.settings.TEMPERATURE,\n                \"retrieval_top_k\": self.settings.RETRIEVAL_TOP_K\n            }\n        }\n        \n        return json.dumps(export_data, indent=2)\n    \n    def clear_conversation(self):\n        \"\"\"Clear the current conversation\"\"\"\n        if 'chat_history' in st.session_state:\n            st.session_state.chat_history = []\n        if 'current_conversation' in st.session_state:\n            st.session_state.current_conversation = []\n    \n    def render(self):\n        \"\"\"Render the chat interface\"\"\"\n        st.header(\"💬 Medical AI Chat Interface\")\n        \n        # Initialize session state\n        if 'current_conversation' not in st.session_state:\n            st.session_state.current_conversation = []\n        \n        # Chat configuration\n        st.subheader(\"Chat Configuration\")\n        \n        \n        \n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            use_rag = st.checkbox(\n                \"Enable RAG\", \n                value=True, \n                disabled=not st.session_state.rag_system,\n                help=\"Use Retrieval-Augmented Generation for enhanced responses\"\n            )\n        \n        with col2:\n            max_tokens = st.slider(\n                \"Max Response Tokens\", \n                min_value=50, \n                max_value=1000, \n                value=self.settings.MAX_NEW_TOKENS,\n                step=50\n            )\n        \n        with col3:\n            temperature = st.slider(\n                \"Temperature\", \n                min_value=0.1, \n                max_value=1.0, \n                value=self.settings.TEMPERATURE,\n                step=0.1\n            )\n        \n        # Model status\n        if not self.model:\n            st.warning(\"⚠️ Model not loaded. Loading default model...\")\n            if st.button(\"Load Model\"):\n                if self.load_model():\n                    st.success(\"✅ Model loaded successfully!\")\n                    st.rerun()\n        else:\n            st.success(\"✅ Model ready for conversation\")\n        \n        # Main chat interface\n        st.subheader(\"Conversation\")\n        \n        # Display current conversation\n        chat_container = st.container()\n        with chat_container:\n            if st.session_state.current_conversation:\n                for msg in st.session_state.current_conversation:\n                    if msg[\"type\"] == \"user\":\n                        with st.chat_message(\"user\"):\n                            st.write(msg[\"content\"])\n                    elif msg[\"type\"] == \"assistant\":\n                        with st.chat_message(\"assistant\"):\n                            st.markdown(msg[\"content\"])\n                            if \"retrieved_docs\" in msg and msg[\"retrieved_docs\"]:\n                                self.display_retrieved_context(msg[\"retrieved_docs\"])\n            else:\n                st.info(\"👋 Welcome! Ask me any medical question for educational purposes.\")\n        \n        # Chat input - FIXED: Separate forms for input and buttons\n        with st.form(\"chat_input_form\", clear_on_submit=True):\n            user_input = st.text_area(\n                \"Your question:\",\n                placeholder=\"Ask a medical question...\",\n                height=120,  # Fixed: minimum 68px requirement\n                label_visibility=\"collapsed\"\n            )\n            \n            context_input = st.text_area(\n                \"Additional context (optional):\",\n                placeholder=\"Provide any relevant context or background information...\",\n                height=100,  # Fixed: minimum 68px requirement\n                label_visibility=\"collapsed\"\n            )\n            \n            # Submit button inside the form\n            submitted = st.form_submit_button(\"Send\", type=\"primary\", use_container_width=True)\n        \n        # Separate form for clear button to avoid conflicts\n        with st.form(\"chat_management_form\"):\n            clear_submitted = st.form_submit_button(\"Clear Chat\", use_container_width=True)\n        \n        # Process user input\n        if submitted and user_input.strip():\n            if not self.model:\n                st.error(\"Please load a model first.\")\n                return\n            \n            # Add user message to conversation\n            user_msg = {\n                \"type\": \"user\",\n                \"content\": user_input,\n                \"timestamp\": time.strftime(\"%H:%M:%S\")\n            }\n            st.session_state.current_conversation.append(user_msg)\n            \n            # Generate response\n            with st.spinner(\"Generating response...\"):\n                start_time = time.time()\n                \n                if use_rag and st.session_state.rag_system:\n                    response, retrieved_docs = self.generate_response_with_rag(\n                        user_input, context_input, st.session_state.rag_system\n                    )\n                else:\n                    response = self.generate_response(user_input, context_input, max_tokens)\n                    retrieved_docs = []\n                \n                response_time = time.time() - start_time\n            \n            # Add assistant response to conversation\n            assistant_msg = {\n                \"type\": \"assistant\",\n                \"content\": response,\n                \"timestamp\": time.strftime(\"%H:%M:%S\"),\n                \"response_time\": response_time,\n                \"retrieved_docs\": retrieved_docs\n            }\n            st.session_state.current_conversation.append(assistant_msg)\n            \n            # Update global chat history\n            if 'chat_history' not in st.session_state:\n                st.session_state.chat_history = []\n            \n            st.session_state.chat_history.extend([\n                self.format_chat_message(user_input, \"user\"),\n                self.format_chat_message(response, \"assistant\")\n            ])\n            \n            st.rerun()\n        \n        # Clear conversation\n        if clear_submitted:\n            self.clear_conversation()\n            st.rerun()\n        \n        # Conversation management\n        if st.session_state.current_conversation:\n            st.subheader(\"Conversation Management\")\n            \n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                if st.button(\"📊 Analyze Conversation\"):\n                    self.analyze_conversation()\n            \n            with col2:\n                conversation_data = self.export_chat_session(st.session_state.current_conversation)\n                st.download_button(\n                    label=\"💾 Save Conversation\",\n                    data=conversation_data,\n                    file_name=f\"medical_chat_{int(time.time())}.json\",\n                    mime=\"application/json\",\n                    use_container_width=True\n                )\n            \n            with col3:\n                if st.button(\"🔄 New Conversation\", use_container_width=True):\n                    self.clear_conversation()\n                    st.rerun()\n    \n    def analyze_conversation(self):\n        \"\"\"Analyze the current conversation for insights\"\"\"\n        if not st.session_state.current_conversation:\n            st.warning(\"No conversation to analyze.\")\n            return\n        \n        # Count messages and calculate metrics\n        user_messages = [msg for msg in st.session_state.current_conversation if msg[\"type\"] == \"user\"]\n        assistant_messages = [msg for msg in st.session_state.current_conversation if msg[\"type\"] == \"assistant\"]\n        \n        # Calculate average response time\n        response_times = [msg.get(\"response_time\", 0) for msg in assistant_messages]\n        avg_response_time = sum(response_times) / len(response_times) if response_times else 0\n        \n        # Count RAG usage\n        rag_used = sum(1 for msg in assistant_messages if msg.get(\"retrieved_docs\"))\n        \n        # Display analysis\n        with st.expander(\"📊 Conversation Analysis\", expanded=True):\n            col1, col2, col3, col4 = st.columns(4)\n            \n            with col1:\n                st.metric(\"Total Messages\", len(st.session_state.current_conversation))\n            \n            with col2:\n                st.metric(\"User Questions\", len(user_messages))\n            \n            with col3:\n                st.metric(\"Avg Response Time\", f\"{avg_response_time:.2f}s\")\n            \n            with col4:\n                st.metric(\"RAG Usage\", f\"{rag_used}/{len(assistant_messages)}\")\n            \n            # Safety analysis\n            safety_checks = 0\n            for msg in user_messages:\n                is_safe, _, _ = self.safety_system.check_input_safety(msg[\"content\"])\n                if not is_safe:\n                    safety_checks += 1\n            \n            if safety_checks > 0:\n                st.warning(f\"⚠️ {safety_checks} messages triggered safety guidelines.\")\n            else:\n                st.success(\"✅ All messages passed safety checks.\")","size_bytes":22061},"components/evaluation.py":{"content":"import streamlit as st\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport re\nfrom collections import Counter\nimport json\nimport logging\nfrom typing import List, Dict, Any, Tuple\nfrom utils.metrics import compute_exact_match, compute_f1_score, compute_bleu_score, compute_rouge_score\nfrom components.chat_interface import ChatInterface\nfrom config.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\nclass EvaluationDashboard:\n    def __init__(self):\n        self.settings = Settings()\n        self.chat_interface = ChatInterface()\n        \n    def normalize_answer(self, text: str) -> str:\n        \"\"\"Normalize answer for comparison\"\"\"\n        text = text.lower()\n        text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        text = ' '.join(text.split())\n        return text\n    \n    def compute_metrics(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n        \"\"\"Compute evaluation metrics\"\"\"\n        metrics = {\n            'exact_match': [],\n            'f1_score': [],\n            'bleu_score': [],\n            'rouge_l': []\n        }\n        \n        for pred, ref in zip(predictions, references):\n            # Exact Match\n            em = compute_exact_match(pred, ref)\n            metrics['exact_match'].append(em)\n            \n            # F1 Score\n            f1 = compute_f1_score(pred, ref)\n            metrics['f1_score'].append(f1)\n            \n            # BLEU Score\n            bleu = compute_bleu_score(pred, ref)\n            metrics['bleu_score'].append(bleu)\n            \n            # ROUGE-L Score\n            rouge = compute_rouge_score(pred, ref)\n            metrics['rouge_l'].append(rouge)\n        \n        # Compute averages\n        avg_metrics = {\n            'exact_match': np.mean(metrics['exact_match']) * 100,\n            'f1_score': np.mean(metrics['f1_score']) * 100,\n            'bleu_score': np.mean(metrics['bleu_score']) * 100,\n            'rouge_l': np.mean(metrics['rouge_l']) * 100\n        }\n        \n        return avg_metrics, metrics\n    \n    def compute_confidence_calibration(self, predictions: List[str], references: List[str], \n                                     confidences: List[float]) -> Dict[str, Any]:\n        \"\"\"Compute confidence calibration metrics\"\"\"\n        if not confidences:\n            return {}\n        \n        accuracies = [compute_exact_match(pred, ref) for pred, ref in zip(predictions, references)]\n        \n        # Bin predictions by confidence\n        bins = np.linspace(0, 1, 11)\n        bin_boundaries = list(zip(bins[:-1], bins[1:]))\n        \n        calibration_data = []\n        for low, high in bin_boundaries:\n            mask = (np.array(confidences) >= low) & (np.array(confidences) < high)\n            if np.sum(mask) > 0:\n                avg_confidence = np.mean(np.array(confidences)[mask])\n                avg_accuracy = np.mean(np.array(accuracies)[mask])\n                count = np.sum(mask)\n                \n                calibration_data.append({\n                    'bin_lower': low,\n                    'bin_upper': high,\n                    'avg_confidence': avg_confidence,\n                    'avg_accuracy': avg_accuracy,\n                    'count': count\n                })\n        \n        # Expected Calibration Error (ECE)\n        ece = 0\n        total_samples = len(predictions)\n        for data in calibration_data:\n            ece += (data['count'] / total_samples) * abs(data['avg_confidence'] - data['avg_accuracy'])\n        \n        return {\n            'calibration_data': calibration_data,\n            'ece': ece\n        }\n    \n    def run_evaluation(self, test_data: List[Dict[str, str]], model_name: str = \"current\") -> Dict[str, Any]:\n        \"\"\"Run comprehensive evaluation\"\"\"\n        predictions = []\n        references = []\n        confidences = []\n        response_times = []\n        retrieved_contexts = []\n        \n        progress_bar = st.progress(0)\n        status_text = st.empty()\n        \n        for i, example in enumerate(test_data):\n            progress = (i + 1) / len(test_data)\n            progress_bar.progress(progress)\n            status_text.text(f\"Evaluating: {i + 1}/{len(test_data)}\")\n            \n            question = example.get('question', example.get('input', ''))\n            reference = example.get('answer', example.get('output', example.get('reference', '')))\n            context = example.get('context', example.get('passage', ''))\n            \n            try:\n                # Get model prediction\n                import time\n                start_time = time.time()\n                \n                if st.session_state.rag_system:\n                    prediction, contexts = self.chat_interface.generate_response_with_rag(\n                        question, context, st.session_state.rag_system\n                    )\n                    retrieved_contexts.append(contexts)\n                else:\n                    prediction = self.chat_interface.generate_response(question, context)\n                    retrieved_contexts.append([])\n                \n                response_time = time.time() - start_time\n                \n                predictions.append(prediction)\n                references.append(reference)\n                response_times.append(response_time)\n                \n                # Placeholder confidence (would need proper implementation)\n                confidences.append(0.8)  # Mock confidence\n                \n            except Exception as e:\n                logger.error(f\"Error evaluating example {i}: {str(e)}\")\n                predictions.append(\"\")\n                references.append(reference)\n                response_times.append(0)\n                confidences.append(0)\n                retrieved_contexts.append([])\n        \n        progress_bar.empty()\n        status_text.empty()\n        \n        # Compute metrics\n        avg_metrics, detailed_metrics = self.compute_metrics(predictions, references)\n        calibration_metrics = self.compute_confidence_calibration(predictions, references, confidences)\n        \n        # Compile results\n        results = {\n            'model_name': model_name,\n            'num_examples': len(test_data),\n            'avg_metrics': avg_metrics,\n            'detailed_metrics': detailed_metrics,\n            'calibration_metrics': calibration_metrics,\n            'avg_response_time': np.mean(response_times),\n            'predictions': predictions,\n            'references': references,\n            'confidences': confidences,\n            'response_times': response_times,\n            'retrieved_contexts': retrieved_contexts,\n            'test_data': test_data\n        }\n        \n        return results\n    \n    def visualize_metrics(self, results: Dict[str, Any]):\n        \"\"\"Create visualizations for evaluation metrics\"\"\"\n        \n        # Metrics overview\n        st.subheader(\"📊 Metrics Overview\")\n        \n        col1, col2, col3, col4 = st.columns(4)\n        \n        with col1:\n            st.metric(\n                \"Exact Match\",\n                f\"{results['avg_metrics']['exact_match']:.1f}%\",\n                delta=None\n            )\n        \n        with col2:\n            st.metric(\n                \"F1 Score\",\n                f\"{results['avg_metrics']['f1_score']:.1f}%\",\n                delta=None\n            )\n        \n        with col3:\n            st.metric(\n                \"BLEU Score\",\n                f\"{results['avg_metrics']['bleu_score']:.1f}%\",\n                delta=None\n            )\n        \n        with col4:\n            st.metric(\n                \"ROUGE-L\",\n                f\"{results['avg_metrics']['rouge_l']:.1f}%\",\n                delta=None\n            )\n        \n        # Performance metrics\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.metric(\n                \"Avg Response Time\",\n                f\"{results['avg_response_time']:.2f}s\",\n                delta=None\n            )\n        \n        with col2:\n            if results['calibration_metrics']:\n                st.metric(\n                    \"Calibration Error (ECE)\",\n                    f\"{results['calibration_metrics']['ece']:.3f}\",\n                    delta=None\n                )\n        \n        # Detailed visualizations\n        tab1, tab2, tab3, tab4 = st.tabs([\"Score Distribution\", \"Calibration\", \"Performance Analysis\", \"Error Analysis\"])\n        \n        with tab1:\n            self.plot_score_distribution(results)\n        \n        with tab2:\n            self.plot_calibration(results)\n        \n        with tab3:\n            self.plot_performance_analysis(results)\n        \n        with tab4:\n            self.show_error_analysis(results)\n    \n    def plot_score_distribution(self, results: Dict[str, Any]):\n        \"\"\"Plot score distribution histograms\"\"\"\n        st.subheader(\"Score Distribution\")\n        \n        metrics_data = results['detailed_metrics']\n        \n        # Create subplots\n        fig = make_subplots(\n            rows=2, cols=2,\n            subplot_titles=('Exact Match', 'F1 Score', 'BLEU Score', 'ROUGE-L'),\n            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n        )\n        \n        # Plot histograms\n        metrics = ['exact_match', 'f1_score', 'bleu_score', 'rouge_l']\n        positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n        \n        for metric, (row, col) in zip(metrics, positions):\n            values = np.array(metrics_data[metric]) * 100 if metric == 'exact_match' else np.array(metrics_data[metric]) * 100\n            \n            fig.add_trace(\n                go.Histogram(\n                    x=values,\n                    name=metric.replace('_', ' ').title(),\n                    nbinsx=20,\n                    showlegend=False\n                ),\n                row=row, col=col\n            )\n        \n        fig.update_layout(height=600, title_text=\"Metric Score Distributions\")\n        st.plotly_chart(fig, use_container_width=True)\n    \n    def plot_calibration(self, results: Dict[str, Any]):\n        \"\"\"Plot calibration curve\"\"\"\n        st.subheader(\"Model Calibration\")\n        \n        if not results['calibration_metrics']:\n            st.warning(\"Calibration data not available.\")\n            return\n        \n        calibration_data = results['calibration_metrics']['calibration_data']\n        \n        if not calibration_data:\n            st.warning(\"Insufficient data for calibration analysis.\")\n            return\n        \n        # Prepare data\n        confidences = [d['avg_confidence'] for d in calibration_data]\n        accuracies = [d['avg_accuracy'] for d in calibration_data]\n        counts = [d['count'] for d in calibration_data]\n        \n        # Create calibration plot\n        fig = go.Figure()\n        \n        # Perfect calibration line\n        fig.add_trace(go.Scatter(\n            x=[0, 1],\n            y=[0, 1],\n            mode='lines',\n            name='Perfect Calibration',\n            line=dict(dash='dash', color='gray')\n        ))\n        \n        # Actual calibration\n        fig.add_trace(go.Scatter(\n            x=confidences,\n            y=accuracies,\n            mode='markers+lines',\n            name='Model Calibration',\n            marker=dict(\n                size=[c/2 for c in counts],  # Size proportional to count\n                sizemode='diameter',\n                sizeref=2.*max(counts)/(20.**2),\n                sizemin=4\n            ),\n            text=[f'Count: {c}' for c in counts],\n            hovertemplate='<b>Confidence:</b> %{x:.2f}<br><b>Accuracy:</b> %{y:.2f}<br>%{text}<extra></extra>'\n        ))\n        \n        fig.update_layout(\n            title='Confidence vs Accuracy (Reliability Diagram)',\n            xaxis_title='Mean Predicted Confidence',\n            yaxis_title='Mean Accuracy',\n            width=700,\n            height=500\n        )\n        \n        st.plotly_chart(fig, use_container_width=True)\n        \n        # ECE display\n        ece = results['calibration_metrics']['ece']\n        st.info(f\"**Expected Calibration Error (ECE):** {ece:.3f}\")\n        st.caption(\"Lower ECE indicates better calibration. ECE < 0.1 is generally considered well-calibrated.\")\n    \n    def plot_performance_analysis(self, results: Dict[str, Any]):\n        \"\"\"Plot performance analysis charts\"\"\"\n        st.subheader(\"Performance Analysis\")\n        \n        # Response time analysis\n        response_times = results['response_times']\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            # Response time histogram\n            fig = px.histogram(\n                x=response_times,\n                nbins=20,\n                title=\"Response Time Distribution\",\n                labels={'x': 'Response Time (seconds)', 'y': 'Count'}\n            )\n            st.plotly_chart(fig, use_container_width=True)\n        \n        with col2:\n            # Response time vs accuracy\n            accuracies = [1 if em else 0 for em in results['detailed_metrics']['exact_match']]\n            \n            fig = px.scatter(\n                x=response_times,\n                y=accuracies,\n                title=\"Response Time vs Accuracy\",\n                labels={'x': 'Response Time (seconds)', 'y': 'Accuracy (Exact Match)'},\n                trendline=\"ols\"\n            )\n            st.plotly_chart(fig, use_container_width=True)\n        \n        # Performance summary\n        st.subheader(\"Performance Summary\")\n        perf_col1, perf_col2, perf_col3 = st.columns(3)\n        \n        with perf_col1:\n            st.metric(\"Min Response Time\", f\"{min(response_times):.2f}s\")\n        \n        with perf_col2:\n            st.metric(\"Max Response Time\", f\"{max(response_times):.2f}s\")\n        \n        with perf_col3:\n            st.metric(\"Std Response Time\", f\"{np.std(response_times):.2f}s\")\n    \n    def show_error_analysis(self, results: Dict[str, Any]):\n        \"\"\"Show detailed error analysis\"\"\"\n        st.subheader(\"Error Analysis\")\n        \n        # Find examples with low scores\n        f1_scores = results['detailed_metrics']['f1_score']\n        em_scores = results['detailed_metrics']['exact_match']\n        \n        # Sort by F1 score (ascending) to show worst examples first\n        sorted_indices = np.argsort(f1_scores)\n        \n        # Show worst performing examples\n        st.subheader(\"Lowest Scoring Examples\")\n        \n        num_examples = min(10, len(sorted_indices))\n        \n        for i in range(num_examples):\n            idx = sorted_indices[i]\n            \n            with st.expander(f\"Example {idx + 1} (F1: {f1_scores[idx]:.2f}, EM: {em_scores[idx]:.0f})\"):\n                col1, col2 = st.columns(2)\n                \n                with col1:\n                    st.write(\"**Question:**\")\n                    st.write(results['test_data'][idx].get('question', 'N/A'))\n                    \n                    st.write(\"**Context:**\")\n                    st.write(results['test_data'][idx].get('context', 'N/A')[:200] + \"...\")\n                \n                with col2:\n                    st.write(\"**Reference Answer:**\")\n                    st.write(results['references'][idx])\n                    \n                    st.write(\"**Model Prediction:**\")\n                    st.write(results['predictions'][idx])\n                \n                # Show retrieved contexts if available\n                if results['retrieved_contexts'][idx]:\n                    st.write(\"**Retrieved Contexts:**\")\n                    for j, ctx in enumerate(results['retrieved_contexts'][idx][:2]):  # Show top 2\n                        st.write(f\"Context {j+1}: {ctx.get('text', '')[:150]}...\")\n        \n        # Error statistics\n        st.subheader(\"Error Statistics\")\n        \n        # Count different types of errors\n        zero_f1 = sum(1 for score in f1_scores if score == 0)\n        low_f1 = sum(1 for score in f1_scores if 0 < score < 0.5)\n        med_f1 = sum(1 for score in f1_scores if 0.5 <= score < 0.8)\n        high_f1 = sum(1 for score in f1_scores if score >= 0.8)\n        \n        error_stats = {\n            'No Match (F1=0)': zero_f1,\n            'Low Match (0<F1<0.5)': low_f1,\n            'Medium Match (0.5≤F1<0.8)': med_f1,\n            'High Match (F1≥0.8)': high_f1\n        }\n        \n        # Create pie chart\n        fig = px.pie(\n            values=list(error_stats.values()),\n            names=list(error_stats.keys()),\n            title=\"Distribution of F1 Score Ranges\"\n        )\n        st.plotly_chart(fig, use_container_width=True)\n    \n    def export_results(self, results: Dict[str, Any]):\n        \"\"\"Export evaluation results\"\"\"\n        export_data = {\n            'model_name': results['model_name'],\n            'evaluation_date': pd.Timestamp.now().isoformat(),\n            'metrics': results['avg_metrics'],\n            'num_examples': results['num_examples'],\n            'detailed_results': [\n                {\n                    'question': results['test_data'][i].get('question', ''),\n                    'reference': results['references'][i],\n                    'prediction': results['predictions'][i],\n                    'exact_match': results['detailed_metrics']['exact_match'][i],\n                    'f1_score': results['detailed_metrics']['f1_score'][i],\n                    'bleu_score': results['detailed_metrics']['bleu_score'][i],\n                    'rouge_l': results['detailed_metrics']['rouge_l'][i],\n                    'response_time': results['response_times'][i],\n                    'confidence': results['confidences'][i]\n                }\n                for i in range(len(results['predictions']))\n            ]\n        }\n        \n        return json.dumps(export_data, indent=2)\n    \n    def render(self):\n        \"\"\"Render the evaluation dashboard\"\"\"\n        st.header(\"📊 Evaluation Dashboard\")\n        \n        # Load test data\n        st.subheader(\"Test Data\")\n        \n        data_source = st.radio(\n            \"Select test data source:\",\n            [\"Upload Test File\", \"Use Manual Examples\", \"Generate Synthetic Data\"]\n        )\n        \n        test_data = []\n        \n        if data_source == \"Upload Test File\":\n            uploaded_file = st.file_uploader(\n                \"Upload test data\",\n                type=['json', 'csv', 'jsonl'],\n                help=\"Upload test data in JSON, CSV, or JSONL format with questions and reference answers\"\n            )\n            \n            if uploaded_file:\n                try:\n                    if uploaded_file.type == 'application/json':\n                        test_data = json.load(uploaded_file)\n                    elif uploaded_file.type == 'text/csv':\n                        df = pd.read_csv(uploaded_file)\n                        test_data = df.to_dict('records')\n                    \n                    st.success(f\"Loaded {len(test_data)} test examples\")\n                    \n                    # Show preview\n                    if st.checkbox(\"Show data preview\"):\n                        st.dataframe(pd.DataFrame(test_data[:5]))\n                        \n                except Exception as e:\n                    st.error(f\"Error loading test data: {str(e)}\")\n        \n        elif data_source == \"Use Manual Examples\":\n            if 'manual_test_data' not in st.session_state:\n                st.session_state.manual_test_data = []\n            \n            with st.form(\"manual_test_entry\"):\n                question = st.text_area(\"Test Question:\")\n                answer = st.text_area(\"Reference Answer:\")\n                context = st.text_area(\"Context (optional):\")\n                \n                if st.form_submit_button(\"Add Test Example\"):\n                    if question and answer:\n                        example = {\n                            \"question\": question,\n                            \"answer\": answer,\n                            \"context\": context if context else \"\"\n                        }\n                        st.session_state.manual_test_data.append(example)\n                        st.success(\"Test example added!\")\n                        st.rerun()\n            \n            if st.session_state.manual_test_data:\n                st.write(f\"Current test examples: {len(st.session_state.manual_test_data)}\")\n                test_data = st.session_state.manual_test_data\n        \n        elif data_source == \"Generate Synthetic Data\":\n            st.info(\"Synthetic data generation is not implemented. Please upload real test data.\")\n        \n        # Model selection\n        st.subheader(\"Model Selection\")\n        \n        model_options = [\"Current Loaded Model\"]\n        if st.session_state.model_trained:\n            model_options.append(\"Fine-tuned Model\")\n        \n        selected_model = st.selectbox(\"Select model to evaluate:\", model_options)\n        \n        # Evaluation settings\n        st.subheader(\"Evaluation Settings\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            include_rag = st.checkbox(\n                \"Include RAG Retrieval\", \n                value=True, \n                disabled=not st.session_state.rag_system,\n                help=\"Use RAG system for enhanced responses\"\n            )\n        \n        with col2:\n            batch_size = st.selectbox(\"Evaluation Batch Size\", [1, 5, 10], index=1)\n        \n        # Run evaluation\n        if test_data and st.button(\"Run Evaluation\", type=\"primary\"):\n            with st.spinner(\"Running evaluation...\"):\n                results = self.run_evaluation(test_data, selected_model)\n                st.session_state.evaluation_results = results\n            \n            st.success(\"Evaluation completed!\")\n        \n        # Display results\n        if 'evaluation_results' in st.session_state:\n            results = st.session_state.evaluation_results\n            \n            st.markdown(\"---\")\n            st.header(\"Evaluation Results\")\n            \n            # Visualize metrics\n            self.visualize_metrics(results)\n            \n            # Export results\n            st.subheader(\"Export Results\")\n            export_data = self.export_results(results)\n            \n            st.download_button(\n                label=\"Download Evaluation Report\",\n                data=export_data,\n                file_name=f\"evaluation_report_{results['model_name']}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json\",\n                mime=\"application/json\"\n            )\n","size_bytes":22602},"components/fine_tuning.py":{"content":"import streamlit as st\nimport os\nimport json\nimport pandas as pd\nfrom pathlib import Path\nfrom typing import Dict, Any, List\nimport logging\nfrom utils.data_processing import prepare_training_data\nfrom config.settings import Settings\nfrom utils.fallbacks import (\n    HAS_TORCH, HAS_TRANSFORMERS, HAS_PEFT, HAS_TRL, HAS_DATASETS,\n    get_model_and_tokenizer, FallbackModel, FallbackTokenizer\n)\n\n# Import available packages - moved to avoid import errors\ntorch = None\nBitsAndBytesConfig = None\nTrainingArguments = None\nLoraConfig = None\nget_peft_model = None\nTaskType = None\nSFTTrainer = None\nSFTConfig = None\nDataset = None\n\ntry:\n    if HAS_TORCH:\n        import torch\nexcept ImportError:\n    pass\n\ntry:\n    if HAS_TRANSFORMERS:\n        from transformers import BitsAndBytesConfig, TrainingArguments\nexcept ImportError:\n    pass\n\ntry:\n    if HAS_PEFT:\n        from peft import LoraConfig, get_peft_model, TaskType\nexcept ImportError:\n    pass\n\ntry:\n    if HAS_TRL:\n        from trl import SFTTrainer, SFTConfig\nexcept ImportError:\n    pass\n\ntry:\n    if HAS_DATASETS:\n        from datasets import Dataset\nexcept ImportError:\n    pass\n\nlogger = logging.getLogger(__name__)\n\nclass FineTuningInterface:\n    def __init__(self):\n        self.settings = Settings()\n        self.model = None\n        self.tokenizer = None\n        self.trainer = None\n        \n    def load_model(self, model_name: str = None):\n        \"\"\"Load base model and tokenizer\"\"\"\n        if model_name is None:\n            model_name = self.settings.BASE_MODEL\n            \n        try:\n            with st.spinner(f\"Loading model {model_name}...\"):\n                self.model, self.tokenizer = get_model_and_tokenizer(model_name)\n            return True\n        except Exception as e:\n            logger.error(f\"Error loading model: {str(e)}\")\n            st.error(f\"Error loading model: {str(e)}\")\n            return False\n    \n    def create_lora_config(self, r: int, alpha: int, dropout: float, target_modules: List[str]):\n        \"\"\"Create LoRA configuration\"\"\"\n        if HAS_PEFT:\n            return LoraConfig(\n                r=r,\n                lora_alpha=alpha,\n                target_modules=target_modules,\n                lora_dropout=dropout,\n                bias=\"none\",\n                task_type=TaskType.CAUSAL_LM,\n            )\n        else:\n            # Return a simple config dict for fallback\n            return {\n                \"r\": r,\n                \"lora_alpha\": alpha,\n                \"target_modules\": target_modules,\n                \"lora_dropout\": dropout,\n                \"bias\": \"none\",\n                \"task_type\": \"CAUSAL_LM\",\n            }\n    \n    def prepare_dataset(self, data: List[Dict[str, str]], tokenizer):\n        \"\"\"Prepare dataset for training\"\"\"\n        def format_instruction(example):\n            if \"instruction\" in example and \"response\" in example:\n                text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n            elif \"question\" in example and \"answer\" in example:\n                text = f\"### Question:\\n{example['question']}\\n\\n### Answer:\\n{example['answer']}\"\n            elif \"input\" in example and \"output\" in example:\n                text = f\"### Input:\\n{example['input']}\\n\\n### Output:\\n{example['output']}\"\n            else:\n                text = example.get(\"text\", str(example))\n            \n            return {\"text\": text}\n        \n        if HAS_DATASETS:\n            dataset = Dataset.from_list(data)\n            dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n            return dataset\n        else:\n            # Simple fallback dataset\n            formatted_data = [format_instruction(item) for item in data]\n            return formatted_data\n    \n    def start_training(self, train_data, eval_data, training_config, lora_config):\n        \"\"\"Start the fine-tuning process\"\"\"\n        try:\n            if not HAS_PEFT or not HAS_TRL:\n                st.warning(\"⚠️ Advanced training features not available. Running in demo mode.\")\n                \n                # Simulate training for demo\n                import time\n                for i in range(5):\n                    time.sleep(0.5)\n                    st.session_state.training_progress = (i + 1) / 5\n                \n                # Create dummy trainer for demo\n                class DemoTrainer:\n                    def __init__(self):\n                        self.state = type('State', (), {\n                            'log_history': [{'train_loss': 0.5, 'step': 100, 'learning_rate': 2e-5}],\n                            'global_step': 100,\n                            'max_steps': 100\n                        })()\n                    \n                    def add_callback(self, callback):\n                        pass\n                    \n                    def train(self):\n                        st.info(\"Demo training completed successfully!\")\n                    \n                    def save_model(self):\n                        os.makedirs(training_config[\"output_dir\"], exist_ok=True)\n                        # Save dummy model files\n                        with open(f\"{training_config['output_dir']}/training_completed.txt\", \"w\") as f:\n                            f.write(\"Demo training completed\")\n                \n                self.trainer = DemoTrainer()\n                return True\n            \n            # Apply LoRA to model\n            if HAS_PEFT:\n                self.model = get_peft_model(self.model, lora_config)\n            \n            # Prepare datasets\n            train_dataset = self.prepare_dataset(train_data, self.tokenizer)\n            eval_dataset = self.prepare_dataset(eval_data, self.tokenizer) if eval_data else None\n            \n            # Create SFT config\n            if HAS_TRL:\n                sft_config = SFTConfig(\n                    output_dir=training_config[\"output_dir\"],\n                    per_device_train_batch_size=training_config[\"batch_size\"],\n                    gradient_accumulation_steps=training_config[\"gradient_accumulation_steps\"],\n                    num_train_epochs=training_config[\"num_epochs\"],\n                    learning_rate=training_config[\"learning_rate\"],\n                    weight_decay=training_config[\"weight_decay\"],\n                    logging_steps=training_config[\"logging_steps\"],\n                    save_steps=training_config[\"save_steps\"],\n                    save_total_limit=training_config[\"save_total_limit\"],\n                    fp16=training_config[\"fp16\"],\n                    bf16=training_config[\"bf16\"],\n                    packing=training_config[\"packing\"],\n                    dataset_text_field=\"text\",\n                    max_seq_length=training_config[\"max_seq_length\"],\n                    report_to=[\"tensorboard\"] if training_config[\"use_tensorboard\"] else [],\n                    push_to_hub=training_config[\"push_to_hub\"],\n                    hub_model_id=training_config.get(\"hub_model_id\"),\n                    dataset_num_proc=4,\n                    remove_unused_columns=False,\n                )\n                \n                # Create trainer\n                self.trainer = SFTTrainer(\n                    model=self.model,\n                    tokenizer=self.tokenizer,\n                    args=sft_config,\n                    train_dataset=train_dataset,\n                    eval_dataset=eval_dataset,\n                )\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error setting up training: {str(e)}\")\n            st.error(f\"Error setting up training: {str(e)}\")\n            return False\n    \n    def render(self):\n        \"\"\"Render the fine-tuning interface\"\"\"\n        st.header(\"🎯 Model Fine-tuning\")\n        \n        # Model selection and loading\n        st.subheader(\"Model Configuration\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            model_name = st.selectbox(\n                \"Base Model\",\n                [ \n                    \"microsoft/DialoGPT-medium\",\n                    \"microsoft/BioGPT\",\n                    \"custom\"\n                ],\n                index=0\n            )\n            \n            if model_name == \"custom\":\n                model_name = st.text_input(\"Custom Model Name/Path:\")\n        \n        with col2:\n            if st.button(\"Load Model\", type=\"primary\"):\n                if self.load_model(model_name):\n                    st.success(f\"Successfully loaded {model_name}\")\n                    st.session_state.current_model = model_name\n        \n        if self.model is None:\n            st.warning(\"Please load a model before proceeding with fine-tuning.\")\n            return\n        \n        # LoRA Configuration\n        st.subheader(\"LoRA Configuration\")\n        \n        col1, col2, col3, col4 = st.columns(4)\n        \n        with col1:\n            lora_r = st.slider(\"LoRA Rank (r)\", min_value=8, max_value=256, value=64, step=8)\n        \n        with col2:\n            lora_alpha = st.slider(\"LoRA Alpha\", min_value=8, max_value=512, value=128, step=8)\n        \n        with col3:\n            lora_dropout = st.slider(\"LoRA Dropout\", min_value=0.0, max_value=0.5, value=0.1, step=0.05)\n        \n        with col4:\n            target_modules = st.multiselect(\n                \"Target Modules\",\n                [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n                default=[\"q_proj\", \"v_proj\"]\n            )\n        \n        # Training Data Upload\n        st.subheader(\"Training Data\")\n        \n        data_source = st.radio(\n            \"Data Source\",\n            [\"Upload File\", \"Manual Entry\", \"Use Sample Data\"]\n        )\n        \n        train_data = []\n        eval_data = []\n        \n        if data_source == \"Upload File\":\n            uploaded_file = st.file_uploader(\n                \"Upload Training Data\",\n                type=['json', 'csv', 'jsonl'],\n                help=\"Upload your medical training data in JSON, CSV, or JSONL format\"\n            )\n            \n            if uploaded_file:\n                try:\n                    train_data, eval_data = prepare_training_data(uploaded_file)\n                    st.success(f\"Loaded {len(train_data)} training examples and {len(eval_data)} evaluation examples\")\n                except Exception as e:\n                    st.error(f\"Error processing file: {str(e)}\")\n        \n        elif data_source == \"Manual Entry\":\n            st.write(\"Enter training examples manually:\")\n            \n            with st.form(\"manual_data_entry\"):\n                question = st.text_area(\"Medical Question:\", height=100)\n                answer = st.text_area(\"Expected Answer:\", height=150)\n                context = st.text_area(\"Additional Context (optional):\", height=100)\n                \n                if st.form_submit_button(\"Add Example\"):\n                    if question and answer:\n                        example = {\n                            \"question\": question,\n                            \"answer\": answer,\n                            \"context\": context if context else \"\"\n                        }\n                        \n                        if 'manual_training_data' not in st.session_state:\n                            st.session_state.manual_training_data = []\n                        \n                        st.session_state.manual_training_data.append(example)\n                        st.success(\"Example added!\")\n                        st.rerun()\n            \n            if 'manual_training_data' in st.session_state and st.session_state.manual_training_data:\n                st.write(f\"Current examples: {len(st.session_state.manual_training_data)}\")\n                \n                if st.button(\"Use Manual Data for Training\"):\n                    train_data = st.session_state.manual_training_data\n                    # Split 80/20 for train/eval\n                    split_idx = int(0.8 * len(train_data))\n                    eval_data = train_data[split_idx:]\n                    train_data = train_data[:split_idx]\n        \n        elif data_source == \"Use Sample Data\":\n            st.info(\"This would normally load sample medical training data. Please upload your own data for production use.\")\n        \n        # Training Configuration\n        st.subheader(\"Training Configuration\")\n        \n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            batch_size = st.selectbox(\"Batch Size\", [1, 2, 4, 8], index=1)\n            gradient_accumulation_steps = st.selectbox(\"Gradient Accumulation Steps\", [1, 2, 4, 8], index=1)\n            num_epochs = st.slider(\"Number of Epochs\", min_value=1, max_value=10, value=3)\n        \n        with col2:\n            learning_rate = st.selectbox(\n                \"Learning Rate\",\n                [1e-5, 2e-5, 5e-5, 1e-4, 2e-4],\n                index=1,\n                format_func=lambda x: f\"{x:.0e}\"\n            )\n            weight_decay = st.slider(\"Weight Decay\", min_value=0.0, max_value=0.1, value=0.01, step=0.01)\n            max_seq_length = st.selectbox(\"Max Sequence Length\", [512, 1024, 2048], index=1)\n        \n        with col3:\n            fp16 = st.checkbox(\"FP16 Training\", value=True)\n            bf16 = st.checkbox(\"BF16 Training\", value=False)\n            packing = st.checkbox(\"Enable Packing\", value=True)\n            use_tensorboard = st.checkbox(\"Use Tensorboard\", value=True)\n        \n        # Advanced settings\n        with st.expander(\"Advanced Settings\"):\n            output_dir = st.text_input(\"Output Directory\", value=\"./fine_tuned_model\")\n            logging_steps = st.number_input(\"Logging Steps\", min_value=1, value=10)\n            save_steps = st.number_input(\"Save Steps\", min_value=10, value=100)\n            save_total_limit = st.number_input(\"Save Total Limit\", min_value=1, value=3)\n            push_to_hub = st.checkbox(\"Push to Hugging Face Hub\")\n            \n            if push_to_hub:\n                hub_model_id = st.text_input(\"Hub Model ID (username/model-name):\")\n            else:\n                hub_model_id = None\n        \n        # Training execution\n        st.subheader(\"Training Execution\")\n        \n        if not train_data:\n            st.warning(\"Please prepare training data before starting training.\")\n            return\n        \n        # Display training summary\n        with st.expander(\"Training Summary\"):\n            training_config = {\n                \"output_dir\": output_dir,\n                \"batch_size\": batch_size,\n                \"gradient_accumulation_steps\": gradient_accumulation_steps,\n                \"num_epochs\": num_epochs,\n                \"learning_rate\": learning_rate,\n                \"weight_decay\": weight_decay,\n                \"logging_steps\": logging_steps,\n                \"save_steps\": save_steps,\n                \"save_total_limit\": save_total_limit,\n                \"fp16\": fp16,\n                \"bf16\": bf16,\n                \"packing\": packing,\n                \"max_seq_length\": max_seq_length,\n                \"use_tensorboard\": use_tensorboard,\n                \"push_to_hub\": push_to_hub,\n                \"hub_model_id\": hub_model_id,\n            }\n            \n            lora_config = self.create_lora_config(lora_r, lora_alpha, lora_dropout, target_modules)\n            \n            st.json({\n                \"model\": model_name,\n                \"training_examples\": len(train_data),\n                \"eval_examples\": len(eval_data),\n                \"lora_config\": {\n                    \"r\": lora_r,\n                    \"alpha\": lora_alpha,\n                    \"dropout\": lora_dropout,\n                    \"target_modules\": target_modules\n                },\n                \"training_config\": training_config\n            })\n        \n        # Start training button\n        if st.button(\"Start Fine-tuning\", type=\"primary\"):\n            if self.start_training(train_data, eval_data, training_config, lora_config):\n                st.success(\"Training setup completed! Starting training...\")\n                \n                # Create progress containers\n                progress_container = st.container()\n                log_container = st.container()\n                \n                with progress_container:\n                    progress_bar = st.progress(0)\n                    status_text = st.empty()\n                \n                with log_container:\n                    log_expander = st.expander(\"Training Logs\", expanded=True)\n                    log_text = log_expander.empty()\n                \n                try:\n                    # Start training with progress tracking\n                    class ProgressCallback:\n                        def __init__(self, progress_bar, status_text, log_text):\n                            self.progress_bar = progress_bar\n                            self.status_text = status_text\n                            self.log_text = log_text\n                            self.logs = []\n                        \n                        def on_log(self, args, state, control, model=None, **kwargs):\n                            if state.log_history:\n                                latest_log = state.log_history[-1]\n                                progress = state.global_step / state.max_steps if state.max_steps else 0\n                                \n                                self.progress_bar.progress(progress)\n                                self.status_text.text(f\"Step {state.global_step}/{state.max_steps} | Loss: {latest_log.get('train_loss', 'N/A'):.4f}\")\n                                \n                                self.logs.append(f\"Step {state.global_step}: {latest_log}\")\n                                self.log_text.text(\"\\n\".join(self.logs[-10:]))  # Show last 10 logs\n                    \n                    # Add callback to trainer\n                    callback = ProgressCallback(progress_bar, status_text, log_text)\n                    self.trainer.add_callback(callback)\n                    \n                    # Train the model\n                    self.trainer.train()\n                    \n                    # Save the model\n                    self.trainer.save_model()\n                    \n                    st.success(\"Fine-tuning completed successfully!\")\n                    st.session_state.model_trained = True\n                    \n                    # Display final metrics\n                    if self.trainer.state.log_history:\n                        final_metrics = self.trainer.state.log_history[-1]\n                        st.subheader(\"Final Training Metrics\")\n                        \n                        metrics_col1, metrics_col2, metrics_col3 = st.columns(3)\n                        \n                        with metrics_col1:\n                            st.metric(\"Final Loss\", f\"{final_metrics.get('train_loss', 0):.4f}\")\n                        \n                        with metrics_col2:\n                            st.metric(\"Steps Completed\", final_metrics.get('step', 0))\n                        \n                        with metrics_col3:\n                            st.metric(\"Learning Rate\", f\"{final_metrics.get('learning_rate', 0):.2e}\")\n                \n                except Exception as e:\n                    st.error(f\"Training failed: {str(e)}\")\n                    logger.error(f\"Training error: {str(e)}\")\n        \n        # Model management\n        st.subheader(\"Model Management\")\n        \n        if os.path.exists(output_dir):\n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                if st.button(\"Load Trained Model\"):\n                    try:\n                        # Load the fine-tuned model\n                        st.success(\"Fine-tuned model loaded successfully!\")\n                        st.session_state.model_trained = True\n                    except Exception as e:\n                        st.error(f\"Error loading model: {str(e)}\")\n            \n            with col2:\n                if st.button(\"Test Model\"):\n                    st.info(\"Navigate to the Chat Interface to test your fine-tuned model.\")\n            \n            with col3:\n                if st.button(\"Export Model\"):\n                    st.info(\"Navigate to the Model Export section to export your model.\")\n","size_bytes":20343},"components/model_export.py":{"content":"import streamlit as st\nimport os\nimport json\nimport shutil\nimport subprocess\nfrom pathlib import Path\nimport logging\nfrom typing import Dict, Any, Optional\nfrom config.settings import Settings\nfrom utils.fallbacks import HAS_TORCH, HAS_TRANSFORMERS\n\n# Import available packages\ntorch = None\nAutoTokenizer = None\nAutoModelForCausalLM = None\nHfApi = None\ncreate_repo = None\n\ntry:\n    if HAS_TORCH:\n        import torch\nexcept ImportError:\n    pass\n\ntry:\n    if HAS_TRANSFORMERS:\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        from huggingface_hub import HfApi, create_repo\nexcept ImportError:\n    pass\n\nlogger = logging.getLogger(__name__)\n\nclass ModelExport:\n    def __init__(self):\n        self.settings = Settings()\n        self.export_formats = {\n            \"lora_adapters\": \"LoRA Adapter Weights\",\n            \"merged_fp16\": \"Merged FP16 Model\", \n            \"merged_fp32\": \"Merged FP32 Model\",\n            \"gguf\": \"GGUF Format (llama.cpp)\",\n            \"onnx\": \"ONNX Format\",\n            \"tensorrt\": \"TensorRT Optimized\"\n        }\n        \n    def export_lora_adapters(self, model_path: str, output_path: str) -> bool:\n        \"\"\"Export LoRA adapter weights only\"\"\"\n        try:\n            # LoRA adapters are typically already saved during training\n            if os.path.exists(model_path):\n                if not os.path.exists(output_path):\n                    os.makedirs(output_path)\n                \n                # Copy adapter files\n                adapter_files = [\"adapter_config.json\", \"adapter_model.bin\", \"adapter_model.safetensors\"]\n                \n                for file in adapter_files:\n                    src = os.path.join(model_path, file)\n                    if os.path.exists(src):\n                        dst = os.path.join(output_path, file)\n                        shutil.copy2(src, dst)\n                \n                # Also copy tokenizer files\n                tokenizer_files = [\"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\", \"vocab.txt\"]\n                \n                for file in tokenizer_files:\n                    src = os.path.join(model_path, file)\n                    if os.path.exists(src):\n                        dst = os.path.join(output_path, file)\n                        shutil.copy2(src, dst)\n                \n                return True\n            return False\n            \n        except Exception as e:\n            logger.error(f\"Error exporting LoRA adapters: {str(e)}\")\n            return False\n    \n    def export_merged_model(self, model_path: str, output_path: str, precision: str = \"fp16\") -> bool:\n        \"\"\"Export merged model with base weights + LoRA adapters\"\"\"\n        try:\n            from peft import PeftModel, PeftConfig\n            \n            # Load the configuration\n            peft_config = PeftConfig.from_pretrained(model_path)\n            \n            # Load base model\n            base_model = AutoModelForCausalLM.from_pretrained(\n                peft_config.base_model_name_or_path,\n                torch_dtype=torch.float16 if precision == \"fp16\" else torch.float32,\n                device_map=\"auto\"\n            )\n            \n            # Load LoRA model\n            model = PeftModel.from_pretrained(base_model, model_path)\n            \n            # Merge and unload\n            merged_model = model.merge_and_unload()\n            \n            # Save merged model\n            merged_model.save_pretrained(\n                output_path,\n                safe_serialization=True,\n                max_shard_size=\"2GB\"\n            )\n            \n            # Save tokenizer\n            tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n            tokenizer.save_pretrained(output_path)\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error exporting merged model: {str(e)}\")\n            return False\n    \n    def export_to_gguf(self, model_path: str, output_path: str, quantization: str = \"q4_0\") -> bool:\n        \"\"\"Export model to GGUF format for llama.cpp\"\"\"\n        try:\n            # This requires llama.cpp tools to be installed\n            convert_script = \"convert.py\"  # from llama.cpp\n            \n            if not shutil.which(\"python\") or not os.path.exists(convert_script):\n                st.warning(\"llama.cpp conversion tools not found. Please install llama.cpp and ensure convert.py is available.\")\n                return False\n            \n            # Convert to GGUF\n            cmd = [\n                \"python\", convert_script,\n                model_path,\n                \"--outfile\", output_path,\n                \"--outtype\", quantization\n            ]\n            \n            result = subprocess.run(cmd, capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                return True\n            else:\n                logger.error(f\"GGUF conversion failed: {result.stderr}\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Error exporting to GGUF: {str(e)}\")\n            return False\n    \n    def export_to_onnx(self, model_path: str, output_path: str) -> bool:\n        \"\"\"Export model to ONNX format\"\"\"\n        try:\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            import torch\n            \n            # Load model and tokenizer\n            model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32)\n            tokenizer = AutoTokenizer.from_pretrained(model_path)\n            \n            # Create dummy input\n            dummy_input = tokenizer(\"Hello world\", return_tensors=\"pt\")\n            \n            # Export to ONNX\n            torch.onnx.export(\n                model,\n                (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n                output_path,\n                input_names=[\"input_ids\", \"attention_mask\"],\n                output_names=[\"logits\"],\n                dynamic_axes={\n                    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n                    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n                    \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n                },\n                opset_version=11\n            )\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error exporting to ONNX: {str(e)}\")\n            return False\n    \n    def create_model_card(self, model_info: Dict[str, Any]) -> str:\n        \"\"\"Create a model card with metadata\"\"\"\n        card_content = f\"\"\"\n    # {model_info.get('model_name', 'Medical AI Model')}\n\n    ## Model Description\n\n    This is a fine-tuned medical AI model based on {model_info.get('base_model', 'Unknown')} for educational purposes.\n\n    ## Training Details\n\n    - **Base Model**: {model_info.get('base_model', 'Unknown')}\n    - **Training Data**: {model_info.get('training_examples', 0)} examples\n    - **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\n    - **Training Date**: {model_info.get('training_date', 'Unknown')}\n\n    ## Performance Metrics\n\n    {self.format_metrics(model_info.get('metrics', {}))}\n\n    ## Usage\n\n    This model is intended for educational purposes only and should not be used for actual medical diagnosis or treatment recommendations.\n\n    ```python\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    from peft import PeftModel\n\n    # Load the model\n    base_model = AutoModelForCausalLM.from_pretrained(\"{model_info.get('base_model', '')}\")\n    model = PeftModel.from_pretrained(base_model, \"path/to/adapter\")\n    tokenizer = AutoTokenizer.from_pretrained(\"{model_info.get('base_model', '')}\")\n\n    # Generate response\n    inputs = tokenizer(\"Your medical question here\", return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=256)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    ```\n\n    ## Safety Considerations\n\n    ⚠️ **Important**: This model is for educational purposes only. Do not use for actual medical diagnosis or treatment decisions.\n\n    ## License\n\n    Please refer to the base model's license for usage terms.\n        \"\"\"\n        return card_content.strip()\n    \n    def format_metrics(self, metrics: Dict[str, Any]) -> str:\n        \"\"\"Format metrics for model card\"\"\"\n        if not metrics:\n            return \"No evaluation metrics available.\"\n        \n        formatted = []\n        for metric_name, value in metrics.items():\n            if isinstance(value, float):\n                formatted.append(f\"- **{metric_name.replace('_', ' ').title()}**: {value:.3f}\")\n            else:\n                formatted.append(f\"- **{metric_name.replace('_', ' ').title()}**: {value}\")\n        \n        return \"\\n\".join(formatted)\n    \n    def render(self):\n        \"\"\"Render the model export interface\"\"\"\n        st.header(\"📦 Model Export & Deployment\")\n        \n        # Check if model exists\n        if not st.session_state.model_trained:\n            st.warning(\"⚠️ No trained model available. Please complete fine-tuning first.\")\n            return\n        \n        st.subheader(\"Export Options\")\n        \n        # Export format selection\n        export_format = st.selectbox(\n            \"Select Export Format\",\n            list(self.export_formats.keys()),\n            format_func=lambda x: self.export_formats[x]\n        )\n        \n        # Export settings\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            model_path = st.text_input(\n                \"Source Model Path\", \n                value=\"./fine_tuned_model\",\n                help=\"Path to the trained model directory\"\n            )\n        \n        with col2:\n            output_path = st.text_input(\n                \"Export Path\",\n                value=f\"./exports/{export_format}_model\",\n                help=\"Where to save the exported model\"\n            )\n        \n        # Format-specific options\n        if export_format == \"merged_fp16\" or export_format == \"merged_fp32\":\n            st.info(\"💡 Merged models include both base weights and LoRA adapters for easier deployment.\")\n        \n        elif export_format == \"gguf\":\n            st.info(\"💡 GGUF format is optimized for llama.cpp inference on CPU and edge devices.\")\n            quantization = st.selectbox(\n                \"GGUF Quantization\",\n                [\"q4_0\", \"q4_1\", \"q5_0\", \"q5_1\", \"q8_0\", \"f16\", \"f32\"],\n                index=0\n            )\n        \n        elif export_format == \"onnx\":\n            st.info(\"💡 ONNX format provides cross-platform compatibility and optimization.\")\n        \n        elif export_format == \"lora_adapters\":\n            st.info(\"💡 LoRA adapters are lightweight and can be easily shared or swapped.\")\n        \n        # Model information for card\n        st.subheader(\"Model Information\")\n        \n        with st.form(\"model_info_form\"):\n            model_name = st.text_input(\"Model Name\", value=\"Medical-AI-Assistant\")\n            model_description = st.text_area(\n                \"Model Description\",\n                value=\"A fine-tuned medical AI assistant for educational purposes.\"\n            )\n            training_date = st.date_input(\"Training Date\")\n            \n            submit_export = st.form_submit_button(\"Export Model\", type=\"primary\")\n        \n        # Export execution\n        if submit_export:\n            model_info = {\n                \"model_name\": model_name,\n                \"base_model\": self.settings.BASE_MODEL,\n                \"training_date\": training_date.isoformat(),\n                \"description\": model_description,\n                \"training_examples\": getattr(st.session_state, 'training_examples', 0),\n                \"metrics\": getattr(st.session_state, 'evaluation_metrics', {})\n            }\n            \n            with st.spinner(f\"Exporting model in {self.export_formats[export_format]} format...\"):\n                success = False\n                \n                try:\n                    if export_format == \"lora_adapters\":\n                        success = self.export_lora_adapters(model_path, output_path)\n                    \n                    elif export_format in [\"merged_fp16\", \"merged_fp32\"]:\n                        precision = \"fp16\" if export_format == \"merged_fp16\" else \"fp32\"\n                        success = self.export_merged_model(model_path, output_path, precision)\n                    \n                    elif export_format == \"gguf\":\n                        success = self.export_to_gguf(model_path, output_path, quantization)\n                    \n                    elif export_format == \"onnx\":\n                        success = self.export_to_onnx(model_path, output_path)\n                    \n                    else:\n                        st.error(f\"Export format {export_format} not yet implemented.\")\n                        success = False\n                    \n                    if success:\n                        st.success(f\"✅ Model exported successfully to {output_path}\")\n                        \n                        # Create and save model card\n                        model_card = self.create_model_card(model_info)\n                        card_path = f\"{output_path}/README.md\"\n                        \n                        try:\n                            os.makedirs(output_path, exist_ok=True)\n                            with open(card_path, 'w') as f:\n                                f.write(model_card)\n                            st.success(f\"📄 Model card saved to {card_path}\")\n                        except Exception as e:\n                            st.warning(f\"Could not save model card: {str(e)}\")\n                        \n                        # Show download option\n                        if os.path.exists(output_path):\n                            st.subheader(\"Download Options\")\n                            st.info(f\"Model exported to: `{output_path}`\")\n                            \n                            # Display model card preview\n                            with st.expander(\"📄 Model Card Preview\"):\n                                st.markdown(model_card)\n                    \n                    else:\n                        st.error(\"❌ Export failed. Please check the logs for details.\")\n                        \n                except Exception as e:\n                    st.error(f\"❌ Export failed: {str(e)}\")\n        \n        # Deployment guidance\n        st.subheader(\"🚀 Deployment Options\")\n        \n        tab1, tab2, tab3 = st.tabs([\"Local Serving\", \"Cloud Deployment\", \"Edge Deployment\"])\n        \n        with tab1:\n            st.markdown(\"\"\"\n            ### Local Serving Options\n            \n            **For LoRA Adapters:**\n            ```python\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            from peft import PeftModel\n            \n            # Load base model\n            base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n            tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n            \n            # Load LoRA adapters\n            model = PeftModel.from_pretrained(base_model, \"./exports/lora_adapters\")\n            ```\n            \"\"\")\n        \n        with tab2:\n            st.markdown(\"\"\"\n            ### Cloud Deployment\n            \n            **AWS SageMaker:**\n            1. Upload model to S3\n            2. Create SageMaker endpoint\n            3. Deploy with auto-scaling\n            \"\"\")\n        \n        with tab3:\n            st.markdown(\"\"\"\n            ### Edge Deployment\n            \n            **For GGUF Models (CPU/Edge):**\n            ```bash\n            # Install llama.cpp\n            git clone https://github.com/ggerganov/llama.cpp\n            cd llama.cpp && make\n            \n            # Run inference\n            ./main -m ./exports/gguf_model/model.gguf -p \"Your medical question\"\n            ```\n            \"\"\")\n","size_bytes":16039},"components/nemo_rag.py":{"content":"# components/nemo_rag.py\n\"\"\"\nNeMoRAG: Hybrid retrieval + generation wrapper (full file)\n\nFeatures:\n- Re-uses FAISS index + sentence-transformers embedder (no duplicate KB)\n- Guarded NeMo support (only active if nemo_toolkit is installed)\n- Supports backends: \"faiss\", \"nemo\", \"hybrid\"\n- HuggingFace generator helper (free/open-source models)\n- Robust merging, normalization, deduplication, and metadata\n- Clear hooks for adapting NeMo retriever/generator loading\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport logging\nfrom typing import List, Dict, Any, Callable, Optional, Tuple, Iterable\nimport math\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n# Guarded NeMo imports\ntry:\n    import nemo.collections.nlp as nemo_nlp\n    import nemo.collections.nlp.modules.common as nemo_common\n    import torch\n    NEMO_AVAILABLE = True\nexcept Exception as e:\n    nemo_nlp = None\n    nemo_common = None\n    torch = None\n    NEMO_AVAILABLE = False\n    _NEMO_IMPORT_ERR = e\n\n# Transformers (HF) imports for fallback generator\ntry:\n    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    TRANSFORMERS_AVAILABLE = True\nexcept Exception as e:\n    AutoTokenizer = None\n    AutoModelForCausalLM = None\n    pipeline = None\n    TRANSFORMERS_AVAILABLE = False\n    _HF_IMPORT_ERR = e\n\n# Utilities\ndef _normalize_scores(scores: Iterable[float]) -> List[float]:\n    \"\"\"Min-max normalize a list of scores to range [0,1].\"\"\"\n    s = list(scores)\n    if not s:\n        return []\n    mn, mx = min(s), max(s)\n    if mx - mn < 1e-12:\n        return [0.5 for _ in s]\n    return [(x - mn) / (mx - mn) for x in s]\n\ndef _dedupe_keep_best(items: List[Dict[str, Any]], key_fn=lambda x: x.get(\"text\",\"\")) -> List[Dict[str, Any]]:\n    \"\"\"\n    Deduplicate by key_fn(text) keeping the highest-scoring entry for identical keys.\n    Returns list sorted by score desc.\n    \"\"\"\n    best = {}\n    for it in items:\n        k = key_fn(it).strip()\n        if not k:\n            # if empty key, include as unique using id\n            k = f\"_empty_{id(it)}\"\n        cur_score = float(it.get(\"score\", 0.0))\n        if k not in best or cur_score > float(best[k].get(\"score\", 0.0)):\n            best[k] = it\n    return sorted(list(best.values()), key=lambda x: float(x.get(\"score\", 0.0)), reverse=True)\n\nclass NeMoRAG:\n    \"\"\"\n    NeMoRAG class:\n      - attach FAISS index + passages + embedder via set_faiss_index()\n      - retrieve(query, k, backend) with backend in {\"faiss\",\"nemo\",\"hybrid\"}\n      - load_hf_generator() to use HF OSS models for generation\n      - generate_with_hf() or generate_with_nemo() to produce answers\n    \"\"\"\n\n    def __init__(self, settings):\n        self.settings = settings\n        self.nemo_available = NEMO_AVAILABLE\n        self.nemo_model = None\n        self.nemo_retriever = None\n        self.faiss_index = None\n        self.passages: List[Dict[str,Any]] = []\n        self.embedder = None\n        # choose device string\n        try:\n            self.device = \"cuda\" if (torch is not None and torch.cuda.is_available()) else \"cpu\"\n        except Exception:\n            self.device = \"cpu\"\n        # HF generator\n        self.hf_pipeline = None\n        self.hf_tokenizer = None\n        self.hf_model = None\n        # fusion weights\n        self.default_faiss_weight = float(os.getenv(\"HYBRID_FAISS_WEIGHT\", \"1.0\"))\n        self.default_nemo_weight = float(os.getenv(\"HYBRID_NEMO_WEIGHT\", \"1.0\"))\n        # active backend default\n        self.active_backend = getattr(self.settings, \"RAG_BACKEND\", \"faiss\").lower()\n\n        if self.nemo_available:\n            logger.info(\"NeMo detected — NeMoRAG: NeMo features may be enabled.\")\n        else:\n            logger.info(\"NeMo not detected. NeMo features disabled; using FAISS/HF fallbacks.\")\n\n    # -----------------------------\n    # Index wiring / reuse methods\n    # -----------------------------\n    def set_faiss_index(self, index, passages: List[Dict[str,Any]], embedder=None):\n        \"\"\"Attach existing FAISS index + passages + embedder from your RAGSystem.\"\"\"\n        self.faiss_index = index\n        self.passages = passages or []\n        self.embedder = embedder\n        logger.info(f\"[NeMoRAG] FAISS index attached (n_passages={len(self.passages)})\")\n\n    # -----------------------------\n    # NeMo loader scaffolds\n    # -----------------------------\n    def load_nemo_generator(self, config: Optional[Dict[str,Any]] = None) -> bool:\n        \"\"\"\n        Attempt to load a NeMo generator model. This is a scaffold: adapt to the exact NeMo model API you plan to use.\n        Example config keys: {'model_path': '/path/to/checkpoint', 'pretrained_name': 'nvidia/...'}\n        \"\"\"\n        if not self.nemo_available:\n            logger.warning(\"NeMo not installed; cannot load NeMo generator.\")\n            return False\n        try:\n            model_path = None\n            if config:\n                model_path = config.get(\"model_path\") or config.get(\"pretrained_name\")\n            if not model_path:\n                logger.warning(\"No model_path provided in config. Please supply 'model_path' or 'pretrained_name'.\")\n                return False\n\n            logger.info(f\"[NeMoRAG] Loading NeMo generator from {model_path} (scaffold)...\")\n            # TODO: replace with concrete NeMo loader call appropriate for the chosen NeMo model.\n            # e.g. self.nemo_model = nemo_nlp.models.YourModel.restore_from(model_path)\n            # for now, keep as scaffold:\n            self.nemo_model = None\n            logger.warning(\"NeMo generator loader is scaffolded. Implement loader for your NeMo model.\")\n            return False\n        except Exception as e:\n            logger.exception(f\"Failed to load NeMo generator: {e}\")\n            return False\n\n    def load_nemo_retriever(self, config: Optional[Dict[str,Any]] = None) -> bool:\n        \"\"\"\n        Attempt to load a NeMo retriever (scaffold). If you have a NeMo retriever, implement here.\n        If not implemented, nemo_retrieve will gracefully return [] and hybrid mode will fall back to FAISS.\n        \"\"\"\n        if not self.nemo_available:\n            logger.warning(\"NeMo not installed; cannot load NeMo retriever.\")\n            return False\n        try:\n            logger.info(\"[NeMoRAG] load_nemo_retriever scaffold called — implement retriever init here.\")\n            # TODO: implement depending on your chosen NeMo retriever\n            self.nemo_retriever = None\n            return False\n        except Exception as e:\n            logger.exception(f\"Failed to load NeMo retriever: {e}\")\n            return False\n\n    # -----------------------------\n    # HuggingFace generator loader (free/open-source)\n    # -----------------------------\n    def load_hf_generator(self, model_name: str = None, device: Optional[str] = None) -> bool:\n        \"\"\"\n        Load a HuggingFace text-generation pipeline for free/open-source models.\n        model_name examples: \"mistralai/Mistral-7B-Instruct-v0.2\", \"bigscience/bloomz-7b1\", \"OmerShah/medgemma-270m\" (small)\n        Device: \"cuda\" or \"cpu\". For CUDA we try to use device_map=\"auto\".\n        Returns True on success.\n        \"\"\"\n        if not TRANSFORMERS_AVAILABLE:\n            logger.error(\"transformers not available. Install 'transformers' to use HF generator.\")\n            return False\n        device = device or self.device\n        if model_name is None:\n            # gentle default small model (change if you have others)\n            model_name = os.getenv(\"HF_DEFAULT_GEN\", getattr(self.settings, \"BASE_MODEL\", \"facebook/opt-125m\"))\n\n        try:\n            logger.info(f\"[NeMoRAG] Loading HF generator '{model_name}' on device={device}\")\n            # Load tokenizer + model with safe options\n            # Use device_map=\"auto\" if CUDA available, else load on CPU\n            from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline as hf_pipeline\n            if device == \"cuda\":\n                tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=None)\n                # pipeline will attempt to place model on GPU\n                pipe = hf_pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n                pipe = hf_pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=-1)\n            # store references\n            self.hf_tokenizer = tokenizer\n            self.hf_model = model\n            self.hf_pipeline = pipe\n            logger.info(\"[NeMoRAG] HF generator loaded successfully.\")\n            return True\n        except Exception as e:\n            logger.exception(f\"Failed to load HF generator '{model_name}': {e}\")\n            return False\n\n    # -----------------------------\n    # Retrieval methods\n    # -----------------------------\n    def faiss_retrieve(self, query: str, k: int = 5) -> List[Dict[str,Any]]:\n        \"\"\"Retrieve from the attached FAISS index using the attached embedder.\"\"\"\n        if self.faiss_index is None or self.embedder is None:\n            logger.debug(\"faiss_retrieve: missing index or embedder; returning [].\")\n            return []\n        try:\n            q_emb = self.embedder.encode([query], convert_to_numpy=True)\n            # normalize\n            try:\n                import numpy as np\n                q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12)\n            except Exception:\n                pass\n            D, I = self.faiss_index.search(q_emb, k)\n            results = []\n            for i, idx in enumerate(I[0]):\n                if idx >= 0 and idx < len(self.passages):\n                    p = self.passages[idx].copy()\n                    score = float(D[0][i]) if D is not None else 0.0\n                    p[\"score\"] = score\n                    p[\"_backend\"] = \"faiss\"\n                    results.append(p)\n            return results\n        except Exception as e:\n            logger.exception(f\"faiss_retrieve error: {e}\")\n            return []\n\n    def nemo_retrieve(self, query: str, k: int = 5) -> List[Dict[str,Any]]:\n        \"\"\"\n        Use NeMo retriever if available. If not implemented or not available, return [].\n        This is a scaffold — implement according to the NeMo retriever API you adopt.\n        \"\"\"\n        if not self.nemo_available:\n            logger.debug(\"nemo_retrieve: NeMo not available. Returning [].\")\n            return []\n        if self.nemo_retriever is None:\n            logger.debug(\"nemo_retrieve: No NeMo retriever loaded. Returning [].\")\n            return []\n\n        try:\n            # Example pseudocode (adapt to your retriever):\n            # hits = self.nemo_retriever.retrieve(query, top_k=k)\n            # results = [{\"id\": h.id, \"text\": h.text, \"source\": getattr(h, \"source\", \"nemo\"), \"score\": float(h.score), \"_backend\": \"nemo\"} for h in hits]\n            logger.warning(\"nemo_retrieve: NeMo retriever scaffold in use — implement concrete retrieval logic.\")\n            return []\n        except Exception as e:\n            logger.exception(f\"nemo_retrieve error: {e}\")\n            return []\n\n    def _merge_and_rerank(self,\n                          faiss_docs: List[Dict[str,Any]],\n                          nemo_docs: List[Dict[str,Any]],\n                          k: int,\n                          faiss_weight: float,\n                          nemo_weight: float) -> List[Dict[str,Any]]:\n        \"\"\"\n        Merge FAISS + NeMo doc lists, normalize scores, apply weights, dedupe, and return top-k.\n        \"\"\"\n        combined = []\n        faiss_scores = [d.get(\"score\", 0.0) for d in faiss_docs]\n        nemo_scores = [d.get(\"score\", 0.0) for d in nemo_docs]\n\n        faiss_norm = _normalize_scores(faiss_scores) if faiss_scores else []\n        nemo_norm = _normalize_scores(nemo_scores) if nemo_scores else []\n\n        for i, d in enumerate(faiss_docs):\n            d2 = dict(d)\n            d2[\"_orig_score\"] = float(d.get(\"score\", 0.0))\n            d2[\"_score_norm\"] = faiss_norm[i] if i < len(faiss_norm) else 0.0\n            d2[\"_combined_score\"] = faiss_weight * d2[\"_score_norm\"]\n            combined.append(d2)\n\n        for i, d in enumerate(nemo_docs):\n            d2 = dict(d)\n            d2[\"_orig_score\"] = float(d.get(\"score\", 0.0))\n            d2[\"_score_norm\"] = nemo_norm[i] if i < len(nemo_norm) else 0.0\n            d2[\"_combined_score\"] = nemo_weight * d2[\"_score_norm\"]\n            combined.append(d2)\n\n        # Deduplicate and keep highest combined_score\n        deduped = _dedupe_keep_best(combined, key_fn=lambda x: x.get(\"text\",\"\"))\n        deduped_sorted = sorted(deduped, key=lambda x: float(x.get(\"_combined_score\", 0.0)), reverse=True)\n        topk = deduped_sorted[:k]\n        for doc in topk:\n            doc[\"score\"] = float(doc.get(\"_combined_score\", 0.0))\n        return topk\n\n    def retrieve(self, query: str, k: int = 5, backend: Optional[str] = None) -> List[Dict[str,Any]]:\n        \"\"\"\n        Unified retrieval entrypoint supporting 'faiss', 'nemo', 'hybrid'.\n        - backend None -> uses settings.RAG_BACKEND if available, else 'faiss'\n        \"\"\"\n        backend = backend or getattr(self.settings, \"RAG_BACKEND\", \"faiss\")\n        backend = backend.lower()\n        if backend == \"faiss\":\n            return self.faiss_retrieve(query, k)\n        elif backend == \"nemo\":\n            # prefer NeMo retriever, but fallback to FAISS if empty\n            nemo_docs = self.nemo_retrieve(query, k)\n            if nemo_docs:\n                return nemo_docs\n            logger.debug(\"nemo_retrieve returned empty — fallback to faiss_retrieve.\")\n            return self.faiss_retrieve(query, k)\n        elif backend == \"hybrid\":\n            faiss_docs = self.faiss_retrieve(query, k*2) if self.faiss_index is not None else []\n            nemo_docs = self.nemo_retrieve(query, k*2) if self.nemo_available else []\n            if not faiss_docs and not nemo_docs:\n                return []\n            merged = self._merge_and_rerank(faiss_docs, nemo_docs, k, self.default_faiss_weight, self.default_nemo_weight)\n            for d in merged:\n                if \"_backend\" not in d:\n                    d[\"_backend\"] = \"hybrid\"\n            return merged\n        else:\n            logger.warning(f\"Unknown retrieval backend '{backend}', falling back to 'faiss'.\")\n            return self.faiss_retrieve(query, k)\n\n    # -----------------------------\n    # Generation helpers\n    # -----------------------------\n    def _compose_retrieved_block(self, contexts: List[Dict[str,Any]], max_chars_each: int = 1000) -> str:\n        \"\"\"Create a compact retrieved block for prompts with provenance.\"\"\"\n        if not contexts:\n            return \"\"\n        parts = []\n        for i, c in enumerate(contexts, 1):\n            text = c.get(\"text\",\"\")\n            text_preview = text if len(text) <= max_chars_each else text[:max_chars_each].rsplit(\" \",1)[0] + \"...\"\n            src = c.get(\"source\", c.get(\"_backend\",\"unknown\"))\n            score = c.get(\"score\", None)\n            if score is not None:\n                parts.append(f\"[{i}] Source: {src} (score={score:.3f})\\n{text_preview}\")\n            else:\n                parts.append(f\"[{i}] Source: {src}\\n{text_preview}\")\n        return \"\\n\\n\".join(parts)\n\n    def generate_with_hf(self,\n                         hf_generator_fn: Optional[Callable[[str, Optional[str], int], str]],\n                         question: str,\n                         contexts: List[Dict[str,Any]],\n                         max_new_tokens: Optional[int] = None,\n                         temperature: Optional[float] = None) -> Tuple[str, Dict[str,Any]]:\n        \"\"\"\n        Use a HuggingFace generator pipeline (self.hf_pipeline) if loaded, otherwise call hf_generator_fn\n        hf_generator_fn signature: (prompt: str, context_block: Optional[str], max_new_tokens: int) -> str\n        \"\"\"\n        if max_new_tokens is None:\n            max_new_tokens = int(getattr(self.settings, \"MAX_NEW_TOKENS\", 256))\n        if temperature is None:\n            temperature = float(getattr(self.settings, \"TEMPERATURE\", 0.2))\n\n        retrieved_block = self._compose_retrieved_block(contexts)\n        system_prompt = getattr(self.settings, \"SAFETY_SYSTEM_PROMPT\", None) or \\\n                        \"You are a medical-education assistant. Do NOT provide personalized medical advice. Cite sources.\"\n        prompt = f\"{system_prompt}\\n\\nRetrieved Documents:\\n{retrieved_block}\\n\\nUser: {question}\\nAssistant:\"\n\n        # Prefer pipeline if available\n        if self.hf_pipeline is not None:\n            try:\n                outputs = self.hf_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature)\n                # pipeline returned a list of dicts with 'generated_text'\n                gen_text = outputs[0].get(\"generated_text\", \"\")\n                # Remove prompt prefix if model echoes it\n                if gen_text.startswith(prompt):\n                    response = gen_text[len(prompt):].strip()\n                else:\n                    response = gen_text.strip()\n                meta = {\"generator\":\"hf\", \"ok\":True, \"prompt_len\": len(prompt), \"num_ctx\": len(contexts)}\n                return response, meta\n            except Exception as e:\n                logger.exception(f\"HF pipeline generation failed: {e}\")\n                # fallback to user callable if present\n        if hf_generator_fn is not None:\n            try:\n                response = hf_generator_fn(prompt, retrieved_block, max_new_tokens)\n                return response, {\"generator\":\"hf\", \"ok\":True}\n            except Exception as e:\n                logger.exception(f\"User-provided HF generator function failed: {e}\")\n                return (f\"HF generation error: {e}\", {\"generator\":\"hf\", \"ok\":False})\n        return (\"No HF generator available. Call load_hf_generator() or pass a hf_generator_fn.\", {\"generator\":\"hf\", \"ok\":False})\n\n    def generate_with_nemo(self,\n                           question: str,\n                           contexts: List[Dict[str,Any]],\n                           max_new_tokens: int = 256,\n                           generation_config: Optional[Dict[str,Any]] = None) -> Tuple[str, Dict[str,Any]]:\n        \"\"\"\n        Attempt to generate using an attached NeMo generator model (self.nemo_model).\n        This function is a scaffold and must be adapted to the exact NeMo model API you will use.\n        \"\"\"\n        if not self.nemo_available:\n            return (\"NeMo not installed on this machine.\", {\"generator\":\"nemo\", \"ok\":False})\n        if self.nemo_model is None:\n            return (\"NeMo generator not loaded. Call load_nemo_generator() first.\", {\"generator\":\"nemo\", \"ok\":False})\n\n        retrieved_block = self._compose_retrieved_block(contexts)\n        system_prompt = getattr(self.settings, \"SAFETY_SYSTEM_PROMPT\", None) or \\\n                        \"You are a medical-education assistant. Do NOT provide personalized medical advice. Cite sources.\"\n        prompt = f\"{system_prompt}\\n\\nRetrieved Documents:\\n{retrieved_block}\\n\\nUser: {question}\\nAssistant:\"\n\n        try:\n            # TODO: replace following pseudocode with real NeMo generation calls\n            # outputs = self.nemo_model.generate(prompts=[prompt], max_length=max_new_tokens, **(generation_config or {}))\n            # response = outputs[0]\n            logger.warning(\"generate_with_nemo() is scaffolded — implement according to your NeMo model API.\")\n            return (\"NeMo generation scaffold: implement generate_with_nemo() for your NeMo model.\", {\"generator\":\"nemo\", \"ok\":False})\n        except Exception as e:\n            logger.exception(f\"NeMo generation error: {e}\")\n            return (f\"NeMo generation error: {e}\", {\"generator\":\"nemo\", \"ok\":False})\n\n    # -----------------------------\n    # Status & utilities\n    # -----------------------------\n    def status(self) -> Dict[str,Any]:\n        \"\"\"Return status information for debugging / UI display.\"\"\"\n        return {\n            \"nemo_available\": self.nemo_available,\n            \"nemo_model_loaded\": bool(self.nemo_model),\n            \"nemo_retriever_loaded\": bool(self.nemo_retriever),\n            \"hf_generator_loaded\": bool(self.hf_pipeline),\n            \"faiss_index_attached\": bool(self.faiss_index),\n            \"num_passages\": len(self.passages) if self.passages else 0,\n            \"device\": self.device,\n            \"active_backend\": getattr(self.settings, \"RAG_BACKEND\", \"faiss\"),\n            \"faiss_weight\": self.default_faiss_weight,\n            \"nemo_weight\": self.default_nemo_weight\n        }\n\n    def info(self) -> str:\n        s = self.status()\n        return f\"NeMoRAG(status={s})\"\n","size_bytes":20902},"components/rag_system.py":{"content":"import sys\nfrom pathlib import Path\nimport os\n\ncurrent_dir = Path(__file__).parent\nproject_root = current_dir.parent\nsys.path.append(str(project_root))\n\nimport streamlit as st\nimport pickle\nimport json\nimport numpy as np\nimport pandas as pd\nimport logging\nfrom typing import List, Dict, Any\nfrom utils.data_processing import process_medical_documents\nfrom config.settings import Settings\nfrom utils.fallbacks import (\n    get_embedder,\n    get_faiss_index,\n    normalize_l2,\n    HAS_SENTENCE_TRANSFORMERS,\n    HAS_FAISS,\n)\nfrom components.nemo_rag import NeMoRAG  # 🔑 New import\n\nlogger = logging.getLogger(__name__)\n\nclass RAGSystem:\n    def __init__(self):\n        self.settings = Settings()\n        self.embedder = None\n        self.index = None\n        self.passages = []\n        self.index_path = \"data/rag_index.faiss\"\n        self.meta_path = \"data/rag_meta.pkl\"\n\n        # 🔑 Integrate NeMoRAG wrapper\n        self.nemo_rag = NeMoRAG(self.settings)\n\n    def load_embedder(self):\n        \"\"\"Load embedding model based on backend (HF | NeMo)\"\"\"\n        if self.embedder is None:\n            with st.spinner(\"Loading embedding model...\"):\n                if self.settings.RAG_BACKEND == \"nemo\":\n                    # Load NeMo embedding model\n                    self.embedder = get_embedder(self.settings.NEMO_EMBED_MODEL)\n                else:\n                    # Default HuggingFace embedder\n                    self.embedder = get_embedder(self.settings.EMBED_MODEL)\n        return self.embedder\n\n\n    def load_medical_dataset(self) -> List[Dict[str, Any]]:\n        \"\"\"Load medical dataset from HuggingFace\"\"\"\n        try:\n            from datasets import load_dataset\n\n            with st.spinner(\"Loading medical dataset from HuggingFace...\"):\n                ds = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n\n                passages = []\n                if \"train\" in ds:\n                    for i, example in enumerate(ds[\"train\"]):\n                        if \"input\" in example and \"output\" in example:\n                            passage_text = (\n                                f\"Medical Case: {example['input']}\\n\\n\"\n                                f\"Medical Reasoning: {example['output']}\"\n                            )\n                            passages.append(\n                                {\n                                    \"id\": f\"medical_dataset_{i}\",\n                                    \"text\": passage_text,\n                                    \"source\": \"medical-o1-reasoning-SFT\",\n                                    \"metadata\": {\n                                        \"type\": \"medical_reasoning\",\n                                        \"dataset\": \"medical-o1-reasoning-SFT\",\n                                        \"index\": i,\n                                    },\n                                }\n                            )\n\n                logger.info(f\"Loaded {len(passages)} medical reasoning examples\")\n                return passages\n\n        except Exception as e:\n            logger.error(f\"Error loading medical dataset: {str(e)}\")\n            st.warning(\n                f\"Could not load medical dataset: {str(e)}. Using fallback medical data.\"\n            )\n            return self.get_fallback_medical_data()\n\n    def get_fallback_medical_data(self) -> List[Dict[str, Any]]:\n        \"\"\"Provide fallback medical data if dataset loading fails\"\"\"\n        return [\n            {\n                \"id\": \"fallback_1\",\n                \"text\": \"Diabetes is a chronic condition that affects how your body processes blood sugar. Symptoms include increased thirst, frequent urination, extreme hunger, unexplained weight loss, fatigue, and blurred vision. Treatment involves lifestyle changes, medication, and regular monitoring.\",\n                \"source\": \"fallback_medical_knowledge\",\n                \"metadata\": {\"type\": \"condition\", \"condition\": \"diabetes\"},\n            },\n            {\n                \"id\": \"fallback_2\",\n                \"text\": \"Hypertension (high blood pressure) is a common condition where the force of blood against artery walls is too high. It can lead to heart disease, stroke, and other complications. Management includes diet, exercise, and medication.\",\n                \"source\": \"fallback_medical_knowledge\",\n                \"metadata\": {\"type\": \"condition\", \"condition\": \"hypertension\"},\n            },\n            {\n                \"id\": \"fallback_3\",\n                \"text\": \"COVID-19 symptoms range from mild to severe and may appear 2-14 days after exposure. Common symptoms include fever, cough, shortness of breath, fatigue, muscle aches, loss of taste or smell. Prevention includes vaccination, masking, and social distancing.\",\n                \"source\": \"fallback_medical_knowledge\",\n                \"metadata\": {\"type\": \"condition\", \"condition\": \"covid-19\"},\n            },\n        ]\n\n    def build_index(self, passages: List[Dict[str, Any]]) -> bool:\n        \"\"\"Build FAISS index from passages\"\"\"\n        try:\n            os.makedirs(\"data\", exist_ok=True)\n            embedder = self.load_embedder()\n            texts = [p[\"text\"] for p in passages]\n\n            with st.spinner(f\"Generating embeddings for {len(texts)} documents...\"):\n                embeddings = embedder.encode(\n                    texts, show_progress_bar=False, convert_to_numpy=True\n                )\n\n            dim = embeddings.shape[1]\n            self.index = get_faiss_index(dim)\n            embeddings = normalize_l2(embeddings)\n            self.index.add(embeddings)\n\n            if HAS_FAISS:\n                import faiss\n\n                faiss.write_index(index, self.index_path)\n            else:\n                with open(self.index_path + \".fallback\", \"wb\") as f:\n                    pickle.dump(index, f)\n\n            with open(self.meta_path, \"wb\") as f:\n                pickle.dump(passages, f)\n\n            #self.index = index\n            self.passages = passages\n\n            # 🔑 Wire FAISS into NeMoRAG\n            self.nemo_rag.set_faiss_index(self.index, self.passages, self.embedder)\n            \n            \n            # Update session state\n            st.session_state.rag_initialized = True\n\n\n            logger.info(f\"Built index with {len(passages)} documents\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error building index: {str(e)}\")\n            st.error(f\"Error building index: {str(e)}\")\n            return False\n\n    def load_index(self) -> bool:\n        \"\"\"Load existing FAISS index and metadata\"\"\"\n        try:\n            fallback_path = self.index_path + \".fallback\"\n\n            if HAS_FAISS and os.path.exists(self.index_path) and os.path.exists(\n                self.meta_path\n            ):\n                import faiss\n\n                self.index = faiss.read_index(self.index_path)\n                with open(self.meta_path, \"rb\") as f:\n                    self.passages = pickle.load(f)\n\n                self.load_embedder()\n                self.nemo_rag.set_faiss_index(\n                    self.index, self.passages, self.embedder\n                )  # 🔑\n                logger.info(f\"Loaded FAISS index with {len(self.passages)} documents\")\n                return True\n\n            elif os.path.exists(fallback_path) and os.path.exists(self.meta_path):\n                with open(fallback_path, \"rb\") as f:\n                    self.index = pickle.load(f)\n                with open(self.meta_path, \"rb\") as f:\n                    self.passages = pickle.load(f)\n\n                self.load_embedder()\n                self.nemo_rag.set_faiss_index(\n                    self.index, self.passages, self.embedder\n                )  # 🔑\n                logger.info(f\"Loaded fallback index with {len(self.passages)} documents\")\n                return True\n\n            return False\n\n        except Exception as e:\n            logger.error(f\"Error loading index: {str(e)}\")\n            st.error(f\"Error loading index: {str(e)}\")\n            return False\n\n    def retrieve_documents(self, query: str, k: int = None) -> List[Dict[str, Any]]:\n        if k is None:\n            k = self.settings.RETRIEVAL_TOP_K\n        if not self.index or not self.embedder:\n            st.warning(\"⚠️ No index loaded. Please initialize the RAG system first.\")\n            return []\n\n        try:\n            if self.settings.RAG_BACKEND in [\"faiss\", \"nemo\"]:\n                return self.nemo_rag.retrieve(query, k=k, backend=self.settings.RAG_BACKEND)\n            elif self.settings.RAG_BACKEND == \"hybrid\":\n                faiss_docs = self.nemo_rag.retrieve(query, k=k, backend=\"faiss\")\n                nemo_docs = self.nemo_rag.retrieve(query, k=k, backend=\"nemo\")\n                return self.nemo_rag.fuse_results(faiss_docs, nemo_docs, method=self.settings.HYBRID_FUSION_METHOD)\n        except Exception as e:\n            logger.error(f\"Error retrieving documents: {str(e)}\")\n            st.error(f\"Error during retrieval: {str(e)}\")\n            return []\n\n\n    # 🔽 render() remains identical except now retrieves via NeMoRAG\n    def render(self):\n        \"\"\"Render the RAG system interface\"\"\"\n        st.header(\"🔍 Retrieval-Augmented Generation System\")\n\n        # Backend selector synced with Settings + session_state\n        st.subheader(\"🔽 RAG Backend Selection\")\n        backend_choice = st.radio(\n            \"Choose RAG Backend:\",\n            options=[\"faiss\", \"nemo\", \"hybrid\"],\n            index=[\"faiss\", \"nemo\", \"hybrid\"].index(self.settings.RAG_BACKEND),\n            help=\"Select FAISS (local), NeMo (dense retriever), or Hybrid (fusion of both)\"\n        )\n        if backend_choice != self.settings.RAG_BACKEND:\n            self.settings.RAG_BACKEND = backend_choice\n            st.session_state[\"rag_backend\"] = backend_choice\n            st.success(f\"✅ Switched RAG backend to **{backend_choice}**\")\n\n        # Check if index exists\n        fallback_exists = os.path.exists(self.index_path + \".fallback\") and os.path.exists(self.meta_path)\n        index_exists = (os.path.exists(self.index_path) and os.path.exists(self.meta_path)) or fallback_exists\n\n        # Load existing index if available but not loaded\n        if index_exists and not self.index:\n            if st.button(\"📂 Load Existing Index\"):\n                if self.load_index():\n                    st.success(f\"Successfully loaded index with {len(self.passages)} documents!\")\n                    st.session_state.rag_system = self\n                    st.rerun()\n\n        # Document upload and indexing section\n        st.subheader(\"📚 Document Management\")\n        \n        # Load medical dataset option\n        if st.button(\"📖 Load Medical Dataset\", help=\"Load pre-built medical reasoning dataset\"):\n            with st.spinner(\"Loading medical dataset...\"):\n                medical_passages = self.load_medical_dataset()\n                \n                if medical_passages:\n                    if self.build_index(medical_passages):\n                        st.success(f\"Successfully loaded medical dataset with {len(medical_passages)} examples!\")\n                        st.session_state.rag_system = self\n                        st.rerun()\n\n        # File upload section\n        uploaded_files = st.file_uploader(\n            \"Upload Medical Documents\",\n            accept_multiple_files=True,\n            type=['txt', 'pdf', 'json', 'csv'],\n            help=\"Upload medical documents, research papers, or structured medical data\"\n        )\n\n        # Manual text input\n        manual_text = st.text_area(\n            \"Or paste medical text directly:\",\n            height=200,\n            help=\"Paste medical guidelines, protocols, or reference text\"\n        )\n\n        # Build index button\n        if st.button(\"🏗️ Build Knowledge Base\", type=\"primary\"):\n            passages = []\n            \n            # Process uploaded files\n            if uploaded_files:\n                for file in uploaded_files:\n                    try:\n                        file_passages = process_medical_documents(file)\n                        passages.extend(file_passages)\n                        st.success(f\"Processed {file.name}: {len(file_passages)} passages\")\n                    except Exception as e:\n                        st.error(f\"Error processing {file.name}: {str(e)}\")\n            \n            # Process manual text\n            if manual_text.strip():\n                passages.append({\n                    \"id\": f\"manual_input_{len(passages)}\",\n                    \"text\": manual_text.strip(),\n                    \"source\": \"manual_input\",\n                    \"metadata\": {\"type\": \"manual\"}\n                })\n            \n            if passages:\n                if self.build_index(passages):\n                    st.success(f\"Successfully built knowledge base with {len(passages)} documents!\")\n                    st.session_state.rag_system = self\n                    st.rerun()\n            else:\n                st.warning(\"No documents to process. Please upload files or enter text.\")\n\n        # Display current index status\n        if self.index and self.passages:\n            st.subheader(\"📊 Current Knowledge Base\")\n            \n            col1, col2, col3 = st.columns(3)\n            with col1:\n                st.metric(\"Total Documents\", len(self.passages))\n            with col2:\n                st.metric(\"Index Dimension\", self.index.d if hasattr(self.index, 'd') else \"N/A\")\n            with col3:\n                st.metric(\"Total Vectors\", self.index.ntotal if hasattr(self.index, 'ntotal') else len(self.passages))\n            \n            # Document preview\n            if st.checkbox(\"📋 Show Document Preview\"):\n                df = pd.DataFrame([\n                    {\n                        \"ID\": p.get(\"id\", \"unknown\"),\n                        \"Source\": p.get(\"source\", \"unknown\"),\n                        \"Type\": p.get(\"metadata\", {}).get(\"type\", \"unknown\"),\n                        \"Text Preview\": p[\"text\"][:200] + \"...\" if len(p[\"text\"]) > 200 else p[\"text\"]\n                    }\n                    for p in self.passages[:10]\n                ])\n                st.dataframe(df, use_container_width=True)\n\n        # Test retrieval section\n        st.subheader(\"🔍 Test Retrieval\")\n        test_query = st.text_input(\"Enter a medical query to test retrieval:\")\n        \n        if test_query and self.index:\n            with st.spinner(f\"Searching knowledge base using {self.settings.RAG_BACKEND}...\"):\n                results = self.retrieve_documents(test_query)\n            \n            if results:\n                st.write(f\"Found {len(results)} relevant documents:\")\n                \n                for i, doc in enumerate(results):\n                    with st.expander(f\"Document {i+1} (Score: {doc.get('score', 0):.3f})\"):\n                        st.write(f\"**Source:** {doc.get('source', 'unknown')}\")\n                        st.write(f\"**Type:** {doc.get('metadata', {}).get('type', 'unknown')}\")\n                        st.write(f\"**Text:** {doc['text']}\")\n            else:\n                st.warning(\"No relevant documents found.\")\n\n        # Export/Import functionality\n        st.subheader(\"💾 Index Management\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            if st.button(\"📤 Export Index\") and self.index:\n                try:\n                    export_data = {\n                        \"passages\": self.passages,\n                        \"settings\": {\n                            \"embed_model\": self.settings.EMBED_MODEL,\n                            \"created_at\": pd.Timestamp.now().isoformat()\n                        }\n                    }\n                    \n                    export_json = json.dumps(export_data, indent=2)\n                    st.download_button(\n                        label=\"💾 Download Knowledge Base\",\n                        data=export_json,\n                        file_name=\"medical_knowledge_base.json\",\n                        mime=\"application/json\"\n                    )\n                except Exception as e:\n                    st.error(f\"Export failed: {str(e)}\")\n        \n        with col2:\n            import_file = st.file_uploader(\n                \"📥 Import Knowledge Base\",\n                type=['json'],\n                help=\"Import previously exported knowledge base\"\n            )\n            \n            if import_file and st.button(\"📥 Import\"):\n                try:\n                    import_data = json.load(import_file)\n                    if self.build_index(import_data[\"passages\"]):\n                        st.success(\"Successfully imported knowledge base!\")\n                        st.session_state.rag_system = self\n                        st.rerun()\n                except Exception as e:\n                    st.error(f\"Import failed: {str(e)}\")","size_bytes":16888},"components/safety.py":{"content":"import streamlit as st\nimport re\nimport json\nimport logging\nfrom typing import List, Dict, Any, Tuple\nfrom config.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\nclass SafetySystem:\n    def __init__(self):\n        self.settings = Settings()\n        self.safety_rules = self.load_safety_rules()\n        self.blocked_patterns = self.load_blocked_patterns()\n        self.citation_patterns = [\n            r'\\[(\\d+)\\]',  # [1], [2], etc.\n            r'\\((\\d+)\\)',  # (1), (2), etc.\n            r'reference (\\d+)',  # reference 1, reference 2, etc.\n        ]\n        \n    def load_safety_rules(self) -> List[Dict[str, str]]:\n        \"\"\"Load medical safety rules and guidelines\"\"\"\n        return [\n            {\n                \"rule\": \"No personalized medical advice\",\n                \"description\": \"Do not provide specific medical advice for individual patients\",\n                \"keywords\": [\"should I take\", \"what medication\", \"my symptoms\", \"I have\", \"my condition\"],\n                \"response\": \"I cannot provide personalized medical advice. Please consult with a qualified healthcare professional.\"\n            },\n            {\n                \"rule\": \"No diagnosis\",\n                \"description\": \"Do not attempt to diagnose medical conditions\",\n                \"keywords\": [\"do I have\", \"am I sick\", \"what's wrong with me\", \"diagnose\", \"symptoms mean\"],\n                \"response\": \"I cannot diagnose medical conditions. Please see a healthcare provider for proper evaluation.\"\n            },\n            {\n                \"rule\": \"No treatment recommendations\",\n                \"description\": \"Do not recommend specific treatments without proper medical supervision\",\n                \"keywords\": [\"should I take\", \"how much\", \"stop taking\", \"treatment for\", \"cure for\"],\n                \"response\": \"Treatment decisions should always be made with a qualified healthcare professional.\"\n            },\n            {\n                \"rule\": \"Emergency situations\",\n                \"description\": \"Recognize and appropriately respond to medical emergencies\",\n                \"keywords\": [\"emergency\", \"urgent\", \"severe pain\", \"can't breathe\", \"chest pain\", \"suicide\"],\n                \"response\": \"This appears to be a medical emergency. Please contact emergency services immediately or go to the nearest emergency room.\"\n            },\n            {\n                \"rule\": \"Medication safety\",\n                \"description\": \"Emphasize proper medication use and safety\",\n                \"keywords\": [\"drug interaction\", \"side effects\", \"overdose\", \"mixing medications\"],\n                \"response\": \"Medication safety is critical. Please consult your pharmacist or healthcare provider about drug interactions and proper usage.\"\n            }\n        ]\n    \n    def load_blocked_patterns(self) -> List[str]:\n        \"\"\"Load patterns that should be blocked in responses\"\"\"\n        return [\n            r\"take \\d+ pills?\",  # Specific dosage instructions\n            r\"stop taking your medication\",  # Dangerous medication advice\n            r\"you have [a-zA-Z\\s]+ disease\",  # Diagnosis statements\n            r\"you should see a doctor in \\d+ days\",  # Specific medical timing\n            r\"this will cure\",  # Cure claims\n            r\"guaranteed to work\",  # Medical guarantees\n            r\"FDA approved for\",  # False FDA claims\n        ]\n    \n    def get_medical_disclaimer(self) -> str:\n        \"\"\"Get the standard medical disclaimer\"\"\"\n        return \"\"\"\n        ⚠️ **Medical Disclaimer**: This information is for educational purposes only and should not replace professional medical advice. \n        Always consult with a qualified healthcare provider for medical concerns, diagnosis, or treatment decisions.\n        \"\"\"\n    \n    def get_safety_system_prompt(self) -> str:\n        \"\"\"Get the system prompt that enforces safety guidelines\"\"\"\n        return \"\"\"You are a medical education assistant designed to provide accurate, evidence-based information for learning purposes only. \n\nCRITICAL SAFETY GUIDELINES:\n1. DO NOT provide personalized medical advice, diagnosis, or treatment recommendations\n2. DO NOT recommend specific medications or dosages\n3. ALWAYS emphasize consulting healthcare professionals for medical decisions\n4. Include numbered citations [1], [2] for all medical claims using provided sources\n5. If uncertain about medical information, clearly state limitations\n6. For emergency situations, direct users to emergency services\n7. Focus on general medical education and established medical knowledge\n\nResponse format:\n- Provide educational information with citations\n- Include relevant safety warnings\n- End with appropriate disclaimers\n- Direct users to healthcare professionals when appropriate\"\"\"\n    \n    def check_input_safety(self, user_input: str) -> Tuple[bool, str, str]:\n        \"\"\"\n        Check if user input triggers safety concerns\n        Returns: (is_safe, safety_rule_triggered, suggested_response)\n        \"\"\"\n        user_input_lower = user_input.lower()\n        \n        for rule in self.safety_rules:\n            for keyword in rule[\"keywords\"]:\n                if keyword.lower() in user_input_lower:\n                    return False, rule[\"rule\"], rule[\"response\"]\n        \n        return True, \"\", \"\"\n    \n    def check_output_safety(self, response: str) -> Tuple[bool, List[str]]:\n        \"\"\"\n        Check if model output violates safety guidelines\n        Returns: (is_safe, list_of_violations)\n        \"\"\"\n        violations = []\n        \n        for pattern in self.blocked_patterns:\n            if re.search(pattern, response, re.IGNORECASE):\n                violations.append(f\"Contains blocked pattern: {pattern}\")\n        \n        # Check for lack of citations in medical claims\n        medical_claim_patterns = [\n            r\"studies show\",\n            r\"research indicates\",\n            r\"according to\",\n            r\"evidence suggests\",\n            r\"clinical trials\",\n            r\"proven to\",\n        ]\n        \n        has_medical_claims = any(re.search(pattern, response, re.IGNORECASE) for pattern in medical_claim_patterns)\n        has_citations = any(re.search(pattern, response) for pattern in self.citation_patterns)\n        \n        if has_medical_claims and not has_citations:\n            violations.append(\"Medical claims without proper citations\")\n        \n        return len(violations) == 0, violations\n    \n    def enforce_citation_format(self, response: str, retrieved_contexts: List[Dict[str, Any]]) -> str:\n        \"\"\"Ensure proper citation format in responses\"\"\"\n        if not retrieved_contexts:\n            return response\n        \n        # Add citation appendix\n        citation_appendix = \"\\n\\n**Sources:**\\n\"\n        for i, context in enumerate(retrieved_contexts, 1):\n            source = context.get('source', 'Unknown source')\n            citation_appendix += f\"[{i}] {source}\\n\"\n        \n        return response + citation_appendix\n    \n    def sanitize_response(self, response: str, retrieved_contexts: List[Dict[str, Any]] = None) -> str:\n        \"\"\"Apply safety measures to model response\"\"\"\n        # Check output safety\n        is_safe, violations = self.check_output_safety(response)\n        \n        if not is_safe:\n            logger.warning(f\"Safety violations detected: {violations}\")\n            return self.get_safe_fallback_response()\n        \n        # Enforce citations\n        if retrieved_contexts:\n            response = self.enforce_citation_format(response, retrieved_contexts)\n        \n        # Add medical disclaimer\n        response += \"\\n\\n\" + self.get_medical_disclaimer()\n        \n        return response\n    \n    def get_safe_fallback_response(self) -> str:\n        \"\"\"Get a safe fallback response when safety violations are detected\"\"\"\n        return \"\"\"I understand you're looking for medical information, but I need to be careful about providing safe and appropriate responses. \n\nFor specific medical concerns, symptoms, or treatment questions, I strongly recommend:\n\n1. **Consult a healthcare professional** - They can provide personalized advice based on your specific situation\n2. **Contact your doctor or pharmacist** - For medication-related questions\n3. **Visit a medical facility** - For urgent concerns\n4. **Call emergency services** - For any medical emergency\n\nI'm here to provide general medical education and information, but cannot replace professional medical guidance.\n\n\"\"\" + self.get_medical_disclaimer()\n    \n    def generate_safety_report(self, interactions: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Generate a safety compliance report\"\"\"\n        total_interactions = len(interactions)\n        safety_triggered = 0\n        citation_compliance = 0\n        violation_types = {}\n        \n        for interaction in interactions:\n            user_input = interaction.get('user_input', '')\n            response = interaction.get('response', '')\n            \n            # Check input safety\n            is_input_safe, rule_triggered, _ = self.check_input_safety(user_input)\n            if not is_input_safe:\n                safety_triggered += 1\n                violation_types[rule_triggered] = violation_types.get(rule_triggered, 0) + 1\n            \n            # Check citation compliance\n            has_citations = any(re.search(pattern, response) for pattern in self.citation_patterns)\n            if has_citations:\n                citation_compliance += 1\n        \n        return {\n            'total_interactions': total_interactions,\n            'safety_triggered': safety_triggered,\n            'safety_trigger_rate': safety_triggered / total_interactions if total_interactions > 0 else 0,\n            'citation_compliance': citation_compliance,\n            'citation_compliance_rate': citation_compliance / total_interactions if total_interactions > 0 else 0,\n            'violation_breakdown': violation_types,\n            'compliance_score': (total_interactions - safety_triggered + citation_compliance) / (2 * total_interactions) if total_interactions > 0 else 1.0\n        }\n    \n    def test_safety_rules(self) -> List[Dict[str, Any]]:\n        \"\"\"Test safety rules with example inputs\"\"\"\n        test_cases = [\n            \"Should I take aspirin for my headache?\",\n            \"What medication should I take for high blood pressure?\",\n            \"Do I have cancer based on these symptoms?\",\n            \"How much ibuprofen can I take?\",\n            \"Can you diagnose my rash?\",\n            \"I'm having chest pain, what should I do?\",\n            \"What is the general treatment for diabetes?\",  # This should be safe\n            \"How does blood pressure medication work?\",  # This should be safe\n        ]\n        \n        results = []\n        for test_input in test_cases:\n            is_safe, rule_triggered, suggested_response = self.check_input_safety(test_input)\n            results.append({\n                'input': test_input,\n                'is_safe': is_safe,\n                'rule_triggered': rule_triggered,\n                'suggested_response': suggested_response\n            })\n        \n        return results\n    \n    def render(self):\n        \"\"\"Render the safety system interface\"\"\"\n        st.header(\"🛡️ Safety & Guardrails System\")\n        \n        # Overview\n        st.subheader(\"Safety Overview\")\n        st.info(\"\"\"\n        The safety system ensures that the medical AI provides responsible, educational information while avoiding harmful medical advice.\n        It includes input filtering, output sanitization, citation enforcement, and compliance monitoring.\n        \"\"\")\n        \n        # Safety rules configuration\n        st.subheader(\"Safety Rules Configuration\")\n        \n        # Display current rules\n        with st.expander(\"Current Safety Rules\", expanded=True):\n            for i, rule in enumerate(self.safety_rules):\n                st.write(f\"**{i+1}. {rule['rule']}**\")\n                st.write(f\"*Description:* {rule['description']}\")\n                st.write(f\"*Keywords:* {', '.join(rule['keywords'])}\")\n                st.write(f\"*Response:* {rule['response']}\")\n                st.write(\"---\")\n        \n        # Add custom safety rule\n        st.subheader(\"Add Custom Safety Rule\")\n        with st.form(\"add_safety_rule\"):\n            new_rule_name = st.text_input(\"Rule Name:\")\n            new_rule_desc = st.text_area(\"Description:\")\n            new_keywords = st.text_input(\"Keywords (comma-separated):\")\n            new_response = st.text_area(\"Suggested Response:\")\n            \n            if st.form_submit_button(\"Add Rule\"):\n                if new_rule_name and new_keywords and new_response:\n                    keywords_list = [kw.strip() for kw in new_keywords.split(',')]\n                    new_rule = {\n                        \"rule\": new_rule_name,\n                        \"description\": new_rule_desc,\n                        \"keywords\": keywords_list,\n                        \"response\": new_response\n                    }\n                    self.safety_rules.append(new_rule)\n                    st.success(\"Safety rule added successfully!\")\n                    st.rerun()\n        \n        # Safety testing\n        st.subheader(\"Safety Rule Testing\")\n        \n        col1, col2 = st.columns([1, 1])\n        \n        with col1:\n            test_input = st.text_area(\n                \"Test Input:\",\n                placeholder=\"Enter a message to test against safety rules...\",\n                height=100\n            )\n            \n            if st.button(\"Test Safety Rules\"):\n                if test_input:\n                    is_safe, rule_triggered, suggested_response = self.check_input_safety(test_input)\n                    \n                    if is_safe:\n                        st.success(\"✅ Input passed safety checks\")\n                    else:\n                        st.error(f\"❌ Safety rule triggered: {rule_triggered}\")\n                        st.warning(f\"Suggested response: {suggested_response}\")\n        \n        with col2:\n            if st.button(\"Run Automated Safety Tests\"):\n                test_results = self.test_safety_rules()\n                \n                st.write(\"**Test Results:**\")\n                for result in test_results:\n                    status = \"✅\" if result['is_safe'] else \"❌\"\n                    st.write(f\"{status} {result['input']}\")\n                    if not result['is_safe']:\n                        st.caption(f\"Rule: {result['rule_triggered']}\")\n        \n        # Citation system\n        st.subheader(\"Citation System\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.write(\"**Supported Citation Formats:**\")\n            for pattern in self.citation_patterns:\n                st.code(pattern)\n        \n        with col2:\n            citation_test = st.text_area(\n                \"Test Citation Detection:\",\n                placeholder=\"Enter text with citations to test detection...\",\n                height=100\n            )\n            \n            if citation_test:\n                has_citations = any(re.search(pattern, citation_test) for pattern in self.citation_patterns)\n                if has_citations:\n                    st.success(\"✅ Citations detected\")\n                else:\n                    st.warning(\"❌ No citations found\")\n        \n        # Safety monitoring\n        st.subheader(\"Safety Monitoring\")\n        \n        # Generate mock safety report\n        if st.button(\"Generate Safety Report\"):\n            # This would normally use real interaction data\n            mock_interactions = [\n                {\"user_input\": \"What is diabetes?\", \"response\": \"Diabetes is a condition [1]...\"},\n                {\"user_input\": \"Should I take aspirin?\", \"response\": \"I cannot provide personalized medical advice...\"},\n                {\"user_input\": \"How does insulin work?\", \"response\": \"Insulin helps regulate blood sugar [1]...\"},\n            ]\n            \n            report = self.generate_safety_report(mock_interactions)\n            \n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.metric(\"Safety Trigger Rate\", f\"{report['safety_trigger_rate']:.1%}\")\n            \n            with col2:\n                st.metric(\"Citation Compliance\", f\"{report['citation_compliance_rate']:.1%}\")\n            \n            with col3:\n                st.metric(\"Overall Compliance Score\", f\"{report['compliance_score']:.1%}\")\n            \n            if report['violation_breakdown']:\n                st.write(\"**Violation Breakdown:**\")\n                for violation, count in report['violation_breakdown'].items():\n                    st.write(f\"- {violation}: {count}\")\n        \n        # System prompts\n        st.subheader(\"System Prompts\")\n        \n        with st.expander(\"Safety System Prompt\"):\n            st.text_area(\n                \"Current Safety System Prompt:\",\n                value=self.get_safety_system_prompt(),\n                height=300,\n                disabled=True\n            )\n        \n        with st.expander(\"Medical Disclaimer\"):\n            st.text_area(\n                \"Medical Disclaimer Text:\",\n                value=self.get_medical_disclaimer(),\n                height=100,\n                disabled=True\n            )\n        \n        # Export safety configuration\n        st.subheader(\"Export/Import Configuration\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            if st.button(\"Export Safety Config\"):\n                config = {\n                    \"safety_rules\": self.safety_rules,\n                    \"blocked_patterns\": self.blocked_patterns,\n                    \"citation_patterns\": self.citation_patterns\n                }\n                \n                config_json = json.dumps(config, indent=2)\n                st.download_button(\n                    label=\"Download Safety Configuration\",\n                    data=config_json,\n                    file_name=\"safety_config.json\",\n                    mime=\"application/json\"\n                )\n        \n        with col2:\n            uploaded_config = st.file_uploader(\n                \"Import Safety Config\",\n                type=['json'],\n                help=\"Import safety configuration from JSON file\"\n            )\n            \n            if uploaded_config and st.button(\"Import Configuration\"):\n                try:\n                    config = json.load(uploaded_config)\n                    self.safety_rules = config.get(\"safety_rules\", self.safety_rules)\n                    self.blocked_patterns = config.get(\"blocked_patterns\", self.blocked_patterns)\n                    self.citation_patterns = config.get(\"citation_patterns\", self.citation_patterns)\n                    st.success(\"Safety configuration imported successfully!\")\n                    st.rerun()\n                except Exception as e:\n                    st.error(f\"Error importing configuration: {str(e)}\")\n","size_bytes":18950},"config/settings.py":{"content":"import os\nfrom typing import Optional, Dict, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass Settings:\n    \"\"\"Application settings and configuration\"\"\"\n    \n    def __init__(self):\n        # Model settings\n        self.BASE_MODEL = os.getenv(\"BASE_MODEL\", \"microsoft/DialoGPT-medium\")\n        # Allowed base models (OSS + NeMo-compatible)\n        self.ALLOWED_MODELS = [\n            \"meta-llama/Llama-3-8b-instruct\",\n            \"meta-llama/Llama-3-70b-instruct\",\n            \"mistralai/Mistral-7B-Instruct-v0.3\",\n            \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n            \"microsoft/phi-3-mini-4k-instruct\",\n            \"tiiuae/falcon-7b-instruct\",\n            \"tiiuae/falcon-40b-instruct\",\n            \"microsoft/BioGPT\",\n            \"allenai/biomedlm\",\n            \"StanfordAIMI/MedAlpaca\",\n            \"nvidia/nemo-guardrails\",        # Safety / governance\n            \"nvidia/nv-embed-qa\",            # RAG embedding\n            \"nvidia/megatron-gpt-20b\",       # GPT-style LM\n            \"nvidia/megatron-gpt-530b\",      # Very large LM\n            \"nvidia/stt_en_conformer_transducer_large\", # ASR\n            \"nvidia/tts_en_fastpitch\",       # TTS\n        ]\n        \n        # Reset BASE_MODEL if not valid\n        if self.BASE_MODEL not in self.ALLOWED_MODELS:\n            logger.warning(f\"BASE_MODEL={self.BASE_MODEL} not in allowed models, defaulting to {self.ALLOWED_MODELS[0]}\")\n            self.BASE_MODEL = self.ALLOWED_MODELS[0]\n            \n        self.MAX_NEW_TOKENS = int(os.getenv(\"MAX_NEW_TOKENS\", \"256\"))\n        self.TEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n        self.TOP_P = float(os.getenv(\"TOP_P\", \"0.9\"))\n        self.TOP_K = int(os.getenv(\"TOP_K\", \"50\"))\n        self.REPETITION_PENALTY = float(os.getenv(\"REPETITION_PENALTY\", \"1.1\"))\n        \n        # RAG settings\n        self.EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"all-mpnet-base-v2\")\n        self.RETRIEVAL_TOP_K = int(os.getenv(\"RETRIEVAL_TOP_K\", \"3\"))\n        self.RETRIEVAL_THRESHOLD = float(os.getenv(\"RETRIEVAL_THRESHOLD\", \"0.7\"))\n        \n        # RAG backend selector: faiss | nemo | hybrid\n        self.RAG_BACKEND = os.getenv(\"RAG_BACKEND\", \"faiss\").lower()\n        if self.RAG_BACKEND not in [\"faiss\", \"nemo\", \"hybrid\"]:\n            logger.warning(f\"Unknown RAG_BACKEND={self.RAG_BACKEND}, defaulting to 'faiss'\")\n            self.RAG_BACKEND = \"faiss\"\n\n        # NeMo specific configs (free/open-source defaults)\n        self.NEMO_MODEL = os.getenv(\"NEMO_MODEL\", \"nvidia/nemo-guardrails\")  \n        self.NEMO_EMBED_MODEL = os.getenv(\"NEMO_EMBED_MODEL\", \"nvidia/nv-embed-qa\")  \n        self.NEMO_TOP_K = int(os.getenv(\"NEMO_TOP_K\", \"5\"))\n\n        # Hybrid configs\n        self.HYBRID_FUSION_METHOD = os.getenv(\"HYBRID_FUSION_METHOD\", \"reciprocal_rank_fusion\")\n                \n        # Fine-tuning settings\n        self.LORA_R = int(os.getenv(\"LORA_R\", \"64\"))\n        self.LORA_ALPHA = int(os.getenv(\"LORA_ALPHA\", \"128\"))\n        self.LORA_DROPOUT = float(os.getenv(\"LORA_DROPOUT\", \"0.1\"))\n        self.LORA_TARGET_MODULES = os.getenv(\n            \"LORA_TARGET_MODULES\", \n            \"q_proj,v_proj,k_proj,o_proj\"\n        ).split(\",\")\n        \n        # Training settings\n        self.LEARNING_RATE = float(os.getenv(\"LEARNING_RATE\", \"2e-5\"))\n        self.BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"2\"))\n        self.GRADIENT_ACCUMULATION_STEPS = int(os.getenv(\"GRADIENT_ACCUMULATION_STEPS\", \"2\"))\n        self.NUM_EPOCHS = int(os.getenv(\"NUM_EPOCHS\", \"3\"))\n        self.WEIGHT_DECAY = float(os.getenv(\"WEIGHT_DECAY\", \"0.01\"))\n        self.WARMUP_STEPS = int(os.getenv(\"WARMUP_STEPS\", \"100\"))\n        self.MAX_SEQ_LENGTH = int(os.getenv(\"MAX_SEQ_LENGTH\", \"1024\"))\n        \n        # Evaluation settings\n        self.EVAL_BATCH_SIZE = int(os.getenv(\"EVAL_BATCH_SIZE\", \"4\"))\n        self.EVAL_STEPS = int(os.getenv(\"EVAL_STEPS\", \"100\"))\n        self.SAVE_STEPS = int(os.getenv(\"SAVE_STEPS\", \"500\"))\n        self.LOGGING_STEPS = int(os.getenv(\"LOGGING_STEPS\", \"10\"))\n        \n        # Safety settings\n        self.SAFETY_ENABLED = os.getenv(\"SAFETY_ENABLED\", \"true\").lower() == \"true\"\n        self.CITATION_REQUIRED = os.getenv(\"CITATION_REQUIRED\", \"true\").lower() == \"true\"\n        self.MEDICAL_DISCLAIMER = os.getenv(\"MEDICAL_DISCLAIMER\", \"true\").lower() == \"true\"\n        \n        # Storage settings\n        self.DATA_DIR = os.getenv(\"DATA_DIR\", \"data\")\n        self.MODEL_DIR = os.getenv(\"MODEL_DIR\", \"models\")\n        self.EXPORT_DIR = os.getenv(\"EXPORT_DIR\", \"exports\")\n        self.LOGS_DIR = os.getenv(\"LOGS_DIR\", \"logs\")\n        \n        # API settings\n        self.HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"\")\n        self.OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n        self.ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n        \n        # Performance settings\n        self.USE_QUANTIZATION = os.getenv(\"USE_QUANTIZATION\", \"auto\")  # auto, 4bit, 8bit, none\n        self.DEVICE_MAP = os.getenv(\"DEVICE_MAP\", \"auto\")\n        self.MAX_MEMORY = os.getenv(\"MAX_MEMORY\", \"auto\")\n        self.LOW_CPU_MEM_USAGE = os.getenv(\"LOW_CPU_MEM_USAGE\", \"true\").lower() == \"true\"\n        \n        # Database settings (for future use)\n        self.DATABASE_URL = os.getenv(\"DATABASE_URL\", \"\")\n        self.REDIS_URL = os.getenv(\"REDIS_URL\", \"\")\n        \n        # Monitoring settings\n        self.ENABLE_TENSORBOARD = os.getenv(\"ENABLE_TENSORBOARD\", \"true\").lower() == \"true\"\n        self.ENABLE_WANDB = os.getenv(\"ENABLE_WANDB\", \"false\").lower() == \"true\"\n        self.WANDB_PROJECT = os.getenv(\"WANDB_PROJECT\", \"medgemma-ai\")\n        \n        # Security settings\n        self.SECRET_KEY = os.getenv(\"SECRET_KEY\", \"your-secret-key-here\")\n        self.ALLOWED_HOSTS = os.getenv(\"ALLOWED_HOSTS\", \"*\").split(\",\")\n        \n        # Create directories\n        self._create_directories()\n        \n        # Validate settings\n        self._validate_settings()\n    \n    def _create_directories(self):\n        \"\"\"Create necessary directories\"\"\"\n        directories = [\n            self.DATA_DIR,\n            self.MODEL_DIR,\n            self.EXPORT_DIR,\n            self.LOGS_DIR,\n        ]\n        \n        for directory in directories:\n            os.makedirs(directory, exist_ok=True)\n    \n    def _validate_settings(self):\n        \"\"\"Validate configuration settings\"\"\"\n        # Validate temperature range\n        if not 0.0 <= self.TEMPERATURE <= 2.0:\n            logger.warning(f\"Temperature {self.TEMPERATURE} is outside recommended range [0.0, 2.0]\")\n        \n        # Validate top_p range\n        if not 0.0 <= self.TOP_P <= 1.0:\n            logger.warning(f\"Top-p {self.TOP_P} is outside valid range [0.0, 1.0]\")\n        \n        # Validate learning rate\n        if not 1e-6 <= self.LEARNING_RATE <= 1e-2:\n            logger.warning(f\"Learning rate {self.LEARNING_RATE} is outside recommended range [1e-6, 1e-2]\")\n        \n        # Validate LoRA settings\n        if self.LORA_R <= 0:\n            logger.error(\"LoRA rank (r) must be positive\")\n        \n        if self.LORA_ALPHA <= 0:\n            logger.error(\"LoRA alpha must be positive\")\n        \n        # Validate batch sizes\n        if self.BATCH_SIZE <= 0:\n            logger.error(\"Batch size must be positive\")\n        \n        if self.GRADIENT_ACCUMULATION_STEPS <= 0:\n            logger.error(\"Gradient accumulation steps must be positive\")\n    \n    def get_model_config(self) -> Dict[str, Any]:\n        \"\"\"Get model configuration dictionary\"\"\"\n        return {\n            \"base_model\": self.BASE_MODEL,\n            \"max_new_tokens\": self.MAX_NEW_TOKENS,\n            \"temperature\": self.TEMPERATURE,\n            \"top_p\": self.TOP_P,\n            \"top_k\": self.TOP_K,\n            \"repetition_penalty\": self.REPETITION_PENALTY,\n        }\n    \n    def get_rag_config(self) -> Dict[str, Any]:\n        \"\"\"Get RAG configuration dictionary\"\"\"\n        return {\n            \"embed_model\": self.EMBED_MODEL,\n            \"retrieval_top_k\": self.RETRIEVAL_TOP_K,\n            \"retrieval_threshold\": self.RETRIEVAL_THRESHOLD,\n        }\n    \n    def get_rag_backend_config(self) -> Dict[str, Any]:\n        \"\"\"Get active RAG backend configuration\"\"\"\n        return {\n            \"backend\": self.RAG_BACKEND,\n            \"embed_model\": self.NEMO_EMBED_MODEL if self.RAG_BACKEND == \"nemo\" else self.EMBED_MODEL,\n            \"retrieval_top_k\": self.NEMO_TOP_K if self.RAG_BACKEND == \"nemo\" else self.RETRIEVAL_TOP_K,\n            \"retrieval_threshold\": self.RETRIEVAL_THRESHOLD,\n            \"nemo_model\": self.NEMO_MODEL,\n            \"nemo_embed_model\": self.NEMO_EMBED_MODEL,\n            \"nemo_top_k\": self.NEMO_TOP_K,\n            \"hybrid_fusion\": self.HYBRID_FUSION_METHOD,\n        }\n\n\n\n    def get_training_config(self) -> Dict[str, Any]:\n        \"\"\"Get training configuration dictionary\"\"\"\n        return {\n            \"learning_rate\": self.LEARNING_RATE,\n            \"batch_size\": self.BATCH_SIZE,\n            \"gradient_accumulation_steps\": self.GRADIENT_ACCUMULATION_STEPS,\n            \"num_epochs\": self.NUM_EPOCHS,\n            \"weight_decay\": self.WEIGHT_DECAY,\n            \"warmup_steps\": self.WARMUP_STEPS,\n            \"max_seq_length\": self.MAX_SEQ_LENGTH,\n            \"lora_r\": self.LORA_R,\n            \"lora_alpha\": self.LORA_ALPHA,\n            \"lora_dropout\": self.LORA_DROPOUT,\n            \"lora_target_modules\": self.LORA_TARGET_MODULES,\n        }\n    \n    def get_safety_config(self) -> Dict[str, Any]:\n        \"\"\"Get safety configuration dictionary\"\"\"\n        return {\n            \"safety_enabled\": self.SAFETY_ENABLED,\n            \"citation_required\": self.CITATION_REQUIRED,\n            \"medical_disclaimer\": self.MEDICAL_DISCLAIMER,\n        }\n    \n    def update_setting(self, key: str, value: Any):\n        \"\"\"Update a specific setting\"\"\"\n        if hasattr(self, key):\n            setattr(self, key, value)\n            logger.info(f\"Updated setting {key} to {value}\")\n        else:\n            logger.warning(f\"Unknown setting: {key}\")\n    \n    def export_config(self) -> Dict[str, Any]:\n        \"\"\"Export all configuration as dictionary\"\"\"\n        config = {}\n        \n        for attr_name in dir(self):\n            if not attr_name.startswith('_') and not callable(getattr(self, attr_name)):\n                config[attr_name] = getattr(self, attr_name)\n        \n        return config\n    \n    def import_config(self, config: Dict[str, Any]):\n        \"\"\"Import configuration from dictionary\"\"\"\n        for key, value in config.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n                logger.info(f\"Imported setting {key}\")\n            else:\n                logger.warning(f\"Unknown setting in import: {key}\")\n        \n        # Re-validate after import\n        self._validate_settings()\n    \n    def get_quantization_config(self) -> Optional[str]:\n        \"\"\"Get appropriate quantization setting based on environment\"\"\"\n        if self.USE_QUANTIZATION == \"auto\":\n            try:\n                import torch\n                if torch.cuda.is_available():\n                    # Check VRAM\n                    vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n                    if vram_gb >= 24:\n                        return None  # No quantization needed\n                    elif vram_gb >= 12:\n                        return \"8bit\"\n                    else:\n                        return \"4bit\"\n                else:\n                    return \"8bit\"  # Use 8bit for CPU\n            except:\n                return \"4bit\"  # Conservative fallback\n        elif self.USE_QUANTIZATION == \"none\":\n            return None\n        else:\n            return self.USE_QUANTIZATION\n    \n    def get_device_config(self) -> Dict[str, Any]:\n        \"\"\"Get device configuration\"\"\"\n        try:\n            import torch\n            \n            config = {\n                \"device_map\": self.DEVICE_MAP,\n                \"max_memory\": self.MAX_MEMORY,\n                \"low_cpu_mem_usage\": self.LOW_CPU_MEM_USAGE,\n                \"quantization\": self.get_quantization_config(),\n            }\n            \n            # Add device-specific settings\n            if torch.cuda.is_available():\n                config[\"cuda_available\"] = True\n                config[\"cuda_device_count\"] = torch.cuda.device_count()\n                config[\"cuda_memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n            else:\n                config[\"cuda_available\"] = False\n            \n            return config\n            \n        except ImportError:\n            return {\n                \"device_map\": \"cpu\",\n                \"max_memory\": None,\n                \"low_cpu_mem_usage\": True,\n                \"quantization\": \"8bit\",\n                \"cuda_available\": False,\n            }\n    \n    def __str__(self) -> str:\n        \"\"\"String representation of settings\"\"\"\n        return f\"Settings(base_model={self.BASE_MODEL}, safety_enabled={self.SAFETY_ENABLED})\"\n    \n    def __repr__(self) -> str:\n        \"\"\"Detailed representation of settings\"\"\"\n        return f\"Settings({self.export_config()})\"\n\n# Global settings instance\nsettings = Settings()\n","size_bytes":13132},"utils/__init__.py":{"content":"","size_bytes":0},"utils/data_processing.py":{"content":"import pandas as pd\nimport json\nimport csv\nimport re\nimport logging\nfrom typing import List, Dict, Any, Tuple\nfrom pathlib import Path\nimport streamlit as st\n\nlogger = logging.getLogger(__name__)\n\ndef process_medical_documents(uploaded_file) -> List[Dict[str, Any]]:\n    \"\"\"\n    Process uploaded medical documents and extract passages\n    Returns list of document passages with metadata\n    \"\"\"\n    passages = []\n    \n    try:\n        file_extension = Path(uploaded_file.name).suffix.lower()\n        \n        if file_extension == '.txt':\n            passages = process_text_file(uploaded_file)\n        elif file_extension == '.json':\n            passages = process_json_file(uploaded_file)\n        elif file_extension == '.csv':\n            passages = process_csv_file(uploaded_file)\n        elif file_extension == '.pdf':\n            # Validate PDF first\n            if not validate_pdf_file(uploaded_file):\n                st.warning(\"PDF file may be corrupted or invalid. Attempting to process anyway.\")\n            passages = process_pdf_file(uploaded_file)\n        else:\n            raise ValueError(f\"Unsupported file format: {file_extension}\")\n            \n    except Exception as e:\n        logger.error(f\"Error processing file {uploaded_file.name}: {str(e)}\")\n        raise e\n    \n    return passages\n\ndef validate_pdf_file(uploaded_file) -> bool:\n    \"\"\"Validate PDF file before processing\"\"\"\n    try:\n        # Save current position\n        current_pos = uploaded_file.tell()\n        \n        # Check if file is a PDF by magic number\n        if uploaded_file.read(4) != b'%PDF':\n            uploaded_file.seek(current_pos)\n            return False\n        uploaded_file.seek(current_pos)\n        \n        # Try to read the PDF\n        import PyPDF2\n        pdf_reader = PyPDF2.PdfReader(uploaded_file)\n        if len(pdf_reader.pages) == 0:\n            uploaded_file.seek(current_pos)\n            return False\n            \n        uploaded_file.seek(current_pos)\n        return True\n    except Exception:\n        return False\n\ndef process_text_file(uploaded_file) -> List[Dict[str, Any]]:\n    \"\"\"Process plain text files and split into passages\"\"\"\n    content = uploaded_file.read().decode('utf-8')\n    \n    # Split text into paragraphs or sentences\n    paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n    \n    passages = []\n    for i, paragraph in enumerate(paragraphs):\n        if len(paragraph) > 50:  # Filter out very short paragraphs\n            passages.append({\n                \"id\": f\"{uploaded_file.name}_{i}\",\n                \"text\": paragraph,\n                \"source\": uploaded_file.name,\n                \"metadata\": {\n                    \"type\": \"text_file\",\n                    \"paragraph_index\": i,\n                    \"length\": len(paragraph)\n                }\n            })\n    \n    return passages\n\ndef process_json_file(uploaded_file) -> List[Dict[str, Any]]:\n    \"\"\"Process JSON files containing medical data\"\"\"\n    data = json.load(uploaded_file)\n    passages = []\n    \n    if isinstance(data, list):\n        # List of documents/passages\n        for i, item in enumerate(data):\n            passage = extract_passage_from_dict(item, f\"{uploaded_file.name}_{i}\", uploaded_file.name)\n            if passage:\n                passages.append(passage)\n    elif isinstance(data, dict):\n        # Single document or structured data\n        if \"passages\" in data:\n            # Structured format with passages array\n            for i, passage_data in enumerate(data[\"passages\"]):\n                passage = extract_passage_from_dict(passage_data, f\"{uploaded_file.name}_{i}\", uploaded_file.name)\n                if passage:\n                    passages.append(passage)\n        else:\n            # Single document\n            passage = extract_passage_from_dict(data, uploaded_file.name, uploaded_file.name)\n            if passage:\n                passages.append(passage)\n    \n    return passages\n\ndef process_csv_file(uploaded_file) -> List[Dict[str, Any]]:\n    \"\"\"Process CSV files containing medical data\"\"\"\n    df = pd.read_csv(uploaded_file)\n    passages = []\n    \n    # Try to identify text columns\n    text_columns = []\n    for col in df.columns:\n        if any(keyword in col.lower() for keyword in ['text', 'content', 'passage', 'document', 'description', 'answer', 'response']):\n            text_columns.append(col)\n    \n    if not text_columns:\n        # If no obvious text columns, use all string columns\n        text_columns = [col for col in df.columns if df[col].dtype == 'object']\n    \n    for idx, row in df.iterrows():\n        text_parts = []\n        metadata = {}\n        \n        for col in df.columns:\n            if col in text_columns and pd.notna(row[col]):\n                text_parts.append(str(row[col]))\n            else:\n                metadata[col] = row[col] if pd.notna(row[col]) else None\n        \n        if text_parts:\n            combined_text = \" \".join(text_parts)\n            if len(combined_text.strip()) > 50:\n                passages.append({\n                    \"id\": f\"{uploaded_file.name}_{idx}\",\n                    \"text\": combined_text.strip(),\n                    \"source\": uploaded_file.name,\n                    \"metadata\": {\n                        \"type\": \"csv_file\",\n                        \"row_index\": idx,\n                        \"columns_used\": text_columns,\n                        **metadata\n                    }\n                })\n    \n    return passages\n\ndef process_pdf_file(uploaded_file) -> List[Dict[str, Any]]:\n    \"\"\"Process PDF files with robust error handling and fallback methods\"\"\"\n    passages = []\n    \n    try:\n        # Method 1: Try PyPDF2 first\n        try:\n            import PyPDF2\n            uploaded_file.seek(0)  # Reset file pointer\n            pdf_reader = PyPDF2.PdfReader(uploaded_file)\n            \n            for page_num, page in enumerate(pdf_reader.pages):\n                try:\n                    text = page.extract_text()\n                    if text and text.strip():\n                        page_passages = extract_passages_from_text(\n                            text, uploaded_file.name, page_num, \"pdf_file\"\n                        )\n                        passages.extend(page_passages)\n                except Exception as page_error:\n                    logger.warning(f\"Error extracting text from page {page_num}: {str(page_error)}\")\n                    continue\n                    \n        except Exception as pdf_error:\n            logger.warning(f\"PyPDF2 failed: {str(pdf_error)}\")\n            # Reset file pointer for next attempt\n            uploaded_file.seek(0)\n            \n            # Method 2: Try pdfplumber if available\n            try:\n                import pdfplumber\n                with pdfplumber.open(uploaded_file) as pdf:\n                    for page_num, page in enumerate(pdf.pages):\n                        try:\n                            text = page.extract_text()\n                            if text and text.strip():\n                                page_passages = extract_passages_from_text(\n                                    text, uploaded_file.name, page_num, \"pdf_file\"\n                                )\n                                passages.extend(page_passages)\n                        except Exception as page_error:\n                            logger.warning(f\"pdfplumber error on page {page_num}: {str(page_error)}\")\n                            continue\n            except ImportError:\n                logger.warning(\"pdfplumber not available\")\n            except Exception as plumber_error:\n                logger.error(f\"pdfplumber also failed: {str(plumber_error)}\")\n                \n        # If both methods failed, try a fallback approach\n        if not passages:\n            logger.info(\"Trying OCR fallback for PDF\")\n            passages = try_ocr_fallback(uploaded_file)\n            \n    except Exception as e:\n        logger.error(f\"Complete PDF processing failure: {str(e)}\")\n        return []\n    \n    return passages\n\ndef extract_passages_from_text(text: str, filename: str, page_num: int, file_type: str) -> List[Dict[str, Any]]:\n    \"\"\"Extract passages from text with proper cleaning\"\"\"\n    passages = []\n    \n    # Clean text first to prevent regex issues\n    text = clean_text(text)\n    \n    # Split text into paragraphs\n    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    \n    for para_idx, paragraph in enumerate(paragraphs):\n        if len(paragraph) > 100:  # Filter out headers/footers/short fragments\n            passages.append({\n                \"id\": f\"{filename}_page{page_num}_{para_idx}\",\n                \"text\": paragraph,\n                \"source\": filename,\n                \"metadata\": {\n                    \"type\": file_type,\n                    \"page_number\": page_num + 1,\n                    \"paragraph_index\": para_idx,\n                    \"length\": len(paragraph)\n                }\n            })\n    \n    return passages\n\ndef try_ocr_fallback(uploaded_file):\n    \"\"\"Fallback method using OCR for image-based PDFs\"\"\"\n    passages = []\n    try:\n        # Check if OCR libraries are available\n        try:\n            import pytesseract\n            from PIL import Image\n            import io\n        except ImportError:\n            logger.error(\"OCR libraries not installed\")\n            return passages\n            \n        # Try converting PDF to images and OCR\n        try:\n            from pdf2image import convert_from_bytes\n            uploaded_file.seek(0)\n            images = convert_from_bytes(uploaded_file.read())\n            \n            for page_num, image in enumerate(images):\n                text = pytesseract.image_to_string(image)\n                if text and text.strip():\n                    page_passages = extract_passages_from_text(\n                        text, uploaded_file.name, page_num, \"pdf_file_ocr\"\n                    )\n                    passages.extend(page_passages)\n                    \n        except ImportError:\n            logger.warning(\"pdf2image not available for OCR fallback\")\n        except Exception as ocr_error:\n            logger.error(f\"OCR failed: {str(ocr_error)}\")\n            \n    except Exception as e:\n        logger.error(f\"OCR fallback failed: {str(e)}\")\n    \n    return passages\n\ndef extract_passage_from_dict(data: Dict[str, Any], passage_id: str, source: str) -> Dict[str, Any]:\n    \"\"\"Extract passage from dictionary data\"\"\"\n    # Try different possible text fields\n    text_fields = ['text', 'content', 'passage', 'document', 'description', 'answer', 'response', 'body']\n    \n    text_content = None\n    for field in text_fields:\n        if field in data and data[field]:\n            text_content = str(data[field])\n            break\n    \n    if not text_content or len(text_content.strip()) < 20:\n        return None\n    \n    # Extract metadata (all other fields)\n    metadata = {k: v for k, v in data.items() if k not in text_fields}\n    metadata[\"type\"] = \"json_file\"\n    \n    return {\n        \"id\": passage_id,\n        \"text\": clean_text(text_content),\n        \"source\": source,\n        \"metadata\": metadata\n    }\n\ndef clean_text(text: str) -> str:\n    \"\"\"Clean and normalize text content with robust handling\"\"\"\n    if not text:\n        return \"\"\n    \n    # First, replace problematic characters\n    text = re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]', ' ', text)\n    \n    # Remove excessive whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove special characters that might interfere, but keep essential punctuation\n    text = re.sub(r'[^\\w\\s\\.,!?;:()\\[\\]\"\\'%\\-/]', '', text)\n    \n    # Normalize quotes\n    text = re.sub(r'[\"″‟‟\"]', '\"', text)\n    text = re.sub(r'[''′‛‘’]', \"'\", text)\n    \n    return text.strip()\n\ndef prepare_training_data(uploaded_file) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:\n    \"\"\"\n    Prepare training data from uploaded file\n    Returns (train_data, eval_data) as tuple of lists\n    \"\"\"\n    try:\n        file_extension = Path(uploaded_file.name).suffix.lower()\n        \n        if file_extension == '.json':\n            data = json.load(uploaded_file)\n        elif file_extension == '.csv':\n            df = pd.read_csv(uploaded_file)\n            data = df.to_dict('records')\n        elif file_extension == '.jsonl':\n            data = []\n            for line in uploaded_file:\n                data.append(json.loads(line.decode('utf-8')))\n        else:\n            raise ValueError(f\"Unsupported training data format: {file_extension}\")\n        \n        # Normalize data format\n        normalized_data = []\n        for item in data:\n            normalized_item = normalize_training_example(item)\n            if normalized_item:\n                normalized_data.append(normalized_item)\n        \n        # Split into train/eval (80/20)\n        split_index = int(0.8 * len(normalized_data))\n        train_data = normalized_data[:split_index]\n        eval_data = normalized_data[split_index:]\n        \n        return train_data, eval_data\n        \n    except Exception as e:\n        logger.error(f\"Error preparing training data: {str(e)}\")\n        raise e\n\ndef normalize_training_example(item: Dict[str, Any]) -> Dict[str, str]:\n    \"\"\"Normalize a training example to standard format\"\"\"\n    \n    # Try different field combinations for question-answer pairs\n    question_fields = ['question', 'input', 'prompt', 'instruction', 'query']\n    answer_fields = ['answer', 'output', 'response', 'completion', 'target']\n    context_fields = ['context', 'passage', 'background', 'document']\n    \n    question = None\n    answer = None\n    context = \"\"\n    \n    # Find question\n    for field in question_fields:\n        if field in item and item[field]:\n            question = str(item[field]).strip()\n            break\n    \n    # Find answer\n    for field in answer_fields:\n        if field in item and item[field]:\n            answer = str(item[field]).strip()\n            break\n    \n    # Find context\n    for field in context_fields:\n        if field in item and item[field]:\n            context = str(item[field]).strip()\n            break\n    \n    if not question or not answer:\n        return None\n    \n    return {\n        \"question\": question,\n        \"answer\": answer,\n        \"context\": context\n    }\n\ndef validate_medical_content(text: str) -> Tuple[bool, List[str]]:\n    \"\"\"\n    Validate if content appears to be medical in nature\n    Returns (is_medical, validation_notes)\n    \"\"\"\n    medical_keywords = [\n        # General medical terms\n        'medical', 'patient', 'diagnosis', 'treatment', 'therapy', 'clinical',\n        'hospital', 'doctor', 'physician', 'nurse', 'healthcare',\n        \n        # Body systems\n        'cardiovascular', 'respiratory', 'neurological', 'gastrointestinal',\n        'musculoskeletal', 'endocrine', 'immune', 'reproductive',\n        \n        # Common conditions\n        'diabetes', 'hypertension', 'cancer', 'infection', 'inflammation',\n        'disease', 'syndrome', 'disorder', 'condition',\n        \n        # Medical procedures\n        'surgery', 'biopsy', 'scan', 'test', 'examination', 'procedure',\n        'medication', 'drug', 'prescription', 'dose', 'dosage',\n        \n        # Anatomy\n        'heart', 'lung', 'brain', 'liver', 'kidney', 'blood', 'bone',\n        'muscle', 'tissue', 'organ', 'cell', 'vessel'\n    ]\n    \n    text_lower = text.lower()\n    found_keywords = [kw for kw in medical_keywords if kw in text_lower]\n    \n    is_medical = len(found_keywords) >= 2  # Require at least 2 medical keywords\n    \n    validation_notes = []\n    if is_medical:\n        validation_notes.append(f\"Found medical keywords: {', '.join(found_keywords[:5])}\")\n    else:\n        validation_notes.append(\"Low medical content confidence - consider reviewing\")\n    \n    return is_medical, validation_notes\n\ndef extract_medical_entities(text: str) -> Dict[str, List[str]]:\n    \"\"\"\n    Extract medical entities from text using rule-based approach\n    Returns dictionary of entity types and their values\n    \"\"\"\n    entities = {\n        'medications': [],\n        'conditions': [],\n        'procedures': [],\n        'anatomy': [],\n        'symptoms': []\n    }\n    \n    # Simple rule-based entity extraction\n    medication_patterns = [\n        r'\\b\\w+mycin\\b',  # antibiotics ending in -mycin\n        r'\\b\\w+cillin\\b',  # antibiotics ending in -cillin\n        r'\\b\\w+pril\\b',   # ACE inhibitors\n        r'\\b\\w+olol\\b',   # beta blockers\n        r'\\binsulin\\b', r'\\baspirin\\b', r'\\bibuprofen\\b'\n    ]\n    \n    condition_patterns = [\n        r'\\bdiabetes\\b', r'\\bhypertension\\b', r'\\bcancer\\b',\n        r'\\binfection\\b', r'\\bpneumonia\\b', r'\\basthma\\b'\n    ]\n    \n    # Extract entities using patterns\n    text_lower = text.lower()\n    \n    for pattern in medication_patterns:\n        matches = re.findall(pattern, text_lower)\n        entities['medications'].extend(matches)\n    \n    for pattern in condition_patterns:\n        matches = re.findall(pattern, text_lower)\n        entities['conditions'].extend(matches)\n    \n    # Remove duplicates\n    for key in entities:\n        entities[key] = list(set(entities[key]))\n    \n    return entities\n\ndef create_data_summary(passages: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"Create a summary of the processed data\"\"\"\n    if not passages:\n        return {\"error\": \"No passages to summarize\"}\n    \n    # Basic statistics\n    total_passages = len(passages)\n    total_text_length = sum(len(p['text']) for p in passages)\n    avg_passage_length = total_text_length / total_passages\n    \n    # Source distribution\n    sources = {}\n    for passage in passages:\n        source = passage.get('source', 'unknown')\n        sources[source] = sources.get(source, 0) + 1\n    \n    # Metadata analysis\n    metadata_fields = set()\n    for passage in passages:\n        if 'metadata' in passage:\n            metadata_fields.update(passage['metadata'].keys())\n    \n    # Medical content validation\n    medical_passages = 0\n    for passage in passages:\n        is_medical, _ = validate_medical_content(passage['text'])\n        if is_medical:\n            medical_passages += 1\n    \n    return {\n        \"total_passages\": total_passages,\n        \"total_text_length\": total_text_length,\n        \"avg_passage_length\": avg_passage_length,\n        \"medical_content_ratio\": medical_passages / total_passages,\n        \"source_distribution\": sources,\n        \"metadata_fields\": list(metadata_fields),\n        \"quality_score\": calculate_quality_score(passages)\n    }\n\ndef calculate_quality_score(passages: List[Dict[str, Any]]) -> float:\n    \"\"\"Calculate a quality score for the dataset\"\"\"\n    if not passages:\n        return 0.0\n    \n    scores = []\n    for passage in passages:\n        score = 0.0\n        text = passage['text']\n        \n        # Length score (prefer moderate length)\n        if 50 <= len(text) <= 2000:\n            score += 0.3\n        elif len(text) > 2000:\n            score += 0.2\n        \n        # Medical content score\n        is_medical, _ = validate_medical_content(text)\n        if is_medical:\n            score += 0.4\n        \n        # Structure score (has proper sentences)\n        sentences = text.split('.')\n        if len(sentences) >= 2:\n            score += 0.2\n        \n        # Metadata score\n        if 'metadata' in passage and passage['metadata']:\n            score += 0.1\n        \n        scores.append(score)\n    \n    return sum(scores) / len(scores)","size_bytes":19435},"utils/fallbacks.py":{"content":"\"\"\"\nFallback implementations for missing dependencies\nThis ensures the application works even when advanced ML packages aren't available\n\"\"\"\n\nimport os\nimport logging\nimport numpy as np\nfrom typing import List, Dict, Any, Optional, Tuple\n\nlogger = logging.getLogger(__name__)\n\n# Global flags for available packages\nHAS_TORCH = False\nHAS_TRANSFORMERS = False\nHAS_SENTENCE_TRANSFORMERS = False\nHAS_FAISS = False\nHAS_PEFT = False\nHAS_TRL = False\nHAS_DATASETS = False\nHAS_NLTK = False\nHAS_ROUGE = False\nHAS_NEMO = False\n\n\ntry:\n    import nemo.collections.nlp as nemo_nlp\n    HAS_NEMO = True\nexcept ImportError:\n    logger.warning(\"NVIDIA NeMo not available - using fallback implementations\")\n\n\n# Try importing packages and set flags\ntry:\n    import torch\n    HAS_TORCH = True\nexcept ImportError:\n    logger.warning(\"PyTorch not available - using fallback implementations\")\n\ntry:\n    import transformers\n    HAS_TRANSFORMERS = True\nexcept ImportError:\n    logger.warning(\"Transformers not available - using fallback implementations\")\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    HAS_SENTENCE_TRANSFORMERS = True\nexcept ImportError:\n    logger.warning(\"Sentence Transformers not available - using fallback implementations\")\n\ntry:\n    import faiss\n    HAS_FAISS = True\nexcept ImportError:\n    logger.warning(\"FAISS not available - using fallback implementations\")\n\ntry:\n    import peft\n    HAS_PEFT = True\nexcept ImportError:\n    logger.warning(\"PEFT not available - using fallback implementations\")\n\ntry:\n    import trl\n    HAS_TRL = True\nexcept ImportError:\n    logger.warning(\"TRL not available - using fallback implementations\")\n\ntry:\n    import datasets\n    HAS_DATASETS = True\nexcept ImportError:\n    logger.warning(\"Datasets not available - using fallback implementations\")\n\ntry:\n    import nltk\n    HAS_NLTK = True\nexcept ImportError:\n    logger.warning(\"NLTK not available - using fallback implementations\")\n\ntry:\n    from rouge_score import rouge_scorer\n    HAS_ROUGE = True\nexcept ImportError:\n    logger.warning(\"Rouge Score not available - using fallback implementations\")\n\n\nclass FallbackEmbedder:\n    \"\"\"Simple TF-IDF based embedder as fallback for sentence transformers\"\"\"\n    \n    def __init__(self, model_name: str = \"fallback\"):\n        self.model_name = model_name\n        self.vocab = {}\n        self.idf = {}\n        self.fitted = False\n        \n    def encode(self, texts: List[str], show_progress_bar: bool = False, convert_to_numpy: bool = True) -> np.ndarray:\n        \"\"\"Encode texts using simple TF-IDF\"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n            \n        if not self.fitted:\n            self._fit(texts)\n        \n        embeddings = []\n        for text in texts:\n            embedding = self._text_to_vector(text)\n            embeddings.append(embedding)\n            \n        embeddings = np.array(embeddings)\n        return embeddings\n    \n    def _fit(self, texts: List[str]):\n        \"\"\"Fit the TF-IDF model\"\"\"\n        # Build vocabulary\n        word_counts = {}\n        doc_counts = {}\n        \n        for text in texts:\n            words = text.lower().split()\n            unique_words = set(words)\n            \n            for word in words:\n                word_counts[word] = word_counts.get(word, 0) + 1\n                \n            for word in unique_words:\n                doc_counts[word] = doc_counts.get(word, 0) + 1\n        \n        # Create vocabulary (top 5000 words)\n        self.vocab = {word: idx for idx, (word, _) in enumerate(\n            sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:5000]\n        )}\n        \n        # Calculate IDF\n        total_docs = len(texts)\n        for word in self.vocab:\n            self.idf[word] = np.log(total_docs / (doc_counts.get(word, 1) + 1))\n            \n        self.fitted = True\n    \n    def _text_to_vector(self, text: str) -> np.ndarray:\n        \"\"\"Convert text to TF-IDF vector\"\"\"\n        words = text.lower().split()\n        vector = np.zeros(len(self.vocab))\n        \n        # Count word frequencies\n        word_freq = {}\n        for word in words:\n            if word in self.vocab:\n                word_freq[word] = word_freq.get(word, 0) + 1\n        \n        # Calculate TF-IDF\n        for word, freq in word_freq.items():\n            if word in self.vocab:\n                tf = freq / len(words)\n                idf = self.idf.get(word, 0)\n                vector[self.vocab[word]] = tf * idf\n                \n        # Normalize\n        norm = np.linalg.norm(vector)\n        if norm > 0:\n            vector = vector / norm\n            \n        return vector\n\n\nclass FallbackFAISS:\n    \"\"\"Simple similarity search as fallback for FAISS\"\"\"\n    \n    def __init__(self, dimension: int):\n        self.d = dimension\n        self.vectors = []\n        self.ntotal = 0\n        \n    def add(self, vectors: np.ndarray):\n        \"\"\"Add vectors to the index\"\"\"\n        self.vectors.extend(vectors)\n        self.ntotal = len(self.vectors)\n    \n    def search(self, query_vectors: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Search for nearest neighbors\"\"\"\n        if not self.vectors:\n            return np.array([[]]), np.array([[]])\n            \n        query_vector = query_vectors[0]  # Assume single query\n        similarities = []\n        \n        for i, vector in enumerate(self.vectors):\n            # Cosine similarity\n            similarity = np.dot(query_vector, vector) / (\n                np.linalg.norm(query_vector) * np.linalg.norm(vector) + 1e-8\n            )\n            similarities.append((similarity, i))\n        \n        # Sort by similarity (descending)\n        similarities.sort(reverse=True)\n        \n        # Return top k\n        k = min(k, len(similarities))\n        scores = np.array([[sim for sim, _ in similarities[:k]]])\n        indices = np.array([[idx for _, idx in similarities[:k]]])\n        \n        return scores, indices\n\n\nclass FallbackTokenizer:\n    \"\"\"Simple tokenizer fallback\"\"\"\n    \n    def __init__(self, model_name: str):\n        self.model_name = model_name\n        self.pad_token = \"<pad>\"\n        self.eos_token = \"<eos>\"\n        self.pad_token_id = 0\n        self.eos_token_id = 1\n        self.vocab_size = 50000\n        \n    def encode(self, text: str) -> List[int]:\n        \"\"\"Simple word-based encoding\"\"\"\n        words = text.split()\n        return [hash(word) % self.vocab_size for word in words]\n    \n    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:\n        \"\"\"Simple decoding\"\"\"\n        return \" \".join([f\"token_{tid}\" for tid in token_ids])\n    \n    def apply_chat_template(self, messages: List[Dict], tokenize: bool = False, add_generation_prompt: bool = True) -> str:\n        \"\"\"Simple chat template\"\"\"\n        formatted = \"\"\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n            formatted += f\"{role}: {content}\\n\"\n        return formatted\n    \n    def __call__(self, text: str, return_tensors: Optional[str] = None, **kwargs) -> Dict[str, Any]:\n        \"\"\"Tokenize text\"\"\"\n        token_ids = self.encode(text)\n        result = {\"input_ids\": token_ids}\n        if return_tensors == \"pt\":\n            result[\"input_ids\"] = [token_ids]\n            result[\"attention_mask\"] = [1] * len(token_ids)\n        return result\n\n\nclass FallbackModel:\n    \"\"\"Simple model fallback\"\"\"\n    \n    def __init__(self, model_name: str):\n        self.model_name = model_name\n        self.device = \"cpu\"\n        self.dtype = \"float32\"\n        \n    def generate(self, input_ids, max_new_tokens: int = 50, **kwargs) -> np.ndarray:\n        \"\"\"Generate simple response\"\"\"\n        # Return dummy output\n        input_length = len(input_ids[0]) if isinstance(input_ids, (list, np.ndarray)) else 10\n        output_length = input_length + max_new_tokens\n        return np.random.randint(0, 1000, (1, output_length))\n    \n    def eval(self):\n        \"\"\"Set to eval mode\"\"\"\n        pass\n    \n    def save_pretrained(self, path: str, **kwargs):\n        \"\"\"Save model\"\"\"\n        os.makedirs(path, exist_ok=True)\n        with open(os.path.join(path, \"config.json\"), \"w\") as f:\n            import json\n            json.dump({\"model_type\": \"fallback\", \"model_name\": self.model_name}, f)\n\n\ndef get_embedder(model_name: str = \"all-mpnet-base-v2\"):\n    \"\"\"Get embedder with fallback\"\"\"\n    if HAS_SENTENCE_TRANSFORMERS:\n        from sentence_transformers import SentenceTransformer\n        return SentenceTransformer(model_name)\n    else:\n        logger.warning(f\"Using fallback embedder instead of {model_name}\")\n        return FallbackEmbedder(model_name)\n\n\ndef get_faiss_index(dimension: int):\n    \"\"\"Get FAISS index with fallback\"\"\"\n    if HAS_FAISS:\n        import faiss\n        return faiss.IndexFlatIP(dimension)\n    else:\n        logger.warning(\"Using fallback similarity search instead of FAISS\")\n        return FallbackFAISS(dimension)\n\n\ndef normalize_l2(vectors: np.ndarray):\n    \"\"\"Normalize vectors to unit length\"\"\"\n    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n    norms[norms == 0] = 1  # Avoid division by zero\n    return vectors / norms\n\n\ndef get_model_and_tokenizer(model_name: str):\n    \"\"\"Get model and tokenizer with fallback\"\"\"\n    if HAS_TRANSFORMERS:\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n            model = AutoModelForCausalLM.from_pretrained(model_name)\n            return model, tokenizer\n        except Exception as e:\n            logger.warning(f\"Could not load {model_name}: {e}\")\n    \n    logger.warning(f\"Using fallback model instead of {model_name}\")\n    return FallbackModel(model_name), FallbackTokenizer(model_name)\n\n\ndef compute_bleu_fallback(prediction: str, reference: str) -> float:\n    \"\"\"Simple BLEU computation fallback\"\"\"\n    pred_words = prediction.lower().split()\n    ref_words = reference.lower().split()\n    \n    if not pred_words or not ref_words:\n        return 0.0\n    \n    # Simple unigram precision\n    pred_set = set(pred_words)\n    ref_set = set(ref_words)\n    \n    intersection = len(pred_set & ref_set)\n    precision = intersection / len(pred_set) if pred_set else 0\n    \n    # Simple brevity penalty\n    bp = min(1.0, len(pred_words) / len(ref_words)) if ref_words else 0\n    \n    return bp * precision\n\n\ndef compute_rouge_fallback(prediction: str, reference: str) -> float:\n    \"\"\"Simple ROUGE computation fallback\"\"\"\n    pred_words = prediction.lower().split()\n    ref_words = reference.lower().split()\n    \n    if not pred_words or not ref_words:\n        return 0.0\n    \n    # Simple word overlap F1\n    pred_set = set(pred_words)\n    ref_set = set(ref_words)\n    \n    intersection = len(pred_set & ref_set)\n    \n    if intersection == 0:\n        return 0.0\n    \n    precision = intersection / len(pred_set)\n    recall = intersection / len(ref_set)\n    \n    return 2 * precision * recall / (precision + recall)\n\n\n# Export availability flags\n__all__ = [\n    'HAS_TORCH', 'HAS_TRANSFORMERS', 'HAS_SENTENCE_TRANSFORMERS', 'HAS_FAISS',\n    'HAS_PEFT', 'HAS_TRL', 'HAS_DATASETS', 'HAS_NLTK', 'HAS_ROUGE',\n    'get_embedder', 'get_faiss_index', 'normalize_l2', 'get_model_and_tokenizer',\n    'compute_bleu_fallback', 'compute_rouge_fallback',\n    'FallbackEmbedder', 'FallbackFAISS', 'FallbackTokenizer', 'FallbackModel'\n]","size_bytes":11415},"utils/metrics.py":{"content":"import re\nimport numpy as np\nfrom collections import Counter\nfrom typing import List, Tuple, Dict, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef normalize_answer(text: str) -> str:\n    \"\"\"Normalize text for comparison\"\"\"\n    text = text.lower()\n    # Remove articles\n    text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    # Remove punctuation and extra spaces\n    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n    text = ' '.join(text.split())\n    return text\n\ndef compute_exact_match(prediction: str, reference: str) -> float:\n    \"\"\"Compute exact match score (0 or 1)\"\"\"\n    return float(normalize_answer(prediction) == normalize_answer(reference))\n\ndef compute_f1_score(prediction: str, reference: str) -> float:\n    \"\"\"Compute token-level F1 score\"\"\"\n    pred_tokens = normalize_answer(prediction).split()\n    ref_tokens = normalize_answer(reference).split()\n    \n    if not pred_tokens and not ref_tokens:\n        return 1.0\n    if not pred_tokens or not ref_tokens:\n        return 0.0\n    \n    # Count common tokens\n    pred_counter = Counter(pred_tokens)\n    ref_counter = Counter(ref_tokens)\n    common = pred_counter & ref_counter\n    num_common = sum(common.values())\n    \n    if num_common == 0:\n        return 0.0\n    \n    precision = num_common / len(pred_tokens)\n    recall = num_common / len(ref_tokens)\n    \n    f1 = 2 * precision * recall / (precision + recall)\n    return f1\n\ndef compute_bleu_score(prediction: str, reference: str, n: int = 4) -> float:\n    \"\"\"Compute BLEU score\"\"\"\n    try:\n        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n        import nltk\n        \n        # Download required NLTK data if not present\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt', quiet=True)\n        \n        # Tokenize\n        pred_tokens = normalize_answer(prediction).split()\n        ref_tokens = [normalize_answer(reference).split()]\n        \n        if not pred_tokens or not ref_tokens[0]:\n            return 0.0\n        \n        # Use smoothing to handle edge cases\n        smoothing = SmoothingFunction().method1\n        \n        # Compute BLEU with weights for n-grams up to n\n        weights = tuple([1.0/n] * n)\n        \n        bleu = sentence_bleu(\n            ref_tokens, \n            pred_tokens, \n            weights=weights,\n            smoothing_function=smoothing\n        )\n        \n        return bleu\n        \n    except ImportError:\n        # Fallback implementation without NLTK\n        return compute_simple_bleu(prediction, reference)\n\ndef compute_simple_bleu(prediction: str, reference: str) -> float:\n    \"\"\"Simple BLEU implementation without NLTK\"\"\"\n    pred_tokens = normalize_answer(prediction).split()\n    ref_tokens = normalize_answer(reference).split()\n    \n    if not pred_tokens or not ref_tokens:\n        return 0.0\n    \n    # Compute unigram precision\n    pred_counter = Counter(pred_tokens)\n    ref_counter = Counter(ref_tokens)\n    common = pred_counter & ref_counter\n    \n    precision = sum(common.values()) / len(pred_tokens)\n    \n    # Brevity penalty\n    bp = min(1.0, len(pred_tokens) / len(ref_tokens))\n    \n    return bp * precision\n\ndef compute_rouge_score(prediction: str, reference: str, rouge_type: str = \"rouge-l\") -> float:\n    \"\"\"Compute ROUGE score\"\"\"\n    try:\n        from rouge_score import rouge_scorer\n        \n        scorer = rouge_scorer.RougeScorer([rouge_type], use_stemmer=True)\n        scores = scorer.score(reference, prediction)\n        \n        # Return F1 score for the specified ROUGE metric\n        return scores[rouge_type].fmeasure\n        \n    except ImportError:\n        # Fallback to simple ROUGE-L implementation\n        return compute_simple_rouge_l(prediction, reference)\n\ndef compute_simple_rouge_l(prediction: str, reference: str) -> float:\n    \"\"\"Simple ROUGE-L implementation\"\"\"\n    pred_tokens = normalize_answer(prediction).split()\n    ref_tokens = normalize_answer(reference).split()\n    \n    if not pred_tokens or not ref_tokens:\n        return 0.0\n    \n    # Compute longest common subsequence\n    lcs_length = longest_common_subsequence(pred_tokens, ref_tokens)\n    \n    if lcs_length == 0:\n        return 0.0\n    \n    precision = lcs_length / len(pred_tokens)\n    recall = lcs_length / len(ref_tokens)\n    \n    if precision + recall == 0:\n        return 0.0\n    \n    f1 = 2 * precision * recall / (precision + recall)\n    return f1\n\ndef longest_common_subsequence(seq1: List[str], seq2: List[str]) -> int:\n    \"\"\"Compute length of longest common subsequence\"\"\"\n    m, n = len(seq1), len(seq2)\n    \n    # Create DP table\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if seq1[i-1] == seq2[j-1]:\n                dp[i][j] = dp[i-1][j-1] + 1\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n    \n    return dp[m][n]\n\ndef compute_semantic_similarity(prediction: str, reference: str) -> float:\n    \"\"\"Compute semantic similarity using simple word overlap\"\"\"\n    pred_tokens = set(normalize_answer(prediction).split())\n    ref_tokens = set(normalize_answer(reference).split())\n    \n    if not pred_tokens and not ref_tokens:\n        return 1.0\n    if not pred_tokens or not ref_tokens:\n        return 0.0\n    \n    intersection = len(pred_tokens & ref_tokens)\n    union = len(pred_tokens | ref_tokens)\n    \n    return intersection / union if union > 0 else 0.0\n\ndef compute_medical_accuracy_score(prediction: str, reference: str) -> float:\n    \"\"\"Compute medical-specific accuracy considering medical terms\"\"\"\n    \n    # Define medical term patterns\n    medical_patterns = [\n        r'\\b\\d+\\s*(mg|ml|mcg|g|kg|units?)\\b',  # Dosages\n        r'\\b[A-Z][a-z]+\\s+(test|scan|procedure)\\b',  # Medical procedures\n        r'\\b(Type\\s+[12]|Stage\\s+[IVX]+)\\b',  # Medical classifications\n        r'\\b\\w+(itis|osis|emia|uria)\\b',  # Medical conditions\n    ]\n    \n    pred_medical_terms = set()\n    ref_medical_terms = set()\n    \n    # Extract medical terms from both texts\n    for pattern in medical_patterns:\n        pred_medical_terms.update(re.findall(pattern, prediction, re.IGNORECASE))\n        ref_medical_terms.update(re.findall(pattern, reference, re.IGNORECASE))\n    \n    # If no medical terms found, fall back to regular F1\n    if not pred_medical_terms and not ref_medical_terms:\n        return compute_f1_score(prediction, reference)\n    \n    # Compute medical term overlap\n    common_medical = len(pred_medical_terms & ref_medical_terms)\n    total_medical = len(ref_medical_terms)\n    \n    if total_medical == 0:\n        return compute_f1_score(prediction, reference)\n    \n    medical_accuracy = common_medical / total_medical\n    \n    # Combine with regular F1 score\n    regular_f1 = compute_f1_score(prediction, reference)\n    \n    # Weight medical accuracy more heavily\n    return 0.7 * medical_accuracy + 0.3 * regular_f1\n\ndef compute_citation_score(prediction: str, retrieved_contexts: List[Dict[str, Any]]) -> float:\n    \"\"\"Compute score for proper citation usage\"\"\"\n    if not retrieved_contexts:\n        return 1.0  # No citations expected\n    \n    citation_patterns = [\n        r'\\[(\\d+)\\]',\n        r'\\((\\d+)\\)',\n        r'reference\\s+(\\d+)',\n    ]\n    \n    found_citations = set()\n    for pattern in citation_patterns:\n        matches = re.findall(pattern, prediction, re.IGNORECASE)\n        found_citations.update(int(m) for m in matches if m.isdigit())\n    \n    expected_citations = set(range(1, len(retrieved_contexts) + 1))\n    \n    if not expected_citations:\n        return 1.0\n    \n    # Score based on citation coverage and accuracy\n    correct_citations = len(found_citations & expected_citations)\n    total_expected = len(expected_citations)\n    \n    citation_recall = correct_citations / total_expected\n    \n    # Penalize hallucinated citations\n    hallucinated_citations = len(found_citations - expected_citations)\n    citation_precision = correct_citations / len(found_citations) if found_citations else 0.0\n    \n    # Combine recall and precision\n    if citation_recall + citation_precision == 0:\n        return 0.0\n    \n    citation_f1 = 2 * citation_recall * citation_precision / (citation_recall + citation_precision)\n    return citation_f1\n\ndef compute_safety_compliance_score(prediction: str) -> float:\n    \"\"\"Compute score for safety compliance\"\"\"\n    \n    # Check for safety indicators\n    safety_indicators = [\n        r'consult\\s+.*\\s+(doctor|physician|healthcare|professional)',\n        r'seek\\s+medical\\s+attention',\n        r'not\\s+.*\\s+substitute\\s+.*\\s+medical\\s+advice',\n        r'educational\\s+purposes?\\s+only',\n        r'medical\\s+emergency',\n    ]\n    \n    safety_warnings = [\n        r'do\\s+not\\s+.*\\s+(self-medicate|diagnose)',\n        r'cannot\\s+.*\\s+(diagnose|treat|prescribe)',\n        r'always\\s+consult',\n    ]\n    \n    # Check for inappropriate content\n    inappropriate_patterns = [\n        r'you\\s+have\\s+.*\\s+(cancer|disease|condition)',  # Direct diagnosis\n        r'take\\s+\\d+.*\\s+(pills?|tablets?|mg)',  # Specific dosage\n        r'stop\\s+taking\\s+.*\\s+medication',  # Dangerous advice\n    ]\n    \n    prediction_lower = prediction.lower()\n    \n    # Count safety indicators\n    safety_score = 0.0\n    \n    # Positive indicators\n    for pattern in safety_indicators + safety_warnings:\n        if re.search(pattern, prediction_lower):\n            safety_score += 0.2\n    \n    # Negative indicators (penalties)\n    for pattern in inappropriate_patterns:\n        if re.search(pattern, prediction_lower):\n            safety_score -= 0.3\n    \n    # Normalize to [0, 1]\n    safety_score = max(0.0, min(1.0, safety_score))\n    \n    return safety_score\n\ndef compute_comprehensive_score(\n    prediction: str, \n    reference: str, \n    retrieved_contexts: List[Dict[str, Any]] = None\n) -> Dict[str, float]:\n    \"\"\"Compute comprehensive evaluation scores\"\"\"\n    \n    scores = {\n        'exact_match': compute_exact_match(prediction, reference),\n        'f1_score': compute_f1_score(prediction, reference),\n        'bleu_score': compute_bleu_score(prediction, reference),\n        'rouge_l': compute_rouge_score(prediction, reference),\n        'semantic_similarity': compute_semantic_similarity(prediction, reference),\n        'medical_accuracy': compute_medical_accuracy_score(prediction, reference),\n        'safety_compliance': compute_safety_compliance_score(prediction),\n    }\n    \n    # Add citation score if contexts provided\n    if retrieved_contexts:\n        scores['citation_score'] = compute_citation_score(prediction, retrieved_contexts)\n    \n    # Compute overall score as weighted average\n    weights = {\n        'exact_match': 0.15,\n        'f1_score': 0.20,\n        'bleu_score': 0.10,\n        'rouge_l': 0.10,\n        'semantic_similarity': 0.10,\n        'medical_accuracy': 0.20,\n        'safety_compliance': 0.15,\n    }\n    \n    if 'citation_score' in scores:\n        weights['citation_score'] = 0.10\n        # Renormalize other weights\n        total_other = sum(w for k, w in weights.items() if k != 'citation_score')\n        for k in weights:\n            if k != 'citation_score':\n                weights[k] = weights[k] * 0.9 / total_other\n    \n    overall_score = sum(scores[k] * weights[k] for k in scores if k in weights)\n    scores['overall_score'] = overall_score\n    \n    return scores\n\ndef compute_confidence_metrics(predictions: List[str], references: List[str], confidences: List[float]) -> Dict[str, Any]:\n    \"\"\"Compute confidence calibration metrics\"\"\"\n    if not confidences or len(confidences) != len(predictions):\n        return {}\n    \n    # Compute accuracy for each prediction\n    accuracies = [compute_exact_match(pred, ref) for pred, ref in zip(predictions, references)]\n    \n    # Bin predictions by confidence\n    n_bins = 10\n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n    \n    bin_confidences = []\n    bin_accuracies = []\n    bin_counts = []\n    \n    for i in range(n_bins):\n        lower, upper = bin_boundaries[i], bin_boundaries[i + 1]\n        \n        # Find predictions in this bin\n        in_bin = np.array([(lower <= c < upper) or (i == n_bins - 1 and c == upper) \n                          for c in confidences])\n        \n        if np.sum(in_bin) > 0:\n            bin_confidences.append(np.mean(np.array(confidences)[in_bin]))\n            bin_accuracies.append(np.mean(np.array(accuracies)[in_bin]))\n            bin_counts.append(np.sum(in_bin))\n        else:\n            bin_confidences.append(bin_centers[i])\n            bin_accuracies.append(0.0)\n            bin_counts.append(0)\n    \n    # Expected Calibration Error (ECE)\n    ece = 0.0\n    total_samples = len(predictions)\n    \n    for conf, acc, count in zip(bin_confidences, bin_accuracies, bin_counts):\n        if count > 0:\n            ece += (count / total_samples) * abs(conf - acc)\n    \n    # Maximum Calibration Error (MCE)\n    mce = max(abs(conf - acc) for conf, acc in zip(bin_confidences, bin_accuracies))\n    \n    return {\n        'ece': ece,\n        'mce': mce,\n        'bin_confidences': bin_confidences,\n        'bin_accuracies': bin_accuracies,\n        'bin_counts': bin_counts,\n        'reliability_data': list(zip(bin_confidences, bin_accuracies, bin_counts))\n    }\n\ndef compute_diversity_metrics(predictions: List[str]) -> Dict[str, float]:\n    \"\"\"Compute diversity metrics for predictions\"\"\"\n    if not predictions:\n        return {}\n    \n    # Compute lexical diversity\n    all_tokens = []\n    for pred in predictions:\n        tokens = normalize_answer(pred).split()\n        all_tokens.extend(tokens)\n    \n    unique_tokens = set(all_tokens)\n    lexical_diversity = len(unique_tokens) / len(all_tokens) if all_tokens else 0\n    \n    # Compute response length statistics\n    lengths = [len(pred.split()) for pred in predictions]\n    \n    return {\n        'lexical_diversity': lexical_diversity,\n        'avg_response_length': np.mean(lengths),\n        'std_response_length': np.std(lengths),\n        'min_response_length': np.min(lengths),\n        'max_response_length': np.max(lengths),\n    }\n","size_bytes":14152},"utils/model_utils.py":{"content":"\"\"\"\nUnified model loading utility that works across all components\nPreserves all original functionality while adding unified loading\n\"\"\"\n\nimport logging\nfrom typing import Tuple, Optional, Dict, Any\nimport os\nfrom utils.fallbacks import HAS_TORCH, HAS_TRANSFORMERS, HAS_PEFT, get_model_and_tokenizer, FallbackModel, FallbackTokenizer\nfrom sentence_transformers import SentenceTransformer\n\n# Import available packages\ntorch = None\nAutoTokenizer = None\nAutoModelForCausalLM = None\nBitsAndBytesConfig = None\nGenerationConfig = None\nPeftModel = None\nPeftConfig = None\n\ntry:\n    if HAS_TORCH:\n        import torch\nexcept ImportError:\n    pass\n\ntry:\n    if HAS_TRANSFORMERS:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\nexcept ImportError:\n    pass\n\ntry:\n    if HAS_PEFT:\n        from peft import PeftModel, PeftConfig\nexcept ImportError:\n    pass\n\nlogger = logging.getLogger(__name__)\n\n# ==================== UNIFIED MODEL LOADER ====================\ndef get_unified_model_loader():\n    \"\"\"Get the appropriate model loader based on available dependencies\"\"\"\n    try:\n        # Try to use proper transformers first\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        from utils.fallbacks import HAS_TORCH\n        \n        def load_model_transformers(model_name: str):\n            \"\"\"Load model using transformers\"\"\"\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(model_name)\n                \n                if HAS_TORCH and torch is not None:\n                    model = AutoModelForCausalLM.from_pretrained(\n                        model_name,\n                        torch_dtype=torch.float16,\n                        device_map=\"auto\",\n                        low_cpu_mem_usage=True\n                    )\n                else:\n                    model = AutoModelForCausalLM.from_pretrained(model_name)\n                \n                # Set padding token if missing\n                if hasattr(tokenizer, 'pad_token') and tokenizer.pad_token is None:\n                    tokenizer.pad_token = tokenizer.eos_token\n                    tokenizer.pad_token_id = tokenizer.eos_token_id\n                \n                return model, tokenizer\n                \n            except Exception as e:\n                logger.warning(f\"Transformers loading failed: {e}, falling back\")\n                return FallbackModel(model_name), FallbackTokenizer(model_name)\n        \n        return load_model_transformers\n        \n    except ImportError:\n        # Fallback to simple implementation\n        def load_model_fallback(model_name: str):\n            \"\"\"Fallback model loading\"\"\"\n            return FallbackModel(model_name), FallbackTokenizer(model_name)\n        \n        return load_model_fallback\n\n# Global model loader instance\n_model_loader = None\n\ndef get_model_loader():\n    \"\"\"Get or create the global model loader\"\"\"\n    global _model_loader\n    if _model_loader is None:\n        _model_loader = get_unified_model_loader()\n    return _model_loader\n\ndef load_base_model_unified(model_name: str):\n    \"\"\"Unified model loading function for all components\"\"\"\n    loader = get_model_loader()\n    return loader(model_name)\n\n# ==================== ORIGINAL FUNCTIONS (PRESERVED) ====================\ndef detect_device() -> str:\n    \"\"\"Detect the best available device for model inference\"\"\"\n    if HAS_TORCH and torch is not None:\n        if torch.cuda.is_available():\n            return \"cuda\"\n        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n            return \"mps\"\n        else:\n            return \"cpu\"\n    else:\n        return \"cpu\"\n\ndef get_model_info(model_name: str) -> Dict[str, Any]:\n    \"\"\"Get model information and requirements for free & NeMo-compatible models\"\"\"\n    model_configs = {\n        # --- Meta LLaMA 3 ---\n        \"meta-llama/Llama-3-8b-instruct\": {\n            \"size_gb\": 15,\n            \"min_ram_gb\": 16,\n            \"recommended_ram_gb\": 32,\n            \"supports_4bit\": True,\n            \"supports_8bit\": True,\n            \"context_length\": 8192\n        },\n        \"meta-llama/Llama-3-70b-instruct\": {\n            \"size_gb\": 140,\n            \"min_ram_gb\": 64,\n            \"recommended_ram_gb\": 128,\n            \"supports_4bit\": True,\n            \"supports_8bit\": True,\n            \"context_length\": 8192\n        },\n\n        # --- Mistral / Mixtral ---\n        \"mistralai/Mistral-7B-Instruct-v0.3\": {\n            \"size_gb\": 13,\n            \"min_ram_gb\": 16,\n            \"recommended_ram_gb\": 32,\n            \"supports_4bit\": True,\n            \"supports_8bit\": True,\n            \"context_length\": 8192\n        },\n        \"mistralai/Mixtral-8x7B-Instruct-v0.1\": {\n            \"size_gb\": 45,\n            \"min_ram_gb\": 48,\n            \"recommended_ram_gb\": 96,\n            \"supports_4bit\": True,\n            \"supports_8bit\": True,\n            \"context_length\": 32000\n        },\n\n        # --- Microsoft Phi-3 ---\n        \"microsoft/phi-3-mini-4k-instruct\": {\n            \"size_gb\": 3.8,\n            \"min_ram_gb\": 8,\n            \"recommended_ram_gb\": 16,\n            \"supports_4bit\": True,\n            \"supports_8bit\": True,\n            \"context_length\": 4096\n        },\n\n        # --- Falcon ---\n        \"tiiuae/falcon-7b-instruct\": {\n            \"size_gb\": 13,\n            \"min_ram_gb\": 16,\n            \"recommended_ram_gb\": 32,\n            \"supports_4bit\": True,\n            \"supports_8bit\": True,\n            \"context_length\": 2048\n        },\n        \"tiiuae/falcon-40b-instruct\": {\n            \"size_gb\": 90,\n            \"min_ram_gb\": 64,\n            \"recommended_ram_gb\": 128,\n            \"supports_4bit\": True,\n            \"supports_8bit\": True,\n            \"context_length\": 2048\n        },\n\n        # --- Medical / Domain-specific ---\n        \"microsoft/BioGPT\": {\n            \"size_gb\": 1.5,\n            \"min_ram_gb\": 4,\n            \"recommended_ram_gb\": 8,\n            \"supports_4bit\": False,\n            \"supports_8bit\": True,\n            \"context_length\": 1024\n        },\n        \"allenai/biomedlm\": {\n            \"size_gb\": 2.2,\n            \"min_ram_gb\": 4,\n            \"recommended_ram_gb\": 8,\n            \"supports_4bit\": False,\n            \"supports_8bit\": True,\n            \"context_length\": 2048\n        },\n        \"StanfordAIMI/MedAlpaca\": {\n            \"size_gb\": 7,\n            \"min_ram_gb\": 8,\n            \"recommended_ram_gb\": 16,\n            \"supports_4bit\": True,\n            \"supports_8bit\": True,\n            \"context_length\": 2048\n        },\n    }\n    \n    return model_configs.get(model_name, {\n        \"size_gb\": \"unknown\",\n        \"min_ram_gb\": 8,\n        \"recommended_ram_gb\": 16,\n        \"supports_4bit\": True,\n        \"supports_8bit\": True,\n        \"context_length\": 2048\n    })\n\ndef create_quantization_config(quantization_type: str = \"4bit\") -> Optional[BitsAndBytesConfig]:\n    \"\"\"Create quantization configuration for memory efficiency\"\"\"\n    if not HAS_TRANSFORMERS or BitsAndBytesConfig is None:\n        return None\n    \n    if quantization_type == \"4bit\":\n        return BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16 if torch else None,\n            bnb_4bit_use_double_quant=True,\n        )\n    elif quantization_type == \"8bit\":\n        return BitsAndBytesConfig(\n            load_in_8bit=True,\n        )\n    else:\n        return None\n\ndef load_base_model(\n    model_name: str, \n    quantization: Optional[str] = None,\n    device_map: str = \"auto\",\n    trust_remote_code: bool = True\n) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n    \"\"\"\n    Load base model and tokenizer with optimizations\n    Uses unified loader as fallback if transformers not available\n    \"\"\"\n    try:\n        # Try original implementation first\n        model_info = get_model_info(model_name)\n        device = detect_device()\n        \n        logger.info(f\"Loading model {model_name} on device {device}\")\n        \n        # Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=trust_remote_code,\n            padding_side=\"left\"\n        )\n        \n        # Add padding token if missing\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            tokenizer.pad_token_id = tokenizer.eos_token_id\n        \n        # Prepare model loading arguments\n        model_kwargs = {\n            \"trust_remote_code\": trust_remote_code,\n            \"device_map\": device_map,\n            \"torch_dtype\": torch.float16 if device != \"cpu\" else torch.float32,\n        }\n        \n        # Add quantization if specified\n        if quantization and device == \"cuda\":\n            quantization_config = create_quantization_config(quantization)\n            if quantization_config:\n                model_kwargs[\"quantization_config\"] = quantization_config\n                logger.info(f\"Using {quantization} quantization\")\n        \n        # Load model\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            **model_kwargs\n        )\n        \n        # Enable gradient checkpointing for memory efficiency\n        if hasattr(model, 'gradient_checkpointing_enable'):\n            model.gradient_checkpointing_enable()\n        \n        logger.info(f\"Successfully loaded {model_name}\")\n        return model, tokenizer\n        \n    except Exception as e:\n        logger.warning(f\"Original loading failed: {e}, using unified loader\")\n        # Fallback to unified loader\n        return load_base_model_unified(model_name)\n\ndef load_peft_model(\n    base_model_name: str,\n    peft_model_path: str,\n    quantization: Optional[str] = None\n) -> Tuple[PeftModel, AutoTokenizer]:\n    \"\"\"Load a PEFT (LoRA) model\"\"\"\n    try:\n        # Load base model and tokenizer\n        base_model, tokenizer = load_base_model(base_model_name, quantization)\n        \n        # Load PEFT model\n        model = PeftModel.from_pretrained(base_model, peft_model_path)\n        \n        logger.info(f\"Successfully loaded PEFT model from {peft_model_path}\")\n        return model, tokenizer\n        \n    except Exception as e:\n        logger.error(f\"Error loading PEFT model: {str(e)}\")\n        raise e\n\ndef merge_peft_model(peft_model: PeftModel) -> AutoModelForCausalLM:\n    \"\"\"Merge PEFT adapters with base model\"\"\"\n    try:\n        merged_model = peft_model.merge_and_unload()\n        logger.info(\"Successfully merged PEFT adapters\")\n        return merged_model\n    except Exception as e:\n        logger.error(f\"Error merging PEFT model: {str(e)}\")\n        raise e\n\ndef create_generation_config(\n    max_new_tokens: int = 256,\n    temperature: float = 0.7,\n    top_p: float = 0.9,\n    top_k: int = 50,\n    repetition_penalty: float = 1.1,\n    do_sample: bool = True\n) -> GenerationConfig:\n    \"\"\"Create optimized generation configuration\"\"\"\n    if not HAS_TRANSFORMERS or GenerationConfig is None:\n        # Return simple dict for fallback\n        return {\n            \"max_new_tokens\": max_new_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"top_k\": top_k,\n            \"repetition_penalty\": repetition_penalty,\n            \"do_sample\": do_sample\n        }\n    \n    return GenerationConfig(\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        repetition_penalty=repetition_penalty,\n        do_sample=do_sample,\n        pad_token_id=None,\n        eos_token_id=None,\n        use_cache=True,\n        return_dict_in_generate=True,\n        output_scores=True\n    )\n\ndef estimate_memory_usage(model_name: str, quantization: Optional[str] = None) -> Dict[str, float]:\n    \"\"\"Estimate memory usage for model loading\"\"\"\n    model_info = get_model_info(model_name)\n    base_size_gb = model_info.get(\"size_gb\", 8)\n    \n    if isinstance(base_size_gb, str):\n        base_size_gb = 8  # Default fallback\n    \n    memory_estimates = {\n        \"base_model_gb\": base_size_gb,\n        \"recommended_ram_gb\": model_info.get(\"recommended_ram_gb\", 16)\n    }\n    \n    if quantization == \"4bit\":\n        memory_estimates[\"quantized_size_gb\"] = base_size_gb * 0.25\n        memory_estimates[\"total_estimated_gb\"] = base_size_gb * 0.4\n    elif quantization == \"8bit\":\n        memory_estimates[\"quantized_size_gb\"] = base_size_gb * 0.5\n        memory_estimates[\"total_estimated_gb\"] = base_size_gb * 0.7\n    else:\n        memory_estimates[\"total_estimated_gb\"] = base_size_gb * 1.2\n    \n    return memory_estimates\n\ndef check_model_compatibility(model_name: str) -> Dict[str, Any]:\n    \"\"\"Check if model is compatible with current environment\"\"\"\n    device = detect_device()\n    model_info = get_model_info(model_name)\n    \n    # Get available memory\n    available_memory_gb = 0\n    if device == \"cuda\":\n        try:\n            available_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n        except:\n            available_memory_gb = 8  # Conservative estimate\n    else:\n        try:\n            import psutil\n            available_memory_gb = psutil.virtual_memory().available / (1024**3)\n        except ImportError:\n            available_memory_gb = 8  # Conservative estimate\n    \n    # Check compatibility\n    min_required = model_info.get(\"min_ram_gb\", 8)\n    recommended = model_info.get(\"recommended_ram_gb\", 16)\n    \n    compatibility = {\n        \"compatible\": available_memory_gb >= min_required,\n        \"optimal\": available_memory_gb >= recommended,\n        \"available_memory_gb\": available_memory_gb,\n        \"min_required_gb\": min_required,\n        \"recommended_gb\": recommended,\n        \"device\": device,\n        \"suggestions\": []\n    }\n    \n    if not compatibility[\"compatible\"]:\n        compatibility[\"suggestions\"].append(\"Use 4-bit quantization to reduce memory usage\")\n        compatibility[\"suggestions\"].append(\"Consider using a smaller model\")\n    elif not compatibility[\"optimal\"]:\n        compatibility[\"suggestions\"].append(\"Consider using quantization for better performance\")\n    \n    return compatibility\n\ndef optimize_model_for_inference(model: AutoModelForCausalLM) -> AutoModelForCausalLM:\n    \"\"\"Apply optimizations for inference\"\"\"\n    try:\n        # Enable eval mode\n        model.eval()\n        \n        # Compile model if PyTorch 2.0+ is available\n        if hasattr(torch, 'compile'):\n            try:\n                model = torch.compile(model, mode=\"reduce-overhead\")\n                logger.info(\"Model compiled with PyTorch 2.0\")\n            except Exception as e:\n                logger.warning(f\"Could not compile model: {str(e)}\")\n        \n        # Enable attention optimizations if available\n        if hasattr(model.config, 'use_cache'):\n            model.config.use_cache = True\n        \n        return model\n        \n    except Exception as e:\n        logger.warning(f\"Could not apply all optimizations: {str(e)}\")\n        return model\n\ndef get_model_generation_stats(model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> Dict[str, Any]:\n    \"\"\"Get model statistics and capabilities\"\"\"\n    try:\n        config = model.config\n        \n        stats = {\n            \"model_type\": getattr(config, 'model_type', 'unknown'),\n            \"vocab_size\": getattr(config, 'vocab_size', len(tokenizer)),\n            \"hidden_size\": getattr(config, 'hidden_size', 'unknown'),\n            \"num_layers\": getattr(config, 'num_hidden_layers', 'unknown'),\n            \"num_attention_heads\": getattr(config, 'num_attention_heads', 'unknown'),\n            \"max_position_embeddings\": getattr(config, 'max_position_embeddings', 'unknown'),\n            \"torch_dtype\": str(model.dtype),\n            \"device\": str(next(model.parameters()).device),\n            \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n            \"total_params\": sum(p.numel() for p in model.parameters()),\n        }\n        \n        # Calculate model size in MB\n        param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n        stats[\"model_size_mb\"] = (param_size + buffer_size) / (1024 * 1024)\n        \n        return stats\n        \n    except Exception as e:\n        logger.error(f\"Error getting model stats: {str(e)}\")\n        return {\"error\": str(e)}\n\ndef save_model_safely(\n    model: AutoModelForCausalLM, \n    tokenizer: AutoTokenizer, \n    save_path: str,\n    safe_serialization: bool = True,\n    max_shard_size: str = \"2GB\"\n) -> bool:\n    \"\"\"Save model and tokenizer safely\"\"\"\n    try:\n        os.makedirs(save_path, exist_ok=True)\n        \n        # Save model\n        model.save_pretrained(\n            save_path,\n            safe_serialization=safe_serialization,\n            max_shard_size=max_shard_size\n        )\n        \n        # Save tokenizer\n        tokenizer.save_pretrained(save_path)\n        \n        logger.info(f\"Model and tokenizer saved to {save_path}\")\n        return True\n        \n    except Exception as e:\n        logger.error(f\"Error saving model: {str(e)}\")\n        return False\n\n# ==================== BACKWARD COMPATIBILITY ====================\n# Ensure existing code continues to work\ndef get_model_and_tokenizer(model_name: str):\n    \"\"\"Backward compatibility alias\"\"\"\n    return load_base_model_unified(model_name)","size_bytes":17368}},"version":1}